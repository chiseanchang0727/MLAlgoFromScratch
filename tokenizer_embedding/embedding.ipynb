{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7172811c",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5ab95",
   "metadata": {},
   "source": [
    "Here is the implementation including tokenize, embedding and simple training for given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e8d118b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f879908de10>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "from collections import  Counter\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50af2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, texts, min_freq=1, max_vocab_size=10000):\n",
    "        self.texts = texts\n",
    "        self.min_freq = min_freq\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab = self.build_vocab()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        # keep only a-z, A-Z, 0-9, whitespace\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "        return text.split()\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        word_counts = Counter()\n",
    "        for text in self.texts:\n",
    "            tokens = self.preprocess(text)\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "\n",
    "        \"\"\"\n",
    "        `most_common()` returns words from most frequent â†’ least frequent.\n",
    "        Example:\n",
    "        [(\"hello\", 2), (\"world\", 1), (\"Jacy\", 1)]\n",
    "        This ensures high-frequency words get smaller IDs (useful for models).\n",
    "        \"\"\"\n",
    "        for word, freq in word_counts.most_common(self.max_vocab_size-2): # 2 for <pad> and <unk>\n",
    "            if freq >= self.min_freq:\n",
    "                vocab[word] = len(vocab)\n",
    "        \n",
    "        \"\"\"\n",
    "        Ex:\n",
    "        vocab = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unk>\": 1,\n",
    "            \"hello\": 2,\n",
    "            \"world\": 3,\n",
    "            \"sean\": 4\n",
    "        }\n",
    "        \"\"\"\n",
    "        return vocab\n",
    "\n",
    "    def text_to_sequence(self, text):\n",
    "        tokens = self.preprocess(text)\n",
    "        [\"\"]\n",
    "        return [\n",
    "            self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens\n",
    "        ]\n",
    "    \n",
    "    def pad_sequences(self, sequences, pad_value=0):\n",
    "        # convert each list to a tensor\n",
    "        tensors = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "        # pad to same length\n",
    "        padded = pad_sequence(tensors, batch_first=True, padding_value=pad_value)\n",
    "        return padded\n",
    "    \n",
    "    def tokenize_all_text(self):\n",
    "        tokenized_seq = []\n",
    "        for text in self.texts:\n",
    "            tokenized_seq.append(self.text_to_sequence(text))\n",
    "        padded_tensor = self.pad_sequences(tokenized_seq, pad_value=self.vocab[\"<pad>\"])\n",
    "        return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "157972dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"PyTorch is widely used for deep learning tasks and neural networks.\",\n",
    "    \"Tokenizers help convert raw text into numerical representations.\",\n",
    "    \"Machine learning models rely on large datasets to generalize well.\",\n",
    "    \"The fox and the dog became friends after many adventures.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11e8463a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 7, 8, 16, 9, 10, 2, 11, 4]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(texts)\n",
    "tokenizer.text_to_sequence(\"The quick brown for jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "13db33ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ddd: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  7,  8,  3,  9, 10,  2, 11,  4,  0,  0],\n",
       "        [12, 13, 14, 15, 16, 17,  5, 18,  6, 19, 20],\n",
       "        [21, 22, 23, 24, 25, 26, 27, 28,  0,  0,  0],\n",
       "        [29,  5, 30, 31, 32, 33, 34, 35, 36, 37,  0],\n",
       "        [ 2,  3,  6,  2,  4, 38, 39, 40, 41, 42,  0]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_seq = tokenizer.tokenize_all_text()\n",
    "text_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd646b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837468fe",
   "metadata": {},
   "source": [
    "build embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e889e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3b5dd47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextEmbedding(\n",
       "  (embedding): Embedding(43, 8)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = TextEmbedding(VOCAB_SIZE, EMB_DIM).to(DEVICE)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b3fedede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_338396/2888420899.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  text_tensor = torch.tensor(text_seq, dtype=torch.long).to(DEVICE)\n"
     ]
    }
   ],
   "source": [
    "text_tensor = torch.tensor(text_seq, dtype=torch.long).to(DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlalgofromscratch (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
