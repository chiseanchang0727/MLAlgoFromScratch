{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33163345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\MLAlgoFromScratch\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76e3cd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "\n",
    "\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a0dc0",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8628d04",
   "metadata": {},
   "source": [
    "### Building a WordPiece tokenizer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06dfe9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(version=\"1.0\", truncation=None, padding=None, added_tokens=[], normalizer=None, pre_tokenizer=None, post_processor=None, decoder=None, model=WordPiece(unk_token=\"[UNK]\", continuing_subword_prefix=\"##\", max_input_chars_per_word=100, vocab={}))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a12e07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "# These two tokenizer.normalizer has the same manner, but there is slightly different:\n",
    "# BertNormalizer requires `clean_text==True`\n",
    "\n",
    "# tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "\n",
    "print(tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cbff8d",
   "metadata": {},
   "source": [
    "we can see the result of normalizers.Sequence() and BertNormalizer is sligth different in some cases,  \n",
    "but we don't want to deal with this complication here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19705cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    hello world\n"
     ]
    }
   ],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "print(tokenizer.normalizer.normalize_str(\"NÌƒÌƒ    Hello\\tWorld\\u200b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f11eb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    hello\tworldâ€‹\n"
     ]
    }
   ],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "\n",
    "print(tokenizer.normalizer.normalize_str(\"NÌƒÌƒ    Hello\\tWorld\\u200b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61eb59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('my', (11, 13)),\n",
       " ('pre', (14, 17)),\n",
       " ('-', (17, 18)),\n",
       " ('tokenizer', (18, 27)),\n",
       " ('.', (27, 28))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")\n",
    "\n",
    "# so does:\n",
    "# pre_tokenizer = pre_tokenizers.Sequence(\n",
    "#    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f923a5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordPieceTrainer(WordPieceTrainer(bpe_trainer=BpeTrainer(min_frequency=0, vocab_size=25000, show_progress=True, special_tokens=[AddedToken(content=\"[UNK]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[PAD]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[CLS]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[SEP]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True), AddedToken(content=\"[MASK]\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True)], limit_alphabet=None, initial_alphabet=[], continuing_subword_prefix=\"##\", end_of_word_suffix=None, max_token_length=None, words={})))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to pass it all the special tokens you intend to use\n",
    "# otherwise it wonâ€™t add them to the vocabulary, since they are not in the training corpus\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df555895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizer\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c371b2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
      "['let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']\n"
     ]
    }
   ],
   "source": [
    "# test the trainer tokenizer\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding)\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792914df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b388e6",
   "metadata": {},
   "source": [
    "#### post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534881e",
   "metadata": {},
   "source": [
    "add [CLS], [SEP] ... special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e8c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc729f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemplateProcessing(single=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"[SEP]\", type_id=0)], pair=[SpecialToken(id=\"[CLS]\", type_id=0), Sequence(id=A, type_id=0), SpecialToken(id=\"[SEP]\", type_id=0), Sequence(id=B, type_id=1), SpecialToken(id=\"[SEP]\", type_id=1)], special_tokens={\"[CLS]\":SpecialToken(id=\"[CLS]\", ids=[2], tokens=[\"[CLS]\"]), \"[SEP]\":SpecialToken(id=\"[SEP]\", ids=[3], tokens=[\"[SEP]\"])})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs.\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "tokenizer.post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4adeef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'let', \"'\", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfec420",
   "metadata": {},
   "source": [
    "### Wrap the trained tokenizer for transformer training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b794b5",
   "metadata": {},
   "source": [
    "To use this tokenizer in ðŸ¤— Transformers, we have to wrap it in a PreTrainedTokenizerFast. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, BertTokenizerFast). If you apply this lesson to build a brand new tokenizer, you will have to use the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414aaa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generic class since we are creating a brand new tokenizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizerFast\n\u001b[32m      4\u001b[39m wrapped_tokenizer = PreTrainedTokenizerFast(\n\u001b[32m      5\u001b[39m     tokenizer_object=tokenizer,\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     mask_token=\u001b[33m\"\u001b[39m\u001b[33m[MASK]\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Generic class since we are creating a brand new tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# input all special tokens at once\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    # tokenizer_file=\"tokenizer.json\", # You can load from the tokenizer file, alternatively\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a specific tokenizer class (like BertTokenizerFast), \n",
    "# we will only need to specify the special tokens that are different from the default ones (here, none):\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb67498",
   "metadata": {},
   "source": [
    "- We can then use this tokenizer like any other ðŸ¤— Transformers tokenizer. \n",
    "- We can save it with the save_pretrained() method, or upload it to the Hub with the push_to_hub() method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAlgoFromScratch (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
