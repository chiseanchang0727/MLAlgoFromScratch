{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0366e+00, 1.0746e+00, 1.1140e+00, 1.1548e+00, 1.1971e+00,\n",
       "        1.2409e+00, 1.2864e+00, 1.3335e+00, 1.3824e+00, 1.4330e+00, 1.4855e+00,\n",
       "        1.5399e+00, 1.5963e+00, 1.6548e+00, 1.7154e+00, 1.7783e+00, 1.8434e+00,\n",
       "        1.9110e+00, 1.9810e+00, 2.0535e+00, 2.1288e+00, 2.2067e+00, 2.2876e+00,\n",
       "        2.3714e+00, 2.4582e+00, 2.5483e+00, 2.6416e+00, 2.7384e+00, 2.8387e+00,\n",
       "        2.9427e+00, 3.0505e+00, 3.1623e+00, 3.2781e+00, 3.3982e+00, 3.5227e+00,\n",
       "        3.6517e+00, 3.7855e+00, 3.9242e+00, 4.0679e+00, 4.2170e+00, 4.3714e+00,\n",
       "        4.5316e+00, 4.6976e+00, 4.8697e+00, 5.0481e+00, 5.2330e+00, 5.4247e+00,\n",
       "        5.6234e+00, 5.8294e+00, 6.0430e+00, 6.2643e+00, 6.4938e+00, 6.7317e+00,\n",
       "        6.9783e+00, 7.2339e+00, 7.4989e+00, 7.7737e+00, 8.0584e+00, 8.3536e+00,\n",
       "        8.6596e+00, 8.9769e+00, 9.3057e+00, 9.6466e+00, 1.0000e+01, 1.0366e+01,\n",
       "        1.0746e+01, 1.1140e+01, 1.1548e+01, 1.1971e+01, 1.2409e+01, 1.2864e+01,\n",
       "        1.3335e+01, 1.3824e+01, 1.4330e+01, 1.4855e+01, 1.5399e+01, 1.5963e+01,\n",
       "        1.6548e+01, 1.7154e+01, 1.7783e+01, 1.8434e+01, 1.9110e+01, 1.9810e+01,\n",
       "        2.0535e+01, 2.1288e+01, 2.2067e+01, 2.2876e+01, 2.3714e+01, 2.4582e+01,\n",
       "        2.5483e+01, 2.6416e+01, 2.7384e+01, 2.8387e+01, 2.9427e+01, 3.0505e+01,\n",
       "        3.1623e+01, 3.2781e+01, 3.3982e+01, 3.5227e+01, 3.6517e+01, 3.7855e+01,\n",
       "        3.9242e+01, 4.0679e+01, 4.2170e+01, 4.3714e+01, 4.5316e+01, 4.6976e+01,\n",
       "        4.8697e+01, 5.0481e+01, 5.2330e+01, 5.4247e+01, 5.6234e+01, 5.8294e+01,\n",
       "        6.0430e+01, 6.2643e+01, 6.4938e+01, 6.7317e+01, 6.9783e+01, 7.2339e+01,\n",
       "        7.4989e+01, 7.7737e+01, 8.0584e+01, 8.3536e+01, 8.6596e+01, 8.9769e+01,\n",
       "        9.3057e+01, 9.6466e+01, 1.0000e+02, 1.0366e+02, 1.0746e+02, 1.1140e+02,\n",
       "        1.1548e+02, 1.1971e+02, 1.2409e+02, 1.2864e+02, 1.3335e+02, 1.3824e+02,\n",
       "        1.4330e+02, 1.4855e+02, 1.5399e+02, 1.5963e+02, 1.6548e+02, 1.7154e+02,\n",
       "        1.7783e+02, 1.8434e+02, 1.9110e+02, 1.9810e+02, 2.0535e+02, 2.1288e+02,\n",
       "        2.2067e+02, 2.2876e+02, 2.3714e+02, 2.4582e+02, 2.5483e+02, 2.6416e+02,\n",
       "        2.7384e+02, 2.8387e+02, 2.9427e+02, 3.0505e+02, 3.1623e+02, 3.2781e+02,\n",
       "        3.3982e+02, 3.5227e+02, 3.6517e+02, 3.7855e+02, 3.9242e+02, 4.0679e+02,\n",
       "        4.2170e+02, 4.3714e+02, 4.5316e+02, 4.6976e+02, 4.8697e+02, 5.0481e+02,\n",
       "        5.2330e+02, 5.4247e+02, 5.6234e+02, 5.8294e+02, 6.0430e+02, 6.2643e+02,\n",
       "        6.4938e+02, 6.7317e+02, 6.9783e+02, 7.2339e+02, 7.4989e+02, 7.7737e+02,\n",
       "        8.0584e+02, 8.3536e+02, 8.6596e+02, 8.9769e+02, 9.3057e+02, 9.6466e+02,\n",
       "        1.0000e+03, 1.0366e+03, 1.0746e+03, 1.1140e+03, 1.1548e+03, 1.1971e+03,\n",
       "        1.2409e+03, 1.2864e+03, 1.3335e+03, 1.3824e+03, 1.4330e+03, 1.4855e+03,\n",
       "        1.5399e+03, 1.5963e+03, 1.6548e+03, 1.7154e+03, 1.7783e+03, 1.8434e+03,\n",
       "        1.9110e+03, 1.9810e+03, 2.0535e+03, 2.1288e+03, 2.2067e+03, 2.2876e+03,\n",
       "        2.3714e+03, 2.4582e+03, 2.5483e+03, 2.6416e+03, 2.7384e+03, 2.8387e+03,\n",
       "        2.9427e+03, 3.0505e+03, 3.1623e+03, 3.2781e+03, 3.3982e+03, 3.5227e+03,\n",
       "        3.6517e+03, 3.7855e+03, 3.9242e+03, 4.0679e+03, 4.2170e+03, 4.3714e+03,\n",
       "        4.5316e+03, 4.6976e+03, 4.8697e+03, 5.0481e+03, 5.2330e+03, 5.4247e+03,\n",
       "        5.6234e+03, 5.8294e+03, 6.0430e+03, 6.2643e+03, 6.4938e+03, 6.7317e+03,\n",
       "        6.9783e+03, 7.2339e+03, 7.4989e+03, 7.7737e+03, 8.0584e+03, 8.3536e+03,\n",
       "        8.6596e+03, 8.9769e+03, 9.3057e+03, 9.6466e+03])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "torch.pow(torch.tensor(10000.0), torch.arange(0, d_model, 2).float() / d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,  22.,\n",
       "         24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,  44.,  46.,\n",
       "         48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,  66.,  68.,  70.,\n",
       "         72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,  88.,  90.,  92.,  94.,\n",
       "         96.,  98., 100., 102., 104., 106., 108., 110., 112., 114., 116., 118.,\n",
       "        120., 122., 124., 126., 128., 130., 132., 134., 136., 138., 140., 142.,\n",
       "        144., 146., 148., 150., 152., 154., 156., 158., 160., 162., 164., 166.,\n",
       "        168., 170., 172., 174., 176., 178., 180., 182., 184., 186., 188., 190.,\n",
       "        192., 194., 196., 198., 200., 202., 204., 206., 208., 210., 212., 214.,\n",
       "        216., 218., 220., 222., 224., 226., 228., 230., 232., 234., 236., 238.,\n",
       "        240., 242., 244., 246., 248., 250., 252., 254., 256., 258., 260., 262.,\n",
       "        264., 266., 268., 270., 272., 274., 276., 278., 280., 282., 284., 286.,\n",
       "        288., 290., 292., 294., 296., 298., 300., 302., 304., 306., 308., 310.,\n",
       "        312., 314., 316., 318., 320., 322., 324., 326., 328., 330., 332., 334.,\n",
       "        336., 338., 340., 342., 344., 346., 348., 350., 352., 354., 356., 358.,\n",
       "        360., 362., 364., 366., 368., 370., 372., 374., 376., 378., 380., 382.,\n",
       "        384., 386., 388., 390., 392., 394., 396., 398., 400., 402., 404., 406.,\n",
       "        408., 410., 412., 414., 416., 418., 420., 422., 424., 426., 428., 430.,\n",
       "        432., 434., 436., 438., 440., 442., 444., 446., 448., 450., 452., 454.,\n",
       "        456., 458., 460., 462., 464., 466., 468., 470., 472., 474., 476., 478.,\n",
       "        480., 482., 484., 486., 488., 490., 492., 494., 496., 498., 500., 502.,\n",
       "        504., 506., 508., 510.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, d_model, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   3.,   5.,   7.,   9.,  11.,  13.,  15.,  17.,  19.,  21.,  23.,\n",
       "         25.,  27.,  29.,  31.,  33.,  35.,  37.,  39.,  41.,  43.,  45.,  47.,\n",
       "         49.,  51.,  53.,  55.,  57.,  59.,  61.,  63.,  65.,  67.,  69.,  71.,\n",
       "         73.,  75.,  77.,  79.,  81.,  83.,  85.,  87.,  89.,  91.,  93.,  95.,\n",
       "         97.,  99., 101., 103., 105., 107., 109., 111., 113., 115., 117., 119.,\n",
       "        121., 123., 125., 127., 129., 131., 133., 135., 137., 139., 141., 143.,\n",
       "        145., 147., 149., 151., 153., 155., 157., 159., 161., 163., 165., 167.,\n",
       "        169., 171., 173., 175., 177., 179., 181., 183., 185., 187., 189., 191.,\n",
       "        193., 195., 197., 199., 201., 203., 205., 207., 209., 211., 213., 215.,\n",
       "        217., 219., 221., 223., 225., 227., 229., 231., 233., 235., 237., 239.,\n",
       "        241., 243., 245., 247., 249., 251., 253., 255., 257., 259., 261., 263.,\n",
       "        265., 267., 269., 271., 273., 275., 277., 279., 281., 283., 285., 287.,\n",
       "        289., 291., 293., 295., 297., 299., 301., 303., 305., 307., 309., 311.,\n",
       "        313., 315., 317., 319., 321., 323., 325., 327., 329., 331., 333., 335.,\n",
       "        337., 339., 341., 343., 345., 347., 349., 351., 353., 355., 357., 359.,\n",
       "        361., 363., 365., 367., 369., 371., 373., 375., 377., 379., 381., 383.,\n",
       "        385., 387., 389., 391., 393., 395., 397., 399., 401., 403., 405., 407.,\n",
       "        409., 411., 413., 415., 417., 419., 421., 423., 425., 427., 429., 431.,\n",
       "        433., 435., 437., 439., 441., 443., 445., 447., 449., 451., 453., 455.,\n",
       "        457., 459., 461., 463., 465., 467., 469., 471., 473., 475., 477., 479.,\n",
       "        481., 483., 485., 487., 489., 491., 493., 495., 497., 499., 501., 503.,\n",
       "        505., 507., 509., 511.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, d_model, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "# Using torch.exp\n",
    "exp_values = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))\n",
    "\n",
    "# Using torch.pow\n",
    "pow_values = torch.pow(torch.tensor(10000.0), torch.arange(0, d_model, 2).float() / d_model)\n",
    "\n",
    "# Check if both are the same\n",
    "print(torch.allclose(exp_values, pow_values))  # Should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 // 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask in Transfomer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., -inf],\n",
       "        [5., 6., -inf, -inf]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_scores = torch.tensor([[1, 2, 3, 4],\n",
    "                            [5, 6, 7, 8]]).float()\n",
    "\n",
    "pad_mask = torch.tensor([[1, 1, 1, 0],\n",
    "                         [1, 1, 0, 0]])\n",
    "atten_scores.masked_fill(pad_mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mask change the value in unwanted position to -inf, the value will become 0 after passing softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 5\n",
    "# diagonal = 0 : default\n",
    "# diagonal > 0 (up to length) : shift to upper\n",
    "# diagonal < 0 (up to length) : shift to lower\n",
    "# diagonal: the step toward up/down \n",
    "causal_mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1) # torch.tril is lower triangular matrix\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False, False]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False: allow attention\n",
    "# True: Masked(prevent future access)\n",
    "causal_mask.bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(causal_mask == 0).type(torch.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $W_q \\times Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8  # Model embedding size\n",
    "h = 2\n",
    "d_k = d_model // h\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 7.5591e-04,  4.4857e-02,  1.4582e-01, -2.1877e-01,  2.1747e-01,\n",
       "          1.8150e-01,  1.5741e-01,  1.2602e-01],\n",
       "        [ 8.8719e-02,  2.7200e-01, -3.4544e-01,  2.0090e-01, -4.4764e-02,\n",
       "         -1.2815e-01,  9.3956e-02,  4.8173e-02],\n",
       "        [-9.1833e-02,  1.2522e-01, -4.3306e-02, -1.2683e-01, -3.0004e-01,\n",
       "         -1.1195e-01,  3.2550e-01,  1.8483e-01],\n",
       "        [-5.9201e-02, -2.5787e-01,  3.4349e-01,  1.7445e-01, -2.5080e-01,\n",
       "          1.5355e-01, -2.4857e-01,  3.5176e-01],\n",
       "        [ 2.2605e-01, -3.2244e-01, -8.4742e-02,  1.2366e-01, -9.5108e-02,\n",
       "          1.1743e-01,  4.7694e-02, -1.7589e-02],\n",
       "        [ 1.6352e-01, -1.0181e-01, -1.1778e-01,  1.7568e-01, -9.2812e-02,\n",
       "         -3.7827e-02, -3.1650e-01, -1.3969e-02],\n",
       "        [ 7.6046e-03, -1.2586e-04,  2.7877e-01,  5.8528e-02,  2.0851e-01,\n",
       "         -2.6827e-01, -2.3720e-01, -1.8464e-01],\n",
       "        [-2.3086e-01, -3.0483e-02, -1.3578e-01, -1.4407e-01, -9.5851e-02,\n",
       "          2.1293e-02,  2.3807e-01, -2.4179e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3400, -0.3509,  0.2995, -0.1715, -0.2445,  0.1943, -0.3062, -0.2653],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1758, -0.7158, -1.6454, -0.1701, -0.8235, -1.9992, -1.5552,\n",
       "           1.5420],\n",
       "         [ 0.9165,  1.1402, -0.0271,  0.8261,  0.9398, -0.9483,  0.0492,\n",
       "          -0.5483],\n",
       "         [ 1.2590,  1.5621, -1.3211,  0.0960,  1.5383, -0.6539, -0.0911,\n",
       "           0.9099],\n",
       "         [ 0.2910,  0.2309, -0.4954, -1.0792,  0.6189,  0.4962,  0.2605,\n",
       "           0.2538],\n",
       "         [-0.5783,  0.8073,  0.2560, -0.2330,  0.3843,  0.0537, -0.2927,\n",
       "           1.6973]],\n",
       "\n",
       "        [[-2.1391,  0.2016,  0.3085,  0.7132,  1.4065,  0.4722,  1.1029,\n",
       "          -0.2255],\n",
       "         [-1.4241,  0.9203, -0.1398,  0.7591,  1.3070,  1.6616, -0.2865,\n",
       "          -1.0090],\n",
       "         [-0.1426, -1.0962,  0.9393,  0.1482,  1.9969, -1.2529, -0.0523,\n",
       "          -0.0634],\n",
       "         [-0.9436,  1.5869, -0.7038, -0.2044,  0.4028, -0.9870,  0.8727,\n",
       "          -1.8494],\n",
       "         [ 0.8823, -0.6066,  1.8098, -0.2472,  0.3164, -0.4205,  0.5744,\n",
       "           0.3119]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1674,  0.1943,  0.5684,  0.2572, -0.1927,  1.0251, -0.3273,\n",
       "          -0.6617],\n",
       "         [-0.5020,  0.2736, -0.0067, -0.9714, -0.4892,  0.3171,  0.2813,\n",
       "          -0.5930],\n",
       "         [-0.1665,  0.7116,  0.1747, -1.2294, -0.5832,  0.3117, -0.3098,\n",
       "          -0.8411],\n",
       "         [ 0.1320, -0.3626,  0.3505, -0.6611, -0.3372, -0.0751, -0.6180,\n",
       "          -0.1649],\n",
       "         [ 0.0452, -0.2877,  0.5693,  0.2835, -0.7601, -0.0223, -0.4313,\n",
       "          -0.6734]],\n",
       "\n",
       "        [[ 0.0931, -0.4798,  0.2598, -0.5001, -0.7527, -0.5814, -0.2482,\n",
       "           0.2701],\n",
       "         [-0.0728, -0.3731, -0.4027, -0.5964, -0.6825, -0.0618, -0.2307,\n",
       "           0.0309],\n",
       "         [-0.0941, -0.8932, -0.3718, -0.2343, -0.3230,  0.0775,  0.7399,\n",
       "          -0.5631],\n",
       "         [-0.5147,  0.3004,  0.5731, -1.9222, -1.0152, -0.3250, -0.0385,\n",
       "           0.6244],\n",
       "         [ 0.0736, -1.0037,  0.2922,  0.3342, -0.0909, -0.0559,  0.1755,\n",
       "          -0.6386]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_q(q) equals to q * w_q.wieght + bias\n",
    "assert torch.allclose(w_q(q), torch.matmul(q, w_q.weight.T) + w_q.bias), \"w_q(q) doesn't equl to q*w_q^T + bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert torch.allclose(w_q(q), q @ w_q.weight.T), 'they are different'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If w = nn.Linear(d_model, d_model, bias=False) then they will be the same\n",
    "w_test = nn.Linear(d_model, d_model, bias=False)\n",
    "assert torch.allclose(w_test(q), q @ w_test.weight.T), 'they are different'\n",
    "\n",
    "# q @ w_q^T, @ is matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = w_q(q)\n",
    "key = w_k(k)\n",
    "value = w_v(v)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 2, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the batch size and seq_len, decompose the matrix into smaller matrix\n",
    "# so we can give each small matrix different head\n",
    "# later, we can perform independent attention calculations for each head\n",
    "query.view(query.shape[0], query.shape[1], h, d_k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 5, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The reason for using (batch_size, h, seq_len, d_k) instead of (batch_size, seq_len, h, d_k) is that:\n",
    "\n",
    "The attention mechanism operates independently across heads.\n",
    "Each head will perform self-attention on its own set of d_k-dimensional vectors.\n",
    "The typical implementation of multi-head attention expects the shape (batch_size, h, seq_len, d_k).\n",
    "When performing matrix multiplications (like query @ key.T), we want to apply attention per head.\n",
    "This format is easier for batched computations in PyTorch and TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "query.view(query.shape[0], query.shape[1], h, d_k).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.view(query.shape[0], query.shape[1], h, d_k).transpose(1, 2)\n",
    "key = key.view(key.shape[0], key.shape[1], h, d_k).transpose(1, 2)\n",
    "value = value.view(value.shape[0], value.shape[1], h, d_k).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.1674, -0.5020, -0.1665,  0.1320,  0.0452],\n",
       "          [ 0.1943,  0.2736,  0.7116, -0.3626, -0.2877],\n",
       "          [ 0.5684, -0.0067,  0.1747,  0.3505,  0.5693],\n",
       "          [ 0.2572, -0.9714, -1.2294, -0.6611,  0.2835]],\n",
       "\n",
       "         [[-0.1927, -0.4892, -0.5832, -0.3372, -0.7601],\n",
       "          [ 1.0251,  0.3171,  0.3117, -0.0751, -0.0223],\n",
       "          [-0.3273,  0.2813, -0.3098, -0.6180, -0.4313],\n",
       "          [-0.6617, -0.5930, -0.8411, -0.1649, -0.6734]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0931, -0.0728, -0.0941, -0.5147,  0.0736],\n",
       "          [-0.4798, -0.3731, -0.8932,  0.3004, -1.0037],\n",
       "          [ 0.2598, -0.4027, -0.3718,  0.5731,  0.2922],\n",
       "          [-0.5001, -0.5964, -0.2343, -1.9222,  0.3342]],\n",
       "\n",
       "         [[-0.7527, -0.6825, -0.3230, -1.0152, -0.0909],\n",
       "          [-0.5814, -0.0618,  0.0775, -0.3250, -0.0559],\n",
       "          [-0.2482, -0.2307,  0.7399, -0.0385,  0.1755],\n",
       "          [ 0.2701,  0.0309, -0.5631,  0.6244, -0.6386]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.7099e-01, -1.2054e-01,  3.6468e-01,  8.3725e-01, -5.4520e-02],\n",
       "          [-4.9558e-02, -1.2945e-01,  2.4530e-01,  3.5562e-02, -4.2703e-01],\n",
       "          [-2.2668e-01, -2.9068e-01,  3.0124e-01, -1.0333e-01, -6.7126e-01],\n",
       "          [-2.2781e-01, -2.2902e-01, -2.0074e-01, -2.0480e-01, -2.0287e-01],\n",
       "          [-1.4794e-02, -1.6970e-01, -1.6247e-01,  1.5496e-01,  8.4350e-02]],\n",
       "\n",
       "         [[-3.3616e-02,  1.9560e-02, -3.8386e-02,  5.2151e-01,  3.2878e-01],\n",
       "          [-1.6372e-01, -1.1551e-02,  1.6483e-02,  1.5859e-01,  1.2499e-01],\n",
       "          [-2.3306e-01,  3.2176e-03, -1.9670e-02, -5.1747e-02,  1.6120e-01],\n",
       "          [-6.4701e-02,  1.1616e-02,  5.8313e-02, -2.9886e-01, -1.8842e-02],\n",
       "          [-2.4203e-01,  6.9530e-05,  5.6568e-02, -3.3081e-01,  3.9251e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4054e-01, -2.5582e-01, -3.6789e-01, -3.1680e-01, -8.2524e-03],\n",
       "          [ 1.1425e-01, -5.9002e-02, -4.5706e-02,  6.2097e-02, -9.6488e-02],\n",
       "          [ 1.7350e-01, -2.7463e-01,  5.0624e-03,  8.6905e-03,  4.2149e-02],\n",
       "          [ 3.1926e-02, -1.0044e-01, -9.9784e-01, -7.4327e-01, -3.6185e-01],\n",
       "          [ 1.5957e-01, -4.6838e-01, -1.4926e-01, -2.8818e-01,  2.1968e-01]],\n",
       "\n",
       "         [[ 1.3341e-01,  3.6460e-01,  1.7284e-01,  5.0939e-01, -1.8372e-01],\n",
       "          [ 8.8407e-02,  2.9403e-01,  1.3278e-01,  3.7661e-01, -4.3770e-02],\n",
       "          [-1.3649e-01,  3.5916e-01,  3.4134e-01,  2.9308e-01,  3.0412e-01],\n",
       "          [-1.3194e-02,  4.4835e-01,  2.7106e-01,  4.9817e-01, -2.1145e-01],\n",
       "          [ 5.5544e-02,  1.7014e-01,  1.2077e-01,  2.1557e-01,  2.0822e-01]]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score = (query @ key.transpose(-2 ,-1)) / math.sqrt(d_k)\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Dropout Applied:\n",
      " [[ 2.  4.  6.]\n",
      " [ 8. 10.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def numpy_dropout(x, p=0.5):\n",
    "    \"\"\"Applies dropout using NumPy with probability p\"\"\"\n",
    "    keep_prob = 1 - p\n",
    "    mask = np.random.binomial(n=1, p=keep_prob, size=x.shape)  # Create Bernoulli mask\n",
    "    return (x * mask) / keep_prob  # Scale to maintain expectation\n",
    "\n",
    "# Example input matrix\n",
    "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Apply dropout with probability p = 0.5\n",
    "dropout_output = numpy_dropout(x, p=0.5)\n",
    "\n",
    "print(\"Input Tensor:\\n\", x)\n",
    "print(\"Dropout Applied:\\n\", dropout_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide by 1-p for maintaining the expectation\n",
    "\n",
    "test this by multiple runs as follows, we can see the value is nearly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Value of Input Tensor: 3.5\n",
      "Expected Value After Dropout (Averaged over 10000 runs): 3.5075\n"
     ]
    }
   ],
   "source": [
    "def numpy_dropout(x, p=0.5):\n",
    "    \"\"\"Applies dropout using NumPy with probability p\"\"\"\n",
    "    keep_prob = 1 - p\n",
    "    mask = np.random.binomial(n=1, p=keep_prob, size=x.shape)  # Create Bernoulli mask\n",
    "    return (x * mask) / keep_prob  # Scale to maintain expectation\n",
    "\n",
    "# Example input matrix\n",
    "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Compute expectation of the input\n",
    "expected_input = np.mean(x)\n",
    "\n",
    "# Run dropout multiple times and compute mean expectation\n",
    "num_trials = 10000\n",
    "expected_dropout_values = [np.mean(numpy_dropout(x, p=0.5)) for _ in range(num_trials)]\n",
    "\n",
    "# Compute the overall expectation across trials\n",
    "expected_dropout = np.mean(expected_dropout_values)\n",
    "\n",
    "print(\"Expected Value of Input Tensor:\", expected_input)\n",
    "print(\"Expected Value After Dropout (Averaged over {} runs): {:.4f}\".format(num_trials, expected_dropout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.1 0.5 0.9]\n",
      "Bernoulli Samples: [0 3 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def numpy_bernoulli(probabilities):\n",
    "    \"\"\"Simulates torch.bernoulli using NumPy.\"\"\"\n",
    "    return np.random.binomial(n=3, p=probabilities)\n",
    "\n",
    "# Example probability tensor (same as in PyTorch example)\n",
    "probabilities = np.array([0.1, 0.5, 0.9])\n",
    "\n",
    "# Generate Bernoulli samples\n",
    "samples = numpy_bernoulli(probabilities)\n",
    "\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Bernoulli Samples:\", samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Token  ID\n",
      "0    [UNK]   0\n",
      "1    [SOS]   2\n",
      "2     this  12\n",
      "3     test   8\n",
      "4    [PAD]   1\n",
      "5   banana   6\n",
      "6    hello   7\n",
      "7        ,   4\n",
      "8    [EOS]   3\n",
      "9        .   5\n",
      "10      is  11\n",
      "11       a  10\n",
      "12       !   9\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Hello world!\",\n",
    "    \"Hello ChatGPT!\",\n",
    "    \"This is a test sentence.\",\n",
    "    \"Hello, this is another test.\",\n",
    "    \"Test your tokenizer with a sample text.\",\n",
    "    \"Banana, banana, BANANA\"\n",
    "]\n",
    "\n",
    "# Initialize a WordLevel tokenizer\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set normalization (lowercasing)\n",
    "tokenizer.normalizer = Lowercase()\n",
    "\n",
    "# Set pre-tokenizer (whitespace splitting)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Trainer with min_frequency=2\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "vocab_df = pd.DataFrame(vocab.items(), columns=[\"Token\", \"ID\"])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vocab_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', '[UNK]', '!'],\n",
       " ['hello', '[UNK]', '!'],\n",
       " ['this', 'is', 'a', 'test', '[UNK]', '.'],\n",
       " ['hello', ',', 'this', 'is', '[UNK]', 'test', '.'],\n",
       " ['test', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '.'],\n",
       " ['banana', ',', 'banana', ',', 'banana']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [tokenizer.encode(sentence).tokens for sentence in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " '[UNK]',\n",
       " '!',\n",
       " 'hello',\n",
       " '[UNK]',\n",
       " '!',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " '[UNK]',\n",
       " '.',\n",
       " 'hello',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " '[UNK]',\n",
       " 'test',\n",
       " '.',\n",
       " 'test',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '.',\n",
       " 'banana',\n",
       " ',',\n",
       " 'banana',\n",
       " ',',\n",
       " 'banana']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the list of tokenized words\n",
    "flat_tokenized_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "flat_tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Token",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "8363b20d-9614-4763-9fe8-af3a729818ea",
       "rows": [
        [
         "0",
         "hello",
         "3"
        ],
        [
         "1",
         "[UNK]",
         "9"
        ],
        [
         "2",
         "!",
         "2"
        ],
        [
         "3",
         "this",
         "2"
        ],
        [
         "4",
         "is",
         "2"
        ],
        [
         "5",
         "a",
         "2"
        ],
        [
         "6",
         "test",
         "3"
        ],
        [
         "7",
         ".",
         "3"
        ],
        [
         "8",
         ",",
         "3"
        ],
        [
         "9",
         "banana",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>,</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>banana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Token  Count\n",
       "0   hello      3\n",
       "1   [UNK]      9\n",
       "2       !      2\n",
       "3    this      2\n",
       "4      is      2\n",
       "5       a      2\n",
       "6    test      3\n",
       "7       .      3\n",
       "8       ,      3\n",
       "9  banana      3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the occurrences of each word in the vocabulary\n",
    "word_counts = Counter(flat_tokenized_words)\n",
    "word_count_df = pd.DataFrame(word_counts.items(), columns=[\"Token\", \"Count\"])\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  101, 2009, 2003, 1037, 2204, 2154,  102,    1,    2,    2,    2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tokens\n",
    "sos_token = torch.tensor([0])  # Start of Sequence <SOS>\n",
    "eos_token = torch.tensor([1])  # End of Sequence <EOS>\n",
    "pad_token = 2                  # Padding token\n",
    "\n",
    "# Example encoded tokens\n",
    "encoded_input_tokens = torch.tensor([101, 2009, 2003, 1037, 2204, 2154, 102])  # Main sequence\n",
    "encoded_num_padding_tokens = 3  # Number of padding tokens\n",
    "\n",
    "# Construct the encoded input using torch.cat()\n",
    "encoded_input = torch.cat([\n",
    "    sos_token,  \n",
    "    encoded_input_tokens,  \n",
    "    eos_token,  \n",
    "    torch.tensor([pad_token] * encoded_num_padding_tokens, dtype=torch.int64)\n",
    "])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded_input != pad_token).unsqueeze(0).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "a is decoded_input, so there is no eos\n",
    "\"\"\"\n",
    "decoder_input = torch.tensor([0, 12, 13, 2 , 2]) # the real input is 2: 0 is SOS, 2 is pad\n",
    "# b = torch.tensor([2])\n",
    "(decoder_input != pad_token).int() # False = 1: means the input rather thant pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build causal_mask with torch.ones\n",
    "causal_mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)\n",
    "\n",
    "# Use '&' to make the causal mask align with the length of the input sentence\n",
    "(decoder_input != pad_token).int() & (causal_mask == 0).type(torch.int) # Convert the 1 to 0, 0 to 1: align with the (a != b).int(), 1 means the real input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, num_samples=5, seq_len=6, pad_token=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # Fake tokenized sentences with different lengths\n",
    "        self.data = [\n",
    "            [101, 7592, 2088, 102],  # \"Hello world\"\n",
    "            [2054, 2024, 2017, 102],  # \"How are you?\"\n",
    "            [1045, 2572, 2986, 102],  # \"I am fine\"\n",
    "            [2748, 102],  # \"Yes\"\n",
    "            [2204, 2872, 102]  # \"Good morning\"\n",
    "        ]\n",
    "        \n",
    "        # Padding each sequence to `seq_len`\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i] += [self.pad_token] * (self.seq_len - len(self.data[i]))  # Padding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simulated encoder input (source sentence) and decoder input (target sentence)\n",
    "        encoded_input = torch.tensor(self.data[idx], dtype=torch.int64)  # (seq_len)\n",
    "        decoded_input = torch.tensor(self.data[idx], dtype=torch.int64)  # (seq_len)\n",
    "        \n",
    "        # Create masks\n",
    "        encoder_mask = (encoded_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()  # (1, 1, seq_len)\n",
    "        decoder_mask = (decoded_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & self.causal_mask(decoded_input.size(0))  # (1, seq_len, seq_len)\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": encoded_input,  # (seq_len)\n",
    "            \"decoder_input\": decoded_input,  # (seq_len)\n",
    "            \"encoder_mask\": encoder_mask,  # (1, 1, seq_len)\n",
    "            \"decoder_mask\": decoder_mask,  # (1, seq_len, seq_len)\n",
    "            \"label\": decoded_input,  # (seq_len)\n",
    "        }\n",
    "    \n",
    "    def causal_mask(self, size):\n",
    "        \"\"\" Generate a causal mask for the decoder to prevent attending future tokens \"\"\"\n",
    "        mask = torch.tril(torch.ones((size, size), dtype=torch.int))  # Lower triangular matrix\n",
    "        return mask.unsqueeze(0)  # (1, seq_len, seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the tensor before calculating loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain the shape transformation in the following:\n",
    "\n",
    "```\n",
    "(batch_size, seq_len, tgt_vocab_size) -> (batch_size * seq_len, tgt_vocab_size)  \n",
    "loss = loss_fn(project_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input shape of the loss function, CrossEntroyLoss, is: (N, C):\n",
    "- N is the number of samples (each token in this case).\n",
    "- C is the number of classes (vocabulary size in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output shape of project_layer is: (batch_size, seq_len, tgt_vocab_size).  \n",
    "example is following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.1, 0.2, 0.3, 0.1, 0.3],\n",
       "  [0.5, 0.2, 0.1, 0.1, 0.1],\n",
       "  [0.2, 0.3, 0.1, 0.3, 0.1]],\n",
       " [[0.3, 0.1, 0.2, 0.2, 0.2],\n",
       "  [0.4, 0.1, 0.1, 0.2, 0.2],\n",
       "  [0.1, 0.3, 0.2, 0.2, 0.2]]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[   \n",
    "    # three words(or tokens, actually) in the sentence 1\n",
    "    [[0.1, 0.2, 0.3, 0.1, 0.3],  # Token 1, Sentence 1\n",
    "     [0.5, 0.2, 0.1, 0.1, 0.1],  # Token 2, Sentence 1\n",
    "     [0.2, 0.3, 0.1, 0.3, 0.1]], # Token 3, Sentence 1\n",
    "\n",
    "    # sentence 2\n",
    "    [[0.3, 0.1, 0.2, 0.2, 0.2],  # Token 1, Sentence 2\n",
    "     [0.4, 0.1, 0.1, 0.2, 0.2],  # Token 2, Sentence 2\n",
    "     [0.1, 0.3, 0.2, 0.2, 0.2]]  # Token 3, Sentence 2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reshape to batch_size * seq_len, tgt_vocab_size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1, 0.2, 0.3, 0.1, 0.3],\n",
       " [0.5, 0.2, 0.1, 0.1, 0.1],\n",
       " [0.2, 0.3, 0.1, 0.3, 0.1],\n",
       " [0.3, 0.1, 0.2, 0.2, 0.2],\n",
       " [0.4, 0.1, 0.1, 0.2, 0.2],\n",
       " [0.1, 0.3, 0.2, 0.2, 0.2]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    [0.1, 0.2, 0.3, 0.1, 0.3],  # Token 1, Sentence 1\n",
    "    [0.5, 0.2, 0.1, 0.1, 0.1],  # Token 2, Sentence 1\n",
    "    [0.2, 0.3, 0.1, 0.3, 0.1],  # Token 3, Sentence 1\n",
    "    [0.3, 0.1, 0.2, 0.2, 0.2],  # Token 1, Sentence 2\n",
    "    [0.4, 0.1, 0.1, 0.2, 0.2],  # Token 2, Sentence 2\n",
    "    [0.1, 0.3, 0.2, 0.2, 0.2]   # Token 3, Sentence 2\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.tensor([\n",
    "    [2, 0, 1],  # Ground-truth token indices for Sentence 1 (each value represents the correct class index for the corresponding token)\n",
    "    [4, 3, 1]   # Ground-truth token indices for Sentence 2\n",
    "])  # Shape: (2, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 1, 4, 3, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.tensor v.s. torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.1418e+20,  1.9285e-01])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(2) # 2 is treated as a shape, not a value, so it creates a tensor of shape (2,) with randomly initialized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.tensor(2).requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
