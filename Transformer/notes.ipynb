{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0366e+00, 1.0746e+00, 1.1140e+00, 1.1548e+00, 1.1971e+00,\n",
       "        1.2409e+00, 1.2864e+00, 1.3335e+00, 1.3824e+00, 1.4330e+00, 1.4855e+00,\n",
       "        1.5399e+00, 1.5963e+00, 1.6548e+00, 1.7154e+00, 1.7783e+00, 1.8434e+00,\n",
       "        1.9110e+00, 1.9810e+00, 2.0535e+00, 2.1288e+00, 2.2067e+00, 2.2876e+00,\n",
       "        2.3714e+00, 2.4582e+00, 2.5483e+00, 2.6416e+00, 2.7384e+00, 2.8387e+00,\n",
       "        2.9427e+00, 3.0505e+00, 3.1623e+00, 3.2781e+00, 3.3982e+00, 3.5227e+00,\n",
       "        3.6517e+00, 3.7855e+00, 3.9242e+00, 4.0679e+00, 4.2170e+00, 4.3714e+00,\n",
       "        4.5316e+00, 4.6976e+00, 4.8697e+00, 5.0481e+00, 5.2330e+00, 5.4247e+00,\n",
       "        5.6234e+00, 5.8294e+00, 6.0430e+00, 6.2643e+00, 6.4938e+00, 6.7317e+00,\n",
       "        6.9783e+00, 7.2339e+00, 7.4989e+00, 7.7737e+00, 8.0584e+00, 8.3536e+00,\n",
       "        8.6596e+00, 8.9769e+00, 9.3057e+00, 9.6466e+00, 1.0000e+01, 1.0366e+01,\n",
       "        1.0746e+01, 1.1140e+01, 1.1548e+01, 1.1971e+01, 1.2409e+01, 1.2864e+01,\n",
       "        1.3335e+01, 1.3824e+01, 1.4330e+01, 1.4855e+01, 1.5399e+01, 1.5963e+01,\n",
       "        1.6548e+01, 1.7154e+01, 1.7783e+01, 1.8434e+01, 1.9110e+01, 1.9810e+01,\n",
       "        2.0535e+01, 2.1288e+01, 2.2067e+01, 2.2876e+01, 2.3714e+01, 2.4582e+01,\n",
       "        2.5483e+01, 2.6416e+01, 2.7384e+01, 2.8387e+01, 2.9427e+01, 3.0505e+01,\n",
       "        3.1623e+01, 3.2781e+01, 3.3982e+01, 3.5227e+01, 3.6517e+01, 3.7855e+01,\n",
       "        3.9242e+01, 4.0679e+01, 4.2170e+01, 4.3714e+01, 4.5316e+01, 4.6976e+01,\n",
       "        4.8697e+01, 5.0481e+01, 5.2330e+01, 5.4247e+01, 5.6234e+01, 5.8294e+01,\n",
       "        6.0430e+01, 6.2643e+01, 6.4938e+01, 6.7317e+01, 6.9783e+01, 7.2339e+01,\n",
       "        7.4989e+01, 7.7737e+01, 8.0584e+01, 8.3536e+01, 8.6596e+01, 8.9769e+01,\n",
       "        9.3057e+01, 9.6466e+01, 1.0000e+02, 1.0366e+02, 1.0746e+02, 1.1140e+02,\n",
       "        1.1548e+02, 1.1971e+02, 1.2409e+02, 1.2864e+02, 1.3335e+02, 1.3824e+02,\n",
       "        1.4330e+02, 1.4855e+02, 1.5399e+02, 1.5963e+02, 1.6548e+02, 1.7154e+02,\n",
       "        1.7783e+02, 1.8434e+02, 1.9110e+02, 1.9810e+02, 2.0535e+02, 2.1288e+02,\n",
       "        2.2067e+02, 2.2876e+02, 2.3714e+02, 2.4582e+02, 2.5483e+02, 2.6416e+02,\n",
       "        2.7384e+02, 2.8387e+02, 2.9427e+02, 3.0505e+02, 3.1623e+02, 3.2781e+02,\n",
       "        3.3982e+02, 3.5227e+02, 3.6517e+02, 3.7855e+02, 3.9242e+02, 4.0679e+02,\n",
       "        4.2170e+02, 4.3714e+02, 4.5316e+02, 4.6976e+02, 4.8697e+02, 5.0481e+02,\n",
       "        5.2330e+02, 5.4247e+02, 5.6234e+02, 5.8294e+02, 6.0430e+02, 6.2643e+02,\n",
       "        6.4938e+02, 6.7317e+02, 6.9783e+02, 7.2339e+02, 7.4989e+02, 7.7737e+02,\n",
       "        8.0584e+02, 8.3536e+02, 8.6596e+02, 8.9769e+02, 9.3057e+02, 9.6466e+02,\n",
       "        1.0000e+03, 1.0366e+03, 1.0746e+03, 1.1140e+03, 1.1548e+03, 1.1971e+03,\n",
       "        1.2409e+03, 1.2864e+03, 1.3335e+03, 1.3824e+03, 1.4330e+03, 1.4855e+03,\n",
       "        1.5399e+03, 1.5963e+03, 1.6548e+03, 1.7154e+03, 1.7783e+03, 1.8434e+03,\n",
       "        1.9110e+03, 1.9810e+03, 2.0535e+03, 2.1288e+03, 2.2067e+03, 2.2876e+03,\n",
       "        2.3714e+03, 2.4582e+03, 2.5483e+03, 2.6416e+03, 2.7384e+03, 2.8387e+03,\n",
       "        2.9427e+03, 3.0505e+03, 3.1623e+03, 3.2781e+03, 3.3982e+03, 3.5227e+03,\n",
       "        3.6517e+03, 3.7855e+03, 3.9242e+03, 4.0679e+03, 4.2170e+03, 4.3714e+03,\n",
       "        4.5316e+03, 4.6976e+03, 4.8697e+03, 5.0481e+03, 5.2330e+03, 5.4247e+03,\n",
       "        5.6234e+03, 5.8294e+03, 6.0430e+03, 6.2643e+03, 6.4938e+03, 6.7317e+03,\n",
       "        6.9783e+03, 7.2339e+03, 7.4989e+03, 7.7737e+03, 8.0584e+03, 8.3536e+03,\n",
       "        8.6596e+03, 8.9769e+03, 9.3057e+03, 9.6466e+03])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "torch.pow(torch.tensor(10000.0), torch.arange(0, d_model, 2).float() / d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.,  16.,  18.,  20.,  22.,\n",
       "         24.,  26.,  28.,  30.,  32.,  34.,  36.,  38.,  40.,  42.,  44.,  46.,\n",
       "         48.,  50.,  52.,  54.,  56.,  58.,  60.,  62.,  64.,  66.,  68.,  70.,\n",
       "         72.,  74.,  76.,  78.,  80.,  82.,  84.,  86.,  88.,  90.,  92.,  94.,\n",
       "         96.,  98., 100., 102., 104., 106., 108., 110., 112., 114., 116., 118.,\n",
       "        120., 122., 124., 126., 128., 130., 132., 134., 136., 138., 140., 142.,\n",
       "        144., 146., 148., 150., 152., 154., 156., 158., 160., 162., 164., 166.,\n",
       "        168., 170., 172., 174., 176., 178., 180., 182., 184., 186., 188., 190.,\n",
       "        192., 194., 196., 198., 200., 202., 204., 206., 208., 210., 212., 214.,\n",
       "        216., 218., 220., 222., 224., 226., 228., 230., 232., 234., 236., 238.,\n",
       "        240., 242., 244., 246., 248., 250., 252., 254., 256., 258., 260., 262.,\n",
       "        264., 266., 268., 270., 272., 274., 276., 278., 280., 282., 284., 286.,\n",
       "        288., 290., 292., 294., 296., 298., 300., 302., 304., 306., 308., 310.,\n",
       "        312., 314., 316., 318., 320., 322., 324., 326., 328., 330., 332., 334.,\n",
       "        336., 338., 340., 342., 344., 346., 348., 350., 352., 354., 356., 358.,\n",
       "        360., 362., 364., 366., 368., 370., 372., 374., 376., 378., 380., 382.,\n",
       "        384., 386., 388., 390., 392., 394., 396., 398., 400., 402., 404., 406.,\n",
       "        408., 410., 412., 414., 416., 418., 420., 422., 424., 426., 428., 430.,\n",
       "        432., 434., 436., 438., 440., 442., 444., 446., 448., 450., 452., 454.,\n",
       "        456., 458., 460., 462., 464., 466., 468., 470., 472., 474., 476., 478.,\n",
       "        480., 482., 484., 486., 488., 490., 492., 494., 496., 498., 500., 502.,\n",
       "        504., 506., 508., 510.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(0, d_model, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.,   3.,   5.,   7.,   9.,  11.,  13.,  15.,  17.,  19.,  21.,  23.,\n",
       "         25.,  27.,  29.,  31.,  33.,  35.,  37.,  39.,  41.,  43.,  45.,  47.,\n",
       "         49.,  51.,  53.,  55.,  57.,  59.,  61.,  63.,  65.,  67.,  69.,  71.,\n",
       "         73.,  75.,  77.,  79.,  81.,  83.,  85.,  87.,  89.,  91.,  93.,  95.,\n",
       "         97.,  99., 101., 103., 105., 107., 109., 111., 113., 115., 117., 119.,\n",
       "        121., 123., 125., 127., 129., 131., 133., 135., 137., 139., 141., 143.,\n",
       "        145., 147., 149., 151., 153., 155., 157., 159., 161., 163., 165., 167.,\n",
       "        169., 171., 173., 175., 177., 179., 181., 183., 185., 187., 189., 191.,\n",
       "        193., 195., 197., 199., 201., 203., 205., 207., 209., 211., 213., 215.,\n",
       "        217., 219., 221., 223., 225., 227., 229., 231., 233., 235., 237., 239.,\n",
       "        241., 243., 245., 247., 249., 251., 253., 255., 257., 259., 261., 263.,\n",
       "        265., 267., 269., 271., 273., 275., 277., 279., 281., 283., 285., 287.,\n",
       "        289., 291., 293., 295., 297., 299., 301., 303., 305., 307., 309., 311.,\n",
       "        313., 315., 317., 319., 321., 323., 325., 327., 329., 331., 333., 335.,\n",
       "        337., 339., 341., 343., 345., 347., 349., 351., 353., 355., 357., 359.,\n",
       "        361., 363., 365., 367., 369., 371., 373., 375., 377., 379., 381., 383.,\n",
       "        385., 387., 389., 391., 393., 395., 397., 399., 401., 403., 405., 407.,\n",
       "        409., 411., 413., 415., 417., 419., 421., 423., 425., 427., 429., 431.,\n",
       "        433., 435., 437., 439., 441., 443., 445., 447., 449., 451., 453., 455.,\n",
       "        457., 459., 461., 463., 465., 467., 469., 471., 473., 475., 477., 479.,\n",
       "        481., 483., 485., 487., 489., 491., 493., 495., 497., 499., 501., 503.,\n",
       "        505., 507., 509., 511.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(1, d_model, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "# Using torch.exp\n",
    "exp_values = torch.exp(torch.arange(0, d_model, 2).float() * (math.log(10000.0) / d_model))\n",
    "\n",
    "# Using torch.pow\n",
    "pow_values = torch.pow(torch.tensor(10000.0), torch.arange(0, d_model, 2).float() / d_model)\n",
    "\n",
    "# Check if both are the same\n",
    "print(torch.allclose(exp_values, pow_values))  # Should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 // 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask in Transfomer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., -inf],\n",
       "        [5., 6., -inf, -inf]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten_scores = torch.tensor([[1, 2, 3, 4],\n",
    "                            [5, 6, 7, 8]]).float()\n",
    "\n",
    "pad_mask = torch.tensor([[1, 1, 1, 0],\n",
    "                         [1, 1, 0, 0]])\n",
    "atten_scores.masked_fill(pad_mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the mask change the value in unwanted position to -inf, the value will become 0 after passing softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 1., 1., 1.],\n",
       "         [0., 0., 1., 1., 1.],\n",
       "         [0., 0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 5\n",
    "# diagonal = 0 : default\n",
    "# diagonal > 0 (up to length) : shift to upper\n",
    "# diagonal < 0 (up to length) : shift to lower\n",
    "# diagonal: the step toward up/down \n",
    "causal_mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1) # torch.tril is lower triangular matrix\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False, False]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False: allow attention\n",
    "# True: Masked(prevent future access)\n",
    "causal_mask.bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(causal_mask == 0).type(torch.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $W_q \\times Q$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8  # Model embedding size\n",
    "h = 2\n",
    "d_k = d_model // h\n",
    "q = torch.randn(batch_size, seq_len, d_model)\n",
    "k = torch.randn(batch_size, seq_len, d_model)\n",
    "v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=8, out_features=8, bias=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1725, -0.3349, -0.1234, -0.1965, -0.1648,  0.1016,  0.0430,  0.0636],\n",
       "        [-0.1497, -0.1052, -0.3213,  0.3330,  0.2692, -0.0121, -0.1386,  0.0866],\n",
       "        [-0.0665, -0.1916, -0.0689,  0.3433, -0.1171, -0.1061, -0.1027,  0.1022],\n",
       "        [-0.0952, -0.0881,  0.1007,  0.2028, -0.0345,  0.1558, -0.3399, -0.3178],\n",
       "        [-0.0326, -0.0659, -0.0958, -0.0571, -0.3104, -0.2467,  0.3229, -0.2918],\n",
       "        [ 0.3287, -0.0809,  0.1528, -0.2151, -0.1976, -0.0709,  0.2346, -0.0394],\n",
       "        [-0.2604, -0.2229,  0.3502, -0.1222,  0.0841, -0.2988, -0.3391,  0.0222],\n",
       "        [-0.3278, -0.2860, -0.0623, -0.2128,  0.2085, -0.1262,  0.0303, -0.3206]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2998, -0.0377,  0.0533, -0.1341, -0.1464,  0.2658,  0.2902,  0.3178],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5235,  1.0046, -1.7567, -0.9155, -0.3476,  1.5067,  1.5777,\n",
       "           0.5178],\n",
       "         [ 0.0852, -0.0361,  1.3978, -0.2881,  1.2564, -0.6577, -0.2225,\n",
       "          -0.4758],\n",
       "         [-0.8826,  0.1328, -0.1501, -0.8849, -0.5831, -0.6050, -0.6082,\n",
       "           1.2217],\n",
       "         [-0.0983, -0.5606,  1.0783,  2.4776,  1.4231, -1.6117,  0.6679,\n",
       "          -0.0258],\n",
       "         [-1.0718, -1.7601, -0.6874, -0.9611,  1.0130, -0.4155,  2.1231,\n",
       "           0.2980]],\n",
       "\n",
       "        [[-1.0091,  1.0115, -0.7801, -0.2126,  0.4783,  1.0585,  0.6611,\n",
       "           1.4349],\n",
       "         [ 2.3659,  0.2244,  1.1176,  0.6590, -0.4338, -0.6494,  0.7721,\n",
       "          -1.6527],\n",
       "         [ 0.5162,  0.4680, -2.1609, -0.5484, -0.2649, -1.6963, -0.2651,\n",
       "          -0.9882],\n",
       "         [-0.9993, -0.9777, -2.2083,  0.4416,  0.6509, -1.2840,  1.1547,\n",
       "           0.5408],\n",
       "         [ 2.1088, -0.7736, -0.4159, -0.9555,  1.2482,  0.7222,  0.9700,\n",
       "           1.3866]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 5.8082e-01, -9.0997e-02, -5.2589e-01, -9.8945e-01,  1.1944e-01,\n",
       "           2.5246e-01, -1.3038e+00,  1.2532e-01],\n",
       "         [-1.0295e-01, -2.5597e-01, -2.4384e-01,  2.4362e-02, -4.2495e-01,\n",
       "           3.3734e-01,  1.1679e+00,  7.6521e-01],\n",
       "         [ 3.8163e-01, -1.2559e-01,  1.1287e-01, -5.1200e-01, -2.8421e-01,\n",
       "           9.9676e-02,  9.1111e-01,  3.1137e-01],\n",
       "         [-5.2050e-01,  8.2216e-01,  8.7650e-01,  1.6614e-02, -1.7179e-01,\n",
       "          -9.8481e-02,  8.8974e-01,  4.4470e-01],\n",
       "         [ 8.7899e-01,  2.1805e-01, -8.2876e-02, -1.0571e+00,  5.1197e-01,\n",
       "           4.7323e-01,  3.3434e-01,  1.6523e+00]],\n",
       "\n",
       "        [[ 7.3410e-02,  3.3545e-01, -1.8223e-01, -7.8119e-01, -7.0820e-01,\n",
       "          -2.9223e-01, -3.8831e-01, -2.0813e-02],\n",
       "         [ 2.9909e-01, -9.1436e-01, -1.2630e-01,  4.3661e-02,  6.4358e-01,\n",
       "           1.4324e+00, -2.0592e-01, -1.8702e-01],\n",
       "         [ 4.0360e-01,  2.4773e-01,  2.7218e-02, -4.0428e-01,  7.4775e-01,\n",
       "           3.3475e-01, -8.5667e-02,  7.3348e-01],\n",
       "         [ 4.8679e-01,  1.1488e+00,  6.0752e-01, -8.7257e-01,  4.6677e-01,\n",
       "          -2.0404e-01, -1.9291e-04,  1.1279e+00],\n",
       "         [ 1.1593e+00, -1.4366e-01, -4.1862e-01, -1.2033e+00, -7.2671e-01,\n",
       "           1.0385e+00, -5.2442e-01, -1.6898e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_q(q) equals to q * w_q.wieght + bias\n",
    "assert torch.allclose(w_q(q), torch.matmul(q, w_q.weight.T) + w_q.bias), \"w_q(q) doesn't equl to q*w_q^T + bias\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert torch.allclose(w_q(q), q @ w_q.weight.T), 'they are different'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If w = nn.Linear(d_model, d_model, bias=False) then they will be the same\n",
    "w_test = nn.Linear(d_model, d_model, bias=False)\n",
    "assert torch.allclose(w_test(q), q @ w_test.weight.T), 'they are different'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 8])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = w_q(q)\n",
    "key = w_k(k)\n",
    "value = w_v(v)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 2, 4])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the batch size and seq_len, decompose the matrix into smaller matrix\n",
    "# so we can give each small matrix different head\n",
    "# later, we can perform independent attention calculations for each head\n",
    "query.view(query.shape[0], query.shape[1], h, d_k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 5, 4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The reason for using (batch_size, h, seq_len, d_k) instead of (batch_size, seq_len, h, d_k) is that:\n",
    "\n",
    "The attention mechanism operates independently across heads.\n",
    "Each head will perform self-attention on its own set of d_k-dimensional vectors.\n",
    "The typical implementation of multi-head attention expects the shape (batch_size, h, seq_len, d_k).\n",
    "When performing matrix multiplications (like query @ key.T), we want to apply attention per head.\n",
    "This format is easier for batched computations in PyTorch and TensorFlow.\n",
    "\"\"\"\n",
    "\n",
    "query.view(query.shape[0], query.shape[1], h, d_k).transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.view(query.shape[0], query.shape[1], h, d_k).transpose(1, 2)\n",
    "key = key.view(key.shape[0], key.shape[1], h, d_k).transpose(1, 2)\n",
    "value = value.view(value.shape[0], value.shape[1], h, d_k).transpose(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 5.8082e-01, -1.0295e-01,  3.8163e-01, -5.2050e-01,  8.7899e-01],\n",
       "          [-9.0997e-02, -2.5597e-01, -1.2559e-01,  8.2216e-01,  2.1805e-01],\n",
       "          [-5.2589e-01, -2.4384e-01,  1.1287e-01,  8.7650e-01, -8.2876e-02],\n",
       "          [-9.8945e-01,  2.4362e-02, -5.1200e-01,  1.6614e-02, -1.0571e+00]],\n",
       "\n",
       "         [[ 1.1944e-01, -4.2495e-01, -2.8421e-01, -1.7179e-01,  5.1197e-01],\n",
       "          [ 2.5246e-01,  3.3734e-01,  9.9676e-02, -9.8481e-02,  4.7323e-01],\n",
       "          [-1.3038e+00,  1.1679e+00,  9.1111e-01,  8.8974e-01,  3.3434e-01],\n",
       "          [ 1.2532e-01,  7.6521e-01,  3.1137e-01,  4.4470e-01,  1.6523e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 7.3410e-02,  2.9909e-01,  4.0360e-01,  4.8679e-01,  1.1593e+00],\n",
       "          [ 3.3545e-01, -9.1436e-01,  2.4773e-01,  1.1488e+00, -1.4366e-01],\n",
       "          [-1.8223e-01, -1.2630e-01,  2.7218e-02,  6.0752e-01, -4.1862e-01],\n",
       "          [-7.8119e-01,  4.3661e-02, -4.0428e-01, -8.7257e-01, -1.2033e+00]],\n",
       "\n",
       "         [[-7.0820e-01,  6.4358e-01,  7.4775e-01,  4.6677e-01, -7.2671e-01],\n",
       "          [-2.9223e-01,  1.4324e+00,  3.3475e-01, -2.0404e-01,  1.0385e+00],\n",
       "          [-3.8831e-01, -2.0592e-01, -8.5667e-02, -1.9291e-04, -5.2442e-01],\n",
       "          [-2.0813e-02, -1.8702e-01,  7.3348e-01,  1.1279e+00, -1.6898e-01]]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7203, -0.3064,  0.4109,  0.0666, -0.1609],\n",
       "          [-0.1691,  0.0144,  0.0799,  0.0180, -0.0891],\n",
       "          [-0.3267, -0.0824,  0.0505,  0.0658, -0.1116],\n",
       "          [ 0.7380,  0.1746, -0.3156, -0.1843,  0.2521],\n",
       "          [-0.5215, -0.3346,  0.2616,  0.0733, -0.0519]],\n",
       "\n",
       "         [[ 0.0248,  0.4642,  0.0299,  0.0727,  0.1342],\n",
       "          [-0.3030, -0.2166,  0.3387,  0.3365,  0.2946],\n",
       "          [-0.1177, -0.2484,  0.1473,  0.1265,  0.1060],\n",
       "          [-0.2052, -0.3469,  0.1523,  0.1412,  0.0807],\n",
       "          [-1.0858,  0.4589,  0.6272,  0.8261,  0.4979]]],\n",
       "\n",
       "\n",
       "        [[[-0.0701, -0.1955, -0.1165, -0.1708,  0.0830],\n",
       "          [ 0.1233, -0.0567, -0.3200, -0.1809, -0.0901],\n",
       "          [ 0.0708, -0.2932, -0.2034, -0.0876,  0.1254],\n",
       "          [ 0.0525, -0.8598, -0.4889,  0.0127,  0.2267],\n",
       "          [ 0.2080, -0.5737, -0.5914, -0.4694,  0.2601]],\n",
       "\n",
       "         [[-0.3492,  0.8303,  0.8058,  0.3499,  0.1072],\n",
       "          [ 0.3669, -0.6335, -0.4048, -0.2592, -0.4588],\n",
       "          [ 0.3281, -0.4490, -0.5174, -0.2934, -0.3056],\n",
       "          [ 0.2593, -0.1185, -0.3484, -0.1918, -0.1274],\n",
       "          [-0.0688,  0.7102,  0.8395,  0.3241, -0.1291]]]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_score = (query @ key.transpose(-2 ,-1)) / math.sqrt(d_k)\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Dropout Applied:\n",
      " [[ 0.  0.  6.]\n",
      " [ 0. 10. 12.]]\n"
     ]
    }
   ],
   "source": [
    "def numpy_dropout(x, p=0.5):\n",
    "    \"\"\"Applies dropout using NumPy with probability p\"\"\"\n",
    "    keep_prob = 1 - p\n",
    "    mask = np.random.binomial(n=1, p=keep_prob, size=x.shape)  # Create Bernoulli mask\n",
    "    return (x * mask) / keep_prob  # Scale to maintain expectation\n",
    "\n",
    "# Example input matrix\n",
    "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Apply dropout with probability p = 0.5\n",
    "dropout_output = numpy_dropout(x, p=0.5)\n",
    "\n",
    "print(\"Input Tensor:\\n\", x)\n",
    "print(\"Dropout Applied:\\n\", dropout_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide by 1-p for maintaining the expectation\n",
    "\n",
    "test this by multiple runs as follows, we can see the value is nearly the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Value of Input Tensor: 3.5\n",
      "Expected Value After Dropout (Averaged over 10000 runs): 3.5052\n"
     ]
    }
   ],
   "source": [
    "def numpy_dropout(x, p=0.5):\n",
    "    \"\"\"Applies dropout using NumPy with probability p\"\"\"\n",
    "    keep_prob = 1 - p\n",
    "    mask = np.random.binomial(n=1, p=keep_prob, size=x.shape)  # Create Bernoulli mask\n",
    "    return (x * mask) / keep_prob  # Scale to maintain expectation\n",
    "\n",
    "# Example input matrix\n",
    "x = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Compute expectation of the input\n",
    "expected_input = np.mean(x)\n",
    "\n",
    "# Run dropout multiple times and compute mean expectation\n",
    "num_trials = 10000\n",
    "expected_dropout_values = [np.mean(numpy_dropout(x, p=0.5)) for _ in range(num_trials)]\n",
    "\n",
    "# Compute the overall expectation across trials\n",
    "expected_dropout = np.mean(expected_dropout_values)\n",
    "\n",
    "print(\"Expected Value of Input Tensor:\", expected_input)\n",
    "print(\"Expected Value After Dropout (Averaged over {} runs): {:.4f}\".format(num_trials, expected_dropout))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: [0.1 0.5 0.9]\n",
      "Bernoulli Samples: [2 1 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def numpy_bernoulli(probabilities):\n",
    "    \"\"\"Simulates torch.bernoulli using NumPy.\"\"\"\n",
    "    return np.random.binomial(n=3, p=probabilities)\n",
    "\n",
    "# Example probability tensor (same as in PyTorch example)\n",
    "probabilities = np.array([0.1, 0.5, 0.9])\n",
    "\n",
    "# Generate Bernoulli samples\n",
    "samples = numpy_bernoulli(probabilities)\n",
    "\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Bernoulli Samples:\", samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Token  ID\n",
      "0        a  10\n",
      "1    [UNK]   0\n",
      "2       is  11\n",
      "3    [EOS]   3\n",
      "4        ,   4\n",
      "5     this  12\n",
      "6     test   8\n",
      "7        .   5\n",
      "8        !   9\n",
      "9    [PAD]   1\n",
      "10  banana   6\n",
      "11   hello   7\n",
      "12   [SOS]   2\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Hello world!\",\n",
    "    \"Hello ChatGPT!\",\n",
    "    \"This is a test sentence.\",\n",
    "    \"Hello, this is another test.\",\n",
    "    \"Test your tokenizer with a sample text.\",\n",
    "    \"Banana, banana, BANANA\"\n",
    "]\n",
    "\n",
    "# Initialize a WordLevel tokenizer\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set normalization (lowercasing)\n",
    "tokenizer.normalizer = Lowercase()\n",
    "\n",
    "# Set pre-tokenizer (whitespace splitting)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Trainer with min_frequency=2\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "# Train tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "vocab_df = pd.DataFrame(vocab.items(), columns=[\"Token\", \"ID\"])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vocab_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', '[UNK]', '!'],\n",
       " ['hello', '[UNK]', '!'],\n",
       " ['this', 'is', 'a', 'test', '[UNK]', '.'],\n",
       " ['hello', ',', 'this', 'is', '[UNK]', 'test', '.'],\n",
       " ['test', '[UNK]', '[UNK]', '[UNK]', 'a', '[UNK]', '[UNK]', '.'],\n",
       " ['banana', ',', 'banana', ',', 'banana']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [tokenizer.encode(sentence).tokens for sentence in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " '[UNK]',\n",
       " '!',\n",
       " 'hello',\n",
       " '[UNK]',\n",
       " '!',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " '[UNK]',\n",
       " '.',\n",
       " 'hello',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " '[UNK]',\n",
       " 'test',\n",
       " '.',\n",
       " 'test',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " 'a',\n",
       " '[UNK]',\n",
       " '[UNK]',\n",
       " '.',\n",
       " 'banana',\n",
       " ',',\n",
       " 'banana',\n",
       " ',',\n",
       " 'banana']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the list of tokenized words\n",
    "flat_tokenized_words = [word for sentence in tokenized_corpus for word in sentence]\n",
    "flat_tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Token",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "c260fcf1-c643-4c3b-9120-d40c73de7dc7",
       "rows": [
        [
         "0",
         "hello",
         "3"
        ],
        [
         "1",
         "[UNK]",
         "9"
        ],
        [
         "2",
         "!",
         "2"
        ],
        [
         "3",
         "this",
         "2"
        ],
        [
         "4",
         "is",
         "2"
        ],
        [
         "5",
         "a",
         "2"
        ],
        [
         "6",
         "test",
         "3"
        ],
        [
         "7",
         ".",
         "3"
        ],
        [
         "8",
         ",",
         "3"
        ],
        [
         "9",
         "banana",
         "3"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>,</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>banana</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Token  Count\n",
       "0   hello      3\n",
       "1   [UNK]      9\n",
       "2       !      2\n",
       "3    this      2\n",
       "4      is      2\n",
       "5       a      2\n",
       "6    test      3\n",
       "7       .      3\n",
       "8       ,      3\n",
       "9  banana      3"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the occurrences of each word in the vocabulary\n",
    "word_counts = Counter(flat_tokenized_words)\n",
    "word_count_df = pd.DataFrame(word_counts.items(), columns=[\"Token\", \"Count\"])\n",
    "word_count_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,  101, 2009, 2003, 1037, 2204, 2154,  102,    1,    2,    2,    2])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define tokens\n",
    "sos_token = torch.tensor([0])  # Start of Sequence <SOS>\n",
    "eos_token = torch.tensor([1])  # End of Sequence <EOS>\n",
    "pad_token = 2                  # Padding token\n",
    "\n",
    "# Example encoded tokens\n",
    "encoded_input_tokens = torch.tensor([101, 2009, 2003, 1037, 2204, 2154, 102])  # Main sequence\n",
    "encoded_num_padding_tokens = 3  # Number of padding tokens\n",
    "\n",
    "# Construct the encoded input using torch.cat()\n",
    "encoded_input = torch.cat([\n",
    "    sos_token,  \n",
    "    encoded_input_tokens,  \n",
    "    eos_token,  \n",
    "    torch.tensor([pad_token] * encoded_num_padding_tokens, dtype=torch.int64)\n",
    "])\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded_input != pad_token).unsqueeze(0).unsqueeze(0).int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoder_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "a is decoded_input, so there is no eos\n",
    "\"\"\"\n",
    "decoder_input = torch.tensor([0, 12, 13, 2 , 2]) # the real input is 2: 0 is SOS, 2 is pad\n",
    "# b = torch.tensor([2])\n",
    "(decoder_input != pad_token).int() # False = 1: means the input rather thant pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0, 0, 0],\n",
       "         [1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build causal_mask with torch.ones\n",
    "causal_mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)\n",
    "\n",
    "# Use '&' to make the causal mask align with the length of the input sentence\n",
    "(decoder_input != pad_token).int() & (causal_mask == 0).type(torch.int) # Convert the 1 to 0, 0 to 1: align with the (a != b).int(), 1 means the real input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, num_samples=5, seq_len=6, pad_token=0):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # Fake tokenized sentences with different lengths\n",
    "        self.data = [\n",
    "            [101, 7592, 2088, 102],  # \"Hello world\"\n",
    "            [2054, 2024, 2017, 102],  # \"How are you?\"\n",
    "            [1045, 2572, 2986, 102],  # \"I am fine\"\n",
    "            [2748, 102],  # \"Yes\"\n",
    "            [2204, 2872, 102]  # \"Good morning\"\n",
    "        ]\n",
    "        \n",
    "        # Padding each sequence to `seq_len`\n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i] += [self.pad_token] * (self.seq_len - len(self.data[i]))  # Padding\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Simulated encoder input (source sentence) and decoder input (target sentence)\n",
    "        encoded_input = torch.tensor(self.data[idx], dtype=torch.int64)  # (seq_len)\n",
    "        decoded_input = torch.tensor(self.data[idx], dtype=torch.int64)  # (seq_len)\n",
    "        \n",
    "        # Create masks\n",
    "        encoder_mask = (encoded_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()  # (1, 1, seq_len)\n",
    "        decoder_mask = (decoded_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & self.causal_mask(decoded_input.size(0))  # (1, seq_len, seq_len)\n",
    "        \n",
    "        return {\n",
    "            \"encoder_input\": encoded_input,  # (seq_len)\n",
    "            \"decoder_input\": decoded_input,  # (seq_len)\n",
    "            \"encoder_mask\": encoder_mask,  # (1, 1, seq_len)\n",
    "            \"decoder_mask\": decoder_mask,  # (1, seq_len, seq_len)\n",
    "            \"label\": decoded_input,  # (seq_len)\n",
    "        }\n",
    "    \n",
    "    def causal_mask(self, size):\n",
    "        \"\"\" Generate a causal mask for the decoder to prevent attending future tokens \"\"\"\n",
    "        mask = torch.tril(torch.ones((size, size), dtype=torch.int))  # Lower triangular matrix\n",
    "        return mask.unsqueeze(0)  # (1, seq_len, seq_len)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
