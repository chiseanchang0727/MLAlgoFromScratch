{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toy input: Batch of 1 image, 3 channels (RGB), 8x8 size\n",
    "n_samples = 1\n",
    "in_channels = 3  # RGB image\n",
    "img_size = 8\n",
    "patch_size = 4\n",
    "embed_dim = 6  # Output embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2633, -1.8969,  1.4743, -0.3435, -0.2999, -1.3671,  0.0964,\n",
       "            0.4794],\n",
       "          [-0.3527,  0.2987, -2.4611,  1.0021,  0.8559, -0.1230, -1.3567,\n",
       "           -0.0323],\n",
       "          [-0.2216,  2.0662, -2.3393, -0.4756,  0.9114, -0.7342, -0.9184,\n",
       "            0.4454],\n",
       "          [-0.7126, -0.3743,  1.1080,  0.1548, -0.0492,  0.2147,  0.6575,\n",
       "           -0.1207],\n",
       "          [-1.7058, -0.2490, -0.9394, -0.2000,  1.0010, -0.1594,  1.5799,\n",
       "            0.1848],\n",
       "          [ 0.0182, -0.1107, -0.2300, -0.5372,  0.4597, -0.9512,  2.5565,\n",
       "           -0.1111],\n",
       "          [ 1.3373, -0.9067,  0.9606, -1.1488,  0.7076,  0.2331,  0.9773,\n",
       "           -0.7673],\n",
       "          [-0.3334, -0.6814, -0.9837, -0.3038, -1.0723,  1.3645, -1.9279,\n",
       "           -2.3701]],\n",
       "\n",
       "         [[ 2.1932,  0.9822,  1.3177, -1.6895,  0.4818,  1.3684,  0.6193,\n",
       "           -1.0710],\n",
       "          [ 0.2484,  1.4562,  1.3519, -1.2914,  0.8432,  1.6109, -1.9271,\n",
       "           -1.0391],\n",
       "          [-0.5147,  0.1961, -0.5115, -0.6465, -0.0077, -0.4649, -0.0874,\n",
       "           -1.1058],\n",
       "          [ 0.2624,  1.5810, -0.8454, -0.9677,  0.3574, -1.2429,  0.1227,\n",
       "            0.1958],\n",
       "          [ 1.1579, -0.6686, -0.7327,  1.3757, -1.3271,  0.6897,  2.2359,\n",
       "            0.1051],\n",
       "          [ 1.2699, -0.3200, -0.5644, -0.3007,  0.7321,  0.4728,  0.2415,\n",
       "           -0.4844],\n",
       "          [-1.0636, -0.6714,  1.5653, -0.4951,  1.5848, -0.1176, -0.6332,\n",
       "           -1.1868],\n",
       "          [ 0.9579, -0.2585, -1.2965, -1.4039, -0.4792, -0.1769, -1.1111,\n",
       "            0.6946]],\n",
       "\n",
       "         [[-1.2503, -0.6326,  0.0565, -0.0302,  0.8577, -0.4602, -0.1055,\n",
       "            0.0285],\n",
       "          [ 0.2919,  0.6791, -1.1897,  0.4645,  1.1355, -0.9029,  0.6437,\n",
       "           -0.9763],\n",
       "          [ 1.8586, -1.5563, -1.7419, -0.2842,  1.3163, -1.0072,  0.8270,\n",
       "            0.5661],\n",
       "          [-1.0397,  0.4980,  0.7529, -0.5925,  0.6955, -0.7672,  0.2032,\n",
       "            0.8212],\n",
       "          [-0.6121, -0.2361,  0.4671,  0.2898,  0.0640, -0.2690,  0.3208,\n",
       "           -0.4508],\n",
       "          [ 0.9808,  0.3355, -0.7164,  1.0586, -0.1643, -0.0061, -0.6684,\n",
       "            0.0334],\n",
       "          [ 1.1158,  0.5195, -1.0194,  0.2693, -0.1120,  0.1289,  1.0373,\n",
       "           -0.1197],\n",
       "          [-0.2948,  0.4263,  0.4727, -0.6934,  1.2470, -0.7111, -1.7371,\n",
       "           -1.7087]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy image tensor (batch_size=1, channels=3, height=8, width=8)\n",
    "x = torch.randn(n_samples, in_channels, img_size, img_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(4, 4), stride=(4, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PatchEmbedding layer\n",
    "patch_embedding = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    embed_dim,\n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size\n",
    ")\n",
    "patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj = patch_embedding(x)\n",
    "x_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.1967, -0.4240],\n",
       "          [ 0.2409,  0.3412]],\n",
       "\n",
       "         [[ 0.2370, -0.0734],\n",
       "          [ 0.2239, -0.7524]],\n",
       "\n",
       "         [[ 1.0445,  0.0903],\n",
       "          [-0.2994, -0.4388]],\n",
       "\n",
       "         [[ 0.4680, -0.8679],\n",
       "          [ 0.3407, -0.9931]],\n",
       "\n",
       "         [[ 0.8672,  0.6471],\n",
       "          [-0.0216,  0.4335]],\n",
       "\n",
       "         [[ 0.8862, -0.2435],\n",
       "          [-0.8852,  0.4248]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten = x_proj.flatten(2) # merge the the dimension of 2 and 3 into a single dimension\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1967,  0.2370,  1.0445,  0.4680,  0.8672,  0.8862],\n",
       "         [-0.4240, -0.0734,  0.0903, -0.8679,  0.6471, -0.2435],\n",
       "         [ 0.2409,  0.2239, -0.2994,  0.3407, -0.0216, -0.8852],\n",
       "         [ 0.3412, -0.7524, -0.4388, -0.9931,  0.4335,  0.4248]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch: `nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.],\n",
       "        [-1.,  7.],\n",
       "        [ 3.,  5.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[0, 4.], [-1, 7], [3, 5]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = nn.LayerNorm(n_features, elementwise_affine=False) #  elementwise_affine=False: no learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm model (or layer).\n",
    "sum(p.numel() for p in layernorm.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm.weight, layernorm.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.mean(-1) # calculate the mean of the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 1.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If unbiased=False, the standard deviation is computed using N (population standard deviation).\n",
    "If unbiased=True, the standard deviation is computed using N-1 (sample standard deviation, also called Besselâ€™s correction).\n",
    "\n",
    "When calculating the standard deviation of a sample, dividing by N-1 corrects the bias in estimating the population standard deviation.\n",
    "This is useful in statistics when working with small sample sizes.\n",
    "\n",
    "When to Use Each?\n",
    "Use unbiased=True (default) when working with samples and need an unbiased estimator of population std.\n",
    "Use unbiased=False when working with the full dataset (population statistics)\n",
    "\"\"\"\n",
    "input.std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.]), tensor([1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies Layer Normalization to the input tensor and then computes the mean along the last dimension.\n",
    "layernorm(input).mean(-1), layernorm(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm2 = nn.LayerNorm(n_features, elementwise_affine=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm2 model (or layer).\n",
    "sum(p.numel() for p in layernorm2.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm2.weight, layernorm2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.], grad_fn=<MeanBackward1>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both have grad_fn, meaning they are part of the computational graph in PyTorch and support autograd.\n",
    "layernorm2(input).mean(-1), layernorm2(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 4      # number of images in a batch\n",
    "n_patches = 10      # number of patches per image\n",
    "embed_dim = 8       # embedding dimension for each patch/token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings shape: torch.Size([4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# Simulate patch embeddings for a batch of images.\n",
    "# Shape: (batch_size, n_patches, embed_dim)\n",
    "patch_embeddings = torch.randn(batch_size, n_patches, embed_dim)\n",
    "print(\"Patch embeddings shape:\", patch_embeddings.shape)  # (4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6950e+00, -7.2475e-01,  6.8917e-01, -1.2845e+00,  1.0524e+00,\n",
       "           7.2389e-01,  5.6165e-01, -9.3830e-01],\n",
       "         [-9.1075e-01, -8.0083e-01,  2.2623e+00,  3.3333e-02, -9.1884e-02,\n",
       "           1.4499e+00,  3.6943e-01,  1.1622e+00],\n",
       "         [-1.4999e+00,  4.4801e-01, -1.3272e+00, -8.3523e-01, -2.6751e-01,\n",
       "           7.2506e-01, -1.3675e+00,  1.8649e-01],\n",
       "         [ 1.1008e+00,  2.8005e-01, -1.6325e-01,  1.5789e+00,  5.5332e-01,\n",
       "          -1.9090e-01, -1.3704e+00, -1.1637e+00],\n",
       "         [ 8.9358e-01, -2.4815e-01,  7.7868e-01,  4.9711e-01, -7.8787e-01,\n",
       "          -1.0976e+00, -8.3230e-01, -3.2411e-01],\n",
       "         [-1.0501e+00,  4.1691e-04,  2.0845e+00, -4.0261e-01,  1.5550e+00,\n",
       "           2.0501e+00,  3.8149e-01, -9.6061e-01],\n",
       "         [-1.0522e+00,  8.1602e-01, -9.9924e-01, -2.4195e+00,  3.5525e-01,\n",
       "          -6.8756e-02,  4.3238e-02,  2.4476e+00],\n",
       "         [-1.1990e-01,  5.2804e-02, -2.7216e-01, -1.0275e+00, -3.4242e-01,\n",
       "          -3.5289e-01,  3.7096e-01, -4.1317e-01],\n",
       "         [ 1.4670e+00,  7.6097e-01,  3.7019e-01,  3.2090e-01,  1.5646e+00,\n",
       "           1.2420e+00, -5.0783e-02,  2.2840e-01],\n",
       "         [-5.1560e-01,  8.3269e-01, -6.1831e-01, -8.3158e-01, -2.9354e-01,\n",
       "          -1.0142e+00,  4.4626e-01, -1.8248e-01]],\n",
       "\n",
       "        [[-6.3177e-01, -6.8414e-01, -5.1004e-01, -1.6650e-01, -1.4217e+00,\n",
       "           1.9281e+00,  1.3704e-02,  1.9272e-01],\n",
       "         [-1.0590e+00,  3.7246e-01,  3.1049e-01,  9.1071e-01,  1.1663e+00,\n",
       "           2.9609e+00,  7.1826e-01,  1.0828e+00],\n",
       "         [ 9.3805e-01,  9.7240e-01, -1.0135e+00, -3.6016e-01,  7.6743e-01,\n",
       "          -1.3781e+00, -8.9244e-01, -2.3745e-01],\n",
       "         [ 7.3243e-01, -5.4427e-02,  4.8796e-01,  1.2948e+00, -1.3411e+00,\n",
       "          -1.2004e+00, -1.6026e+00,  1.0681e+00],\n",
       "         [-7.5978e-01,  1.3907e+00, -6.0069e-01, -6.5680e-01, -8.8618e-01,\n",
       "           1.1910e+00, -3.4702e-02, -3.6719e-01],\n",
       "         [-2.8927e-02,  1.0530e+00,  7.6078e-01, -5.1322e-01, -1.0772e+00,\n",
       "           1.0786e+00,  7.6192e-02,  1.3447e+00],\n",
       "         [ 9.2878e-01, -3.4299e-01,  1.1445e+00, -4.6813e-01,  1.6968e+00,\n",
       "          -1.9639e+00,  9.8836e-01,  1.4270e-01],\n",
       "         [-3.3277e-01,  1.2319e-01, -3.2681e-01,  2.0284e-01,  3.4154e-01,\n",
       "           4.0809e-01, -6.2304e-02,  1.3760e-01],\n",
       "         [-1.4276e-01,  8.7552e-01,  4.4393e-02, -2.0930e-02,  1.0752e+00,\n",
       "          -8.7295e-01, -4.4957e-01, -9.6057e-01],\n",
       "         [-7.9205e-02, -1.2669e+00, -6.0199e-01, -5.5914e-01,  8.8758e-01,\n",
       "           1.4504e-01, -5.2792e-01,  4.3497e-01]],\n",
       "\n",
       "        [[ 1.9493e-01, -1.9763e+00, -1.3974e+00, -9.0459e-01, -1.7681e+00,\n",
       "          -2.1213e+00, -5.1495e-01, -6.7538e-01],\n",
       "         [-1.0512e+00, -1.6777e+00,  4.4631e-02, -1.6057e+00, -2.9961e-01,\n",
       "           1.2430e+00, -3.3792e-01,  5.4143e-01],\n",
       "         [-4.0090e-01,  1.3132e+00,  4.2778e-01, -9.5346e-01, -1.5331e+00,\n",
       "           1.4712e+00,  1.9495e-01,  1.4190e+00],\n",
       "         [ 1.0916e+00, -4.2743e-01, -7.0367e-01,  2.6539e-01,  6.8468e-01,\n",
       "           1.7440e-01,  2.7428e-01,  7.4068e-01],\n",
       "         [-6.8833e-01, -2.3401e+00,  6.0876e-01,  1.0183e+00,  5.9448e-01,\n",
       "           7.9003e-01, -5.6700e-01, -2.7755e-01],\n",
       "         [ 2.2414e-01,  8.1769e-01, -4.7632e-01,  8.0145e-01, -1.5901e+00,\n",
       "          -1.9079e+00,  4.4200e-04,  1.3935e+00],\n",
       "         [ 6.4498e-02,  1.7068e+00,  9.9341e-01,  1.9405e-02,  3.9839e-02,\n",
       "          -7.4246e-01,  1.6776e-01,  1.0620e+00],\n",
       "         [ 6.2869e-01, -2.3422e-01, -1.9170e+00,  7.5229e-01,  4.8749e-02,\n",
       "          -5.4211e-01, -1.5552e-01, -4.5115e-02],\n",
       "         [-2.2659e-01, -2.4784e-01,  2.1105e-01,  3.3310e-01, -5.4588e-01,\n",
       "           5.7430e-01, -6.7340e-02, -5.1832e-01],\n",
       "         [ 9.0194e-01,  2.0025e-01,  3.5186e-02,  6.3615e-01,  1.0301e+00,\n",
       "          -1.3146e+00, -9.8955e-01,  1.9143e-01]],\n",
       "\n",
       "        [[-5.1065e-01,  4.6036e-01,  6.8807e-01, -1.1599e+00, -8.5764e-01,\n",
       "           2.7716e-01, -4.0062e-01, -1.0421e-01],\n",
       "         [ 9.8210e-03, -5.0929e-01,  1.8295e-01,  6.3723e-01,  1.0428e-01,\n",
       "          -5.4473e-01,  1.2297e+00, -9.0696e-01],\n",
       "         [ 5.4364e-01,  5.7642e-01, -2.6188e-01,  3.3545e-01, -1.4898e+00,\n",
       "           1.2106e+00, -8.9603e-01, -8.9146e-02],\n",
       "         [ 2.4052e+00,  1.8728e+00,  4.5489e-02,  2.3361e+00,  1.2470e+00,\n",
       "           3.4584e-01,  3.7153e-01, -1.2445e-01],\n",
       "         [-1.0924e-01, -9.7043e-01, -5.4370e-01, -1.3412e+00,  3.0541e-01,\n",
       "           1.3698e+00,  1.6866e+00, -9.9025e-02],\n",
       "         [-1.2168e+00, -6.0238e-02,  2.4646e+00,  3.2481e+00,  3.9972e-01,\n",
       "          -2.3256e-01, -2.1125e-01, -1.0568e+00],\n",
       "         [-1.1594e+00, -1.0004e+00, -7.3930e-01, -6.5616e-01,  8.1673e-01,\n",
       "           8.4388e-01,  1.0287e+00, -1.0072e+00],\n",
       "         [ 3.3505e-02, -8.5511e-01, -1.1546e+00,  2.9776e+00, -1.5064e+00,\n",
       "          -1.2526e-01, -5.9416e-01,  1.0357e+00],\n",
       "         [ 3.2848e-01,  2.4486e-01,  1.8567e-01, -2.3401e-02, -1.6701e+00,\n",
       "           1.5607e+00,  1.6121e-01,  4.5684e-01],\n",
       "         [ 1.6516e+00, -1.8183e+00, -3.6990e-01,  6.9494e-01,  5.7490e-01,\n",
       "           1.3830e+00, -2.8445e-01,  9.0096e-01]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token shape: torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the learnable classification token (cls_token)\n",
    "# It is defined as (1, 1, embed_dim) meaning:\n",
    "#   1: placeholder for a single token\n",
    "#   1: one token (the classification token itself)\n",
    "#   embed_dim: the token's embedding dimension\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "print(\"cls_token shape:\", cls_token.shape)  # (1, 1, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token # one token (the classification token itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.unsqueeze(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded cls_token shape: torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# In order to prepend the cls_token to every image in the batch,\n",
    "# we expand it along the batch dimension.\n",
    "# This does not create new data; it simply views the same parameter for each item.\n",
    "expanded_cls_token = cls_token.expand(batch_size, -1, -1) # -1 means \"keep the original size for that dimension\"\n",
    "print(\"Expanded cls_token shape:\", expanded_cls_token.shape)  # (4, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1, 8])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.expand(batch_size, -1, -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape after concatenation: torch.Size([4, 11, 8])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the cls_token with the patch embeddings along the token dimension (dim=1)\n",
    "# The resulting tensor shape will be (batch_size, n_patches + 1, embed_dim)\n",
    "\n",
    "# since the cls_token is prepended to the entire batch of image patches, i.e., n_patches + 1 tokens, the dimension of cls_token is embed_dim.\n",
    "tokens = torch.cat([expanded_cls_token, patch_embeddings], dim=1)\n",
    "print(\"Tokens shape after concatenation:\", tokens.shape)  # (4, 11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings shape: torch.Size([11, 8])\n"
     ]
    }
   ],
   "source": [
    "# Then we add a learnable positional embedding to the tokens\n",
    "# Initialize the positional embeddings\n",
    "# Note: The positional embeddings are shared across the batch\n",
    "n_positions = n_patches + 1  # number of tokens\n",
    "positional_embeddings = nn.Parameter(torch.randn(n_positions, embed_dim))\n",
    "print(\"Positional embeddings shape:\", positional_embeddings.shape)  # (11, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.rand() and torch.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.rand()\n",
    "- Generates random numbers from a uniform distribution between 0 and 1.\n",
    "- Every number in the range [0, 1) has an equal probability of being sampled.\n",
    "- Useful when you need random values bounded within a fixed range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2029, 0.4354]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn()\n",
    "- Generates random numbers from a normal (Gaussian) distribution with:\n",
    "    - Mean = 0\n",
    "    - Standard deviation = 1\n",
    "- Values are centered around 0, with both positive and negative values.\n",
    "- Useful for initializing weights in neural networks and sampling from normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0208,  0.0275]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
