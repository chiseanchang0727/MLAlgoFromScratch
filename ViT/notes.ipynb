{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toy input: Batch of 1 image, 3 channels (RGB), 8x8 size\n",
    "n_samples = 1\n",
    "in_channels = 3  # RGB image\n",
    "img_size = 8\n",
    "patch_size = 4\n",
    "embed_dim = 6  # Output embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.2316,  0.2791,  0.6735,  1.4968,  0.0726,  0.2044, -2.5776,\n",
       "            1.4435],\n",
       "          [ 0.3110,  0.9913,  0.2106, -0.0137, -1.6977, -0.7567,  1.0226,\n",
       "            0.4551],\n",
       "          [ 0.8429,  0.5899, -0.5186, -0.5533,  0.0640, -0.9399,  1.0161,\n",
       "            0.5079],\n",
       "          [-0.9939,  0.8087, -0.5042, -0.7606,  0.5438,  0.7888,  0.7569,\n",
       "           -1.1863],\n",
       "          [-0.7801,  0.3140, -0.3803, -0.3533, -0.7400, -0.1064,  0.4969,\n",
       "           -0.5699],\n",
       "          [-0.1939, -1.0817,  0.2256, -0.9048,  0.7253, -0.4405,  0.0091,\n",
       "           -0.0549],\n",
       "          [-0.5883, -0.2510,  0.6320,  0.0982, -1.4043,  0.3159,  0.2555,\n",
       "            0.2755],\n",
       "          [ 0.6407, -0.6237, -2.3314, -0.0574, -0.1351, -0.8191,  0.5546,\n",
       "           -0.7069]],\n",
       "\n",
       "         [[-0.0329,  0.7785, -0.6229,  0.5435, -1.8386, -1.9724,  0.4170,\n",
       "            1.2891],\n",
       "          [ 1.4359, -0.6483, -0.7035,  0.3878,  1.3952,  1.5196, -0.2788,\n",
       "           -2.0343],\n",
       "          [-0.1489, -0.5094,  2.2068,  1.3553, -0.2105,  0.8229, -0.3337,\n",
       "           -0.3277],\n",
       "          [-0.6682, -0.3986, -0.2335, -1.0357, -1.8702, -1.3265, -0.1917,\n",
       "           -0.5067],\n",
       "          [ 0.0669,  1.5322, -0.5480, -0.3481,  0.8954, -0.3225, -1.0267,\n",
       "           -0.0560],\n",
       "          [-0.0936, -1.0188,  0.3231, -0.6589, -2.0459,  0.2228, -1.1101,\n",
       "           -0.2181],\n",
       "          [-0.6753,  1.1909,  0.4749, -0.9044,  3.3087, -0.3807, -1.3909,\n",
       "           -0.0546],\n",
       "          [ 1.9931,  0.1000,  1.4381,  0.2707,  0.6331, -0.2307, -0.4201,\n",
       "            0.7492]],\n",
       "\n",
       "         [[ 1.0808,  1.3300, -0.5330, -0.4318,  0.6045,  1.8231, -1.0082,\n",
       "            0.6480],\n",
       "          [-0.6352, -0.3259, -0.1087,  1.3556, -0.0478,  0.9521, -0.0956,\n",
       "            0.2560],\n",
       "          [ 2.2763, -1.4755,  1.8025,  0.1837, -1.4500,  0.2966,  0.1149,\n",
       "           -0.2881],\n",
       "          [ 1.2789, -0.9432,  0.6729,  0.7922,  0.7954,  0.1835, -0.4743,\n",
       "            1.3262],\n",
       "          [ 0.4063, -1.6695, -0.2130, -0.7842,  0.4408,  0.0713, -1.2608,\n",
       "            0.2676],\n",
       "          [-0.2398,  0.5549, -0.5426,  0.2047, -0.0614, -1.2506,  2.1571,\n",
       "           -1.0704],\n",
       "          [-0.0263, -0.3232,  0.1240,  1.1251, -1.2100, -1.6722, -0.5949,\n",
       "           -2.0490],\n",
       "          [ 0.1020,  0.2685,  0.5396,  1.3023,  0.1081,  1.7296,  0.2940,\n",
       "           -0.0393]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy image tensor (batch_size=1, channels=3, height=8, width=8)\n",
    "x = torch.randn(n_samples, in_channels, img_size, img_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(4, 4), stride=(4, 4))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PatchEmbedding layer\n",
    "patch_embedding = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    embed_dim,\n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size\n",
    ")\n",
    "patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj = patch_embedding(x)\n",
    "x_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.8924,  0.3836],\n",
       "          [ 0.5082,  0.2890]],\n",
       "\n",
       "         [[ 0.5248,  0.5434],\n",
       "          [-0.5994,  0.9152]],\n",
       "\n",
       "         [[ 0.0940, -0.4738],\n",
       "          [ 0.2473,  0.6958]],\n",
       "\n",
       "         [[-0.0423, -0.1050],\n",
       "          [ 0.3391, -0.3166]],\n",
       "\n",
       "         [[ 0.3676,  0.1056],\n",
       "          [-0.5193, -0.1797]],\n",
       "\n",
       "         [[ 0.1127,  0.1437],\n",
       "          [ 0.1190, -0.2127]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten = x_proj.flatten(2) # merge the the dimension of 2 and 3 into a single dimension\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8924,  0.5248,  0.0940, -0.0423,  0.3676,  0.1127],\n",
       "         [ 0.3836,  0.5434, -0.4738, -0.1050,  0.1056,  0.1437],\n",
       "         [ 0.5082, -0.5994,  0.2473,  0.3391, -0.5193,  0.1190],\n",
       "         [ 0.2890,  0.9152,  0.6958, -0.3166, -0.1797, -0.2127]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch: `nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.],\n",
       "        [-1.,  7.],\n",
       "        [ 3.,  5.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[0, 4.], [-1, 7], [3, 5]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = nn.LayerNorm(n_features, elementwise_affine=False) #  elementwise_affine=False: no learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm model (or layer).\n",
    "sum(p.numel() for p in layernorm.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm.weight, layernorm.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.mean(-1) # calculate the mean of the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 1.])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If unbiased=False, the standard deviation is computed using N (population standard deviation).\n",
    "If unbiased=True, the standard deviation is computed using N-1 (sample standard deviation, also called Besselâ€™s correction).\n",
    "\n",
    "When calculating the standard deviation of a sample, dividing by N-1 corrects the bias in estimating the population standard deviation.\n",
    "This is useful in statistics when working with small sample sizes.\n",
    "\n",
    "When to Use Each?\n",
    "Use unbiased=True (default) when working with samples and need an unbiased estimator of population std.\n",
    "Use unbiased=False when working with the full dataset (population statistics)\n",
    "\"\"\"\n",
    "input.std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.]), tensor([1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies Layer Normalization to the input tensor and then computes the mean along the last dimension.\n",
    "layernorm(input).mean(-1), layernorm(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm2 = nn.LayerNorm(n_features, elementwise_affine=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm2 model (or layer).\n",
    "sum(p.numel() for p in layernorm2.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm2.weight, layernorm2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.], grad_fn=<MeanBackward1>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both have grad_fn, meaning they are part of the computational graph in PyTorch and support autograd.\n",
    "layernorm2(input).mean(-1), layernorm2(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
