{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toy input: Batch of 1 image, 3 channels (RGB), 8x8 size\n",
    "n_samples = 1\n",
    "in_channels = 3  # RGB image\n",
    "img_size = 8\n",
    "patch_size = 4\n",
    "embed_dim = 6  # Output embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.3085,  2.9301, -0.2634,  0.0959,  0.9889,  0.9827,  0.4778,\n",
       "            0.2156],\n",
       "          [-0.2989, -0.7796, -0.4471,  1.0615, -0.1459,  0.3371, -0.6267,\n",
       "            0.6362],\n",
       "          [ 0.8384,  1.5411,  1.8485,  0.0526, -0.2272, -0.5812,  0.1585,\n",
       "            1.2235],\n",
       "          [ 2.6966,  0.0080, -1.4554, -0.3657,  0.9881, -0.6463,  0.5210,\n",
       "           -0.5178],\n",
       "          [ 1.0496,  1.0235,  0.4265, -0.3547,  0.6718,  1.2416,  0.8128,\n",
       "            1.3974],\n",
       "          [-0.1225,  1.5216,  1.5826, -0.3536,  1.4035,  1.3632, -0.2961,\n",
       "           -0.2238],\n",
       "          [ 0.5618, -1.2004, -0.0455, -0.7475, -1.2903,  0.8388, -0.7475,\n",
       "           -0.8898],\n",
       "          [ 1.1827, -0.6823,  0.3822,  1.0224,  1.6121, -1.5723,  2.4434,\n",
       "           -0.9830]],\n",
       "\n",
       "         [[ 1.8518,  1.2762,  0.7064, -0.2057,  0.9397, -0.6804, -0.0820,\n",
       "           -1.0543],\n",
       "          [ 0.8019,  0.1416, -0.8532,  1.0030,  0.5649, -0.2463,  0.7291,\n",
       "           -1.1164],\n",
       "          [-0.5022,  0.8088,  1.2477,  0.4497, -1.9194, -0.7025,  0.3873,\n",
       "           -1.5690],\n",
       "          [ 1.3219, -0.0921, -1.2628,  0.1926,  0.2263,  2.5132,  0.2616,\n",
       "           -1.4422],\n",
       "          [-1.7245,  0.4580,  0.7391, -0.4864, -0.2223,  0.4626, -2.1147,\n",
       "            1.1582],\n",
       "          [ 0.0677, -0.2029, -1.2595, -0.5483, -0.4739,  1.0610, -1.1762,\n",
       "           -0.3304],\n",
       "          [ 0.1324, -0.1480,  0.0154,  0.3803, -0.0459,  1.6977, -1.0814,\n",
       "            0.3765],\n",
       "          [-1.4191, -0.1678, -2.1812,  0.4412,  0.0076,  1.3743, -0.6207,\n",
       "            0.1849]],\n",
       "\n",
       "         [[-0.5636,  1.4158,  0.4672,  0.4186, -0.2905, -0.3962,  0.0618,\n",
       "           -0.7757],\n",
       "          [ 0.8332, -0.3999,  0.2490,  0.3951, -0.9133, -0.2731,  0.2855,\n",
       "            0.1609],\n",
       "          [ 1.0658, -0.0238, -0.9405, -0.0845,  2.4678,  1.0538,  0.0978,\n",
       "            0.5870],\n",
       "          [-1.2127,  1.1489,  0.6112,  2.0485, -0.4879,  0.4886,  0.7685,\n",
       "           -0.1385],\n",
       "          [-0.6102, -0.1277,  0.8522,  1.3102,  0.8544,  1.4066,  0.9770,\n",
       "           -2.2931],\n",
       "          [ 0.1361,  2.4177, -1.6272,  1.3667, -1.1653,  0.6280, -0.3686,\n",
       "           -0.5148],\n",
       "          [ 0.4372, -0.4674,  0.4276,  0.9632, -0.3182, -1.2767, -0.4163,\n",
       "            1.7170],\n",
       "          [-0.5406, -0.5114, -0.3338, -0.5238,  1.9714, -0.1311, -2.4525,\n",
       "           -0.0422]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy image tensor (batch_size=1, channels=3, height=8, width=8)\n",
    "x = torch.randn(n_samples, in_channels, img_size, img_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(4, 4), stride=(4, 4))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PatchEmbedding layer\n",
    "patch_embedding = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    embed_dim,\n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size\n",
    ")\n",
    "patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj = patch_embedding(x)\n",
    "x_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.6274,  0.3757],\n",
       "          [ 0.4737,  0.6531]],\n",
       "\n",
       "         [[ 0.0518,  0.1551],\n",
       "          [ 0.1834, -0.2474]],\n",
       "\n",
       "         [[-0.1822,  0.5464],\n",
       "          [ 0.7082,  0.6695]],\n",
       "\n",
       "         [[ 0.1504, -0.7843],\n",
       "          [ 0.3537, -0.8814]],\n",
       "\n",
       "         [[-0.0249,  0.5595],\n",
       "          [-1.3911, -1.0484]],\n",
       "\n",
       "         [[ 0.1888,  0.2588],\n",
       "          [ 0.5194,  0.6042]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten = x_proj.flatten(2) # merge the the dimension of 2 and 3 into a single dimension\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6274,  0.0518, -0.1822,  0.1504, -0.0249,  0.1888],\n",
       "         [ 0.3757,  0.1551,  0.5464, -0.7843,  0.5595,  0.2588],\n",
       "         [ 0.4737,  0.1834,  0.7082,  0.3537, -1.3911,  0.5194],\n",
       "         [ 0.6531, -0.2474,  0.6695, -0.8814, -1.0484,  0.6042]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten.transpose(1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
