{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toy input: Batch of 1 image, 3 channels (RGB), 8x8 size\n",
    "n_samples = 1\n",
    "in_channels = 3  # RGB image\n",
    "img_size = 8\n",
    "patch_size = 4\n",
    "embed_dim = 6  # Output embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.9691e-01,  1.1692e+00, -7.5482e-01,  1.4442e+00,  1.6648e+00,\n",
       "            1.4124e+00, -1.3579e+00, -3.1046e-01],\n",
       "          [ 4.0357e-01, -1.4947e+00,  2.2118e-01, -1.4521e-01, -1.7081e+00,\n",
       "           -1.7768e-01, -8.0303e-01, -4.6541e-01],\n",
       "          [-2.3015e-01, -1.6651e-01, -6.9584e-01,  1.7793e+00, -3.2247e-01,\n",
       "           -1.7106e-01,  1.0860e+00, -8.9969e-01],\n",
       "          [-1.3071e-01, -5.7178e-02, -6.1293e-01, -1.5832e-02, -2.7760e-01,\n",
       "            8.5831e-01, -2.0073e+00,  7.9900e-01],\n",
       "          [ 6.9629e-01, -6.6179e-01, -1.5444e+00,  2.1999e+00,  1.6338e+00,\n",
       "           -6.5353e-02,  1.0792e+00,  5.6553e-01],\n",
       "          [-7.6608e-01,  1.0479e+00, -1.9106e-01,  9.7363e-01,  8.7288e-01,\n",
       "            1.2817e-01,  1.1497e+00,  2.5757e+00],\n",
       "          [ 1.9283e-01, -9.0032e-01,  9.7503e-01,  8.6702e-01, -2.5543e+00,\n",
       "           -1.5097e+00,  8.7910e-01, -7.5360e-02],\n",
       "          [-3.8971e-01, -6.9738e-02,  1.3826e+00, -1.6055e+00,  1.1987e+00,\n",
       "            1.6613e-01, -1.0631e+00, -1.0932e+00]],\n",
       "\n",
       "         [[-1.0886e+00, -3.3847e-01, -2.3175e-01, -1.3246e-02,  8.7147e-01,\n",
       "            1.4791e-01,  2.7742e-01, -5.4111e-01],\n",
       "          [ 1.1762e-01,  1.6225e+00,  1.6738e-01,  2.0531e+00,  3.8042e-01,\n",
       "           -1.3251e+00, -9.9009e-02,  8.5238e-02],\n",
       "          [ 1.1480e+00,  3.9528e-01, -1.8274e+00, -1.1075e+00, -6.0305e-02,\n",
       "            9.0136e-01, -4.4792e-01,  1.3568e+00],\n",
       "          [ 1.1712e+00,  4.1440e-01,  1.7950e-01,  8.5387e-01,  5.6469e-01,\n",
       "           -9.9868e-01,  1.0420e+00, -3.3965e-01],\n",
       "          [ 1.9308e-01,  2.5854e-01, -4.2126e-01,  3.4754e-01, -1.4578e+00,\n",
       "           -2.9230e-01,  1.0819e+00, -5.0478e-01],\n",
       "          [-1.1101e-02, -3.2539e-01, -7.3368e-01,  1.6183e+00,  5.7390e-01,\n",
       "            7.2933e-03, -9.3194e-01, -8.2090e-01],\n",
       "          [ 6.1192e-01, -2.0862e-01,  8.2797e-01,  2.8900e-01, -1.2948e+00,\n",
       "            8.9425e-01,  2.2796e-01, -2.1224e-01],\n",
       "          [-3.3648e-01,  4.5638e-01, -5.6306e-01, -3.3551e-01,  1.0958e+00,\n",
       "           -7.3038e-01,  8.8742e-01, -4.1903e-01]],\n",
       "\n",
       "         [[-3.2022e+00,  6.7002e-01,  7.4148e-01, -5.7667e-01,  1.4926e+00,\n",
       "           -1.6803e+00,  9.3563e-01, -1.6771e+00],\n",
       "          [ 1.7838e-01, -7.8760e-01,  6.5504e-01,  2.0265e+00, -7.6355e-01,\n",
       "            1.3415e+00,  8.6439e-01, -2.0435e+00],\n",
       "          [-1.5484e+00, -6.8898e-01,  9.7172e-01,  3.0107e-01,  2.9412e-01,\n",
       "            2.2589e+00, -1.2643e+00,  4.4353e-01],\n",
       "          [ 1.9609e-03,  1.6775e+00, -5.8762e-01,  1.0627e+00,  8.0130e-02,\n",
       "            1.9623e-01, -7.1549e-01,  1.7959e+00],\n",
       "          [ 2.8745e-01, -4.3755e-01,  9.4491e-01,  6.7323e-01,  9.1116e-02,\n",
       "           -1.4871e+00, -7.1910e-01, -9.6772e-01],\n",
       "          [ 6.6010e-01,  1.1551e+00,  9.3227e-01, -1.0032e+00,  7.4349e-01,\n",
       "           -4.0844e-01,  6.1853e-01, -1.2359e+00],\n",
       "          [-7.6515e-01, -7.3039e-01,  1.5155e+00, -1.2053e+00,  1.5222e+00,\n",
       "            6.9415e-01,  6.2363e-01,  1.1089e+00],\n",
       "          [ 1.5738e-01,  1.2854e+00, -6.9475e-01,  8.9151e-01, -8.3753e-01,\n",
       "           -6.7306e-01,  4.6528e-01,  1.1815e+00]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy image tensor (batch_size=1, channels=3, height=8, width=8)\n",
    "x = torch.randn(n_samples, in_channels, img_size, img_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(4, 4), stride=(4, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PatchEmbedding layer\n",
    "patch_embedding = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    embed_dim,\n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size\n",
    ")\n",
    "patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj = patch_embedding(x)\n",
    "x_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0823, -0.0094],\n",
       "          [ 0.1171,  0.0246]],\n",
       "\n",
       "         [[-0.2331,  0.0752],\n",
       "          [ 0.4173,  0.0734]],\n",
       "\n",
       "         [[-0.3296, -0.2042],\n",
       "          [-0.0456, -0.3677]],\n",
       "\n",
       "         [[-1.2090,  0.5632],\n",
       "          [ 0.5150,  0.6201]],\n",
       "\n",
       "         [[-0.0713,  0.0829],\n",
       "          [-0.2245, -0.2623]],\n",
       "\n",
       "         [[ 0.0262,  0.7794],\n",
       "          [ 0.2296, -0.7697]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten = x_proj.flatten(2) # merge the the dimension of 2 and 3 into a single dimension\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0823, -0.2331, -0.3296, -1.2090, -0.0713,  0.0262],\n",
       "         [-0.0094,  0.0752, -0.2042,  0.5632,  0.0829,  0.7794],\n",
       "         [ 0.1171,  0.4173, -0.0456,  0.5150, -0.2245,  0.2296],\n",
       "         [ 0.0246,  0.0734, -0.3677,  0.6201, -0.2623, -0.7697]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch: `nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.],\n",
       "        [-1.,  7.],\n",
       "        [ 3.,  5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[0, 4.], [-1, 7], [3, 5]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = nn.LayerNorm(n_features, elementwise_affine=False) #  elementwise_affine=False: no learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm model (or layer).\n",
    "sum(p.numel() for p in layernorm.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm.weight, layernorm.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.mean(-1) # calculate the mean of the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If unbiased=False, the standard deviation is computed using N (population standard deviation).\n",
    "If unbiased=True, the standard deviation is computed using N-1 (sample standard deviation, also called Besselâ€™s correction).\n",
    "\n",
    "When calculating the standard deviation of a sample, dividing by N-1 corrects the bias in estimating the population standard deviation.\n",
    "This is useful in statistics when working with small sample sizes.\n",
    "\n",
    "When to Use Each?\n",
    "Use unbiased=True (default) when working with samples and need an unbiased estimator of population std.\n",
    "Use unbiased=False when working with the full dataset (population statistics)\n",
    "\"\"\"\n",
    "input.std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.]), tensor([1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies Layer Normalization to the input tensor and then computes the mean along the last dimension.\n",
    "layernorm(input).mean(-1), layernorm(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm2 = nn.LayerNorm(n_features, elementwise_affine=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm2 model (or layer).\n",
    "sum(p.numel() for p in layernorm2.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm2.weight, layernorm2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.], grad_fn=<MeanBackward1>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both have grad_fn, meaning they are part of the computational graph in PyTorch and support autograd.\n",
    "layernorm2(input).mean(-1), layernorm2(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 4      # number of images in a batch\n",
    "n_patches = 10      # number of patches per image\n",
    "embed_dim = 8       # embedding dimension for each patch/token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings shape: torch.Size([4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# Simulate patch embeddings for a batch of images.\n",
    "# Shape: (batch_size, n_patches, embed_dim)\n",
    "patch_embeddings = torch.randn(batch_size, n_patches, embed_dim)\n",
    "print(\"Patch embeddings shape:\", patch_embeddings.shape)  # (4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9474e+00,  5.3887e-01,  1.3133e+00, -2.7690e+00,  9.5592e-01,\n",
       "           4.3726e-01,  5.3443e-01, -1.1833e+00],\n",
       "         [ 9.7502e-01,  5.1019e-01,  8.5312e-01, -1.3665e+00,  1.5091e+00,\n",
       "           3.9398e-02, -4.6469e-02, -1.4468e+00],\n",
       "         [ 1.0401e+00,  7.1583e-01,  3.8244e-01,  2.2281e-01, -6.0818e-01,\n",
       "           2.5477e-01, -6.4580e-01, -2.5676e-01],\n",
       "         [ 4.9179e-01,  8.5362e-01, -2.4100e+00, -1.7105e+00, -3.3873e-01,\n",
       "           1.0078e+00, -2.0807e-01,  7.6522e-01],\n",
       "         [-2.7206e-01,  6.1063e-02, -4.2747e-01,  5.7891e-02,  1.9277e+00,\n",
       "           8.3726e-01,  3.9668e-02, -9.4248e-01],\n",
       "         [ 9.0130e-01, -3.4441e-01, -2.1394e+00,  1.3755e-01, -1.5094e+00,\n",
       "           2.2980e+00, -3.9261e-01, -5.2776e-01],\n",
       "         [ 1.6774e-01,  2.0639e-01,  6.8004e-01, -4.5908e-01, -1.3388e+00,\n",
       "           5.9610e-01, -4.7307e-01, -3.0141e+00],\n",
       "         [ 2.4464e-01,  1.0840e+00, -5.7899e-01, -1.3965e+00, -6.8272e-01,\n",
       "           9.1381e-01, -4.6850e-01,  2.1077e-01],\n",
       "         [-4.0890e-01, -6.5346e-01, -9.6858e-01, -1.0332e+00, -1.0029e-01,\n",
       "          -4.6053e-01,  2.4565e-01, -4.1073e-01],\n",
       "         [ 4.1428e-01,  5.7658e-01,  1.5278e+00, -2.6940e-01, -5.6080e-01,\n",
       "          -5.0092e-01,  6.7808e-01,  5.2157e-01]],\n",
       "\n",
       "        [[ 2.3090e-01,  3.9345e-01, -2.3713e-01,  4.9653e-01, -4.8861e-01,\n",
       "           3.8636e-01,  2.3896e+00,  1.2981e+00],\n",
       "         [-6.9779e-01, -4.2334e-01,  1.6357e+00, -1.3697e-01, -4.7907e-01,\n",
       "           4.8421e-02, -3.2092e-01,  7.0222e-01],\n",
       "         [ 3.2994e-02,  1.1630e+00, -7.1812e-01,  1.3041e+00, -6.5890e-01,\n",
       "          -9.5561e-02, -1.9512e+00, -2.0043e+00],\n",
       "         [ 2.2169e+00,  8.4738e-01,  6.7895e-01, -4.4825e-01,  1.0996e-01,\n",
       "          -7.6182e-01, -1.1202e+00,  1.0225e+00],\n",
       "         [ 1.4019e+00, -3.5432e-01,  1.9165e-01, -8.2188e-01, -1.1930e+00,\n",
       "           5.6154e-01, -7.5303e-01,  8.5498e-01],\n",
       "         [ 1.5957e+00, -9.2298e-01,  6.2641e-01,  1.2084e+00,  1.8283e+00,\n",
       "          -7.4385e-02,  6.8571e-01,  4.7118e-01],\n",
       "         [-1.2073e+00, -7.0663e-01,  3.3335e-01,  1.6571e+00,  4.5495e-03,\n",
       "          -9.3358e-03,  5.7164e-02,  1.0241e+00],\n",
       "         [-2.1910e-01,  2.1963e-01, -1.6350e+00,  2.2351e-01, -1.0597e+00,\n",
       "           8.5204e-01,  7.7632e-01,  1.6711e+00],\n",
       "         [-1.8314e+00, -2.8548e-01,  1.6283e+00,  7.3590e-01, -7.6748e-01,\n",
       "          -7.1920e-01,  1.0809e+00, -9.0340e-01],\n",
       "         [ 1.6180e+00, -5.6978e-01, -2.7653e-01, -5.9027e-01, -1.7717e+00,\n",
       "          -3.9507e-01, -1.0553e+00, -3.7676e-01]],\n",
       "\n",
       "        [[ 9.4822e-01,  1.4294e-01,  2.1925e-01, -1.2366e+00, -1.7692e+00,\n",
       "          -1.6732e-01, -1.3117e+00,  4.1229e-01],\n",
       "         [-4.9477e-01, -9.7001e-01, -7.9691e-01, -2.2320e-01,  5.2949e-02,\n",
       "          -4.3681e-01,  1.1180e+00,  1.1061e+00],\n",
       "         [ 1.8879e+00,  3.6857e-01, -1.2195e+00, -8.7087e-01,  4.1109e-02,\n",
       "          -6.1457e-01,  6.6796e-01,  8.6810e-01],\n",
       "         [ 1.0848e+00, -1.7812e-01,  2.5329e-01,  8.9276e-01,  1.2363e+00,\n",
       "           2.9788e-01,  5.0963e-01, -5.7507e-01],\n",
       "         [ 8.9999e-01,  9.1383e-01,  7.3054e-01,  1.1314e+00,  1.2109e+00,\n",
       "          -1.4346e-01,  4.4637e-01, -9.2809e-01],\n",
       "         [ 6.4350e-01, -8.2026e-01, -5.9372e-01, -2.1310e+00,  4.2208e-01,\n",
       "           1.1784e+00, -5.3479e-01, -5.3882e-03],\n",
       "         [ 1.8215e+00,  1.5679e+00,  5.1483e-02, -4.2528e-02, -4.2845e-01,\n",
       "           5.3669e-01, -1.9893e+00,  2.8827e-01],\n",
       "         [-2.8968e-01, -5.8382e-01,  6.5437e-01, -1.2046e+00, -2.5019e+00,\n",
       "          -1.0893e-01, -3.7012e-01, -2.4710e-01],\n",
       "         [ 1.9055e-03, -1.1482e+00,  7.2441e-01,  1.6286e+00, -1.7296e+00,\n",
       "          -5.4014e-01, -1.2307e+00,  7.9002e-01],\n",
       "         [-4.2038e-01,  6.4627e-02, -4.4849e-01,  9.6893e-01, -3.9016e-01,\n",
       "          -9.7730e-01,  6.3659e-01,  9.9556e-01]],\n",
       "\n",
       "        [[-1.0088e-02,  9.2433e-01, -3.1592e-01,  1.4918e+00,  1.7603e-01,\n",
       "          -1.3769e+00,  4.8279e-01, -1.5138e-01],\n",
       "         [ 1.3442e+00,  5.9037e-01,  1.4394e+00, -5.8784e-01, -8.2363e-01,\n",
       "          -8.0357e-01,  1.0457e-01,  3.6846e-01],\n",
       "         [-5.7221e-01,  3.2805e-02,  5.2190e-01,  1.0802e-01,  1.1249e+00,\n",
       "           5.4637e-02, -1.0569e+00,  4.8062e-01],\n",
       "         [ 4.0090e-01,  3.0196e-01, -1.0521e+00, -7.1459e-01,  5.6664e-01,\n",
       "           1.5630e+00,  1.2698e+00, -8.9522e-02],\n",
       "         [-5.7522e-01, -1.3849e+00, -9.3872e-02,  1.0784e+00,  7.2146e-01,\n",
       "           2.2047e-01,  1.3343e-01,  1.3423e+00],\n",
       "         [ 1.1212e+00,  1.4650e+00,  8.7050e-03,  2.3079e+00,  4.4937e-01,\n",
       "          -1.1086e+00,  1.1198e+00, -2.2020e-01],\n",
       "         [ 9.7637e-01, -3.1417e-01, -1.3442e+00,  1.1900e+00,  9.3897e-01,\n",
       "          -6.6804e-01,  1.5307e-01, -2.2369e-01],\n",
       "         [ 3.0070e-01, -8.1220e-02, -4.7481e-01,  6.6849e-01, -8.5847e-01,\n",
       "          -4.5736e-02,  5.3812e-01,  6.6336e-01],\n",
       "         [-1.5023e-02,  8.2287e-01,  3.9062e-01, -5.8017e-01, -1.2304e+00,\n",
       "          -2.6132e-01,  2.0013e+00, -3.0238e-02],\n",
       "         [-1.1177e+00,  2.6106e-01,  2.4062e-03,  1.0561e+00,  3.0287e-01,\n",
       "          -1.5952e+00, -7.0563e-01, -1.1969e+00]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token shape: torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the learnable classification token (cls_token)\n",
    "# It is defined as (1, 1, embed_dim) meaning:\n",
    "#   1: placeholder for a single token\n",
    "#   1: one token (the classification token itself)\n",
    "#   embed_dim: the token's embedding dimension\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "print(\"cls_token shape:\", cls_token.shape)  # (1, 1, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token # one token (the classification token itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.unsqueeze(3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded cls_token shape: torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# In order to prepend the cls_token to every image in the batch,\n",
    "# we expand it along the batch dimension.\n",
    "# This does not create new data; it simply views the same parameter for each item.\n",
    "expanded_cls_token = cls_token.expand(batch_size, -1, -1) # -1 means \"keep the original size for that dimension\"\n",
    "print(\"Expanded cls_token shape:\", expanded_cls_token.shape)  # (4, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.expand(batch_size, -1, -1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape after concatenation: torch.Size([4, 11, 8])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the cls_token with the patch embeddings along the token dimension (dim=1)\n",
    "# The resulting tensor shape will be (batch_size, n_patches + 1, embed_dim)\n",
    "\n",
    "# since the cls_token is prepended to the entire batch of image patches, i.e., n_patches + 1 tokens, the dimension of cls_token is embed_dim.\n",
    "tokens = torch.cat([expanded_cls_token, patch_embeddings], dim=1)\n",
    "print(\"Tokens shape after concatenation:\", tokens.shape)  # (4, 11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings shape: torch.Size([11, 8])\n"
     ]
    }
   ],
   "source": [
    "# Then we add a learnable positional embedding to the tokens\n",
    "# Initialize the positional embeddings\n",
    "# Note: The positional embeddings are shared across the batch\n",
    "n_positions = n_patches + 1  # number of tokens\n",
    "positional_embeddings = nn.Parameter(torch.randn(n_positions, embed_dim))\n",
    "print(\"Positional embeddings shape:\", positional_embeddings.shape)  # (11, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.rand() and torch.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.rand()\n",
    "- Generates random numbers from a uniform distribution between 0 and 1.\n",
    "- Every number in the range [0, 1) has an equal probability of being sampled.\n",
    "- Useful when you need random values bounded within a fixed range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4325, 0.4274]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand((1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.randn()\n",
    "- Generates random numbers from a normal (Gaussian) distribution with:\n",
    "    - Mean = 0\n",
    "    - Standard deviation = 1\n",
    "- Values are centered around 0, with both positive and negative values.\n",
    "- Useful for initializing weights in neural networks and sampling from normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0177, -0.0786]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((1, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
