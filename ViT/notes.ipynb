{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define toy input: Batch of 1 image, 3 channels (RGB), 8x8 size\n",
    "n_samples = 1\n",
    "in_channels = 3  # RGB image\n",
    "img_size = 8\n",
    "patch_size = 4\n",
    "embed_dim = 6  # Output embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5579,  0.4759,  1.8356,  0.4074,  0.7715, -0.2655,  1.2270,\n",
       "           -1.1819],\n",
       "          [ 2.9807,  0.1846, -0.5290, -1.1848,  1.2755,  1.3569, -1.8161,\n",
       "            1.0120],\n",
       "          [ 0.3811,  1.1015, -1.4335,  2.1419, -1.0955,  0.7276, -0.2383,\n",
       "            0.3690],\n",
       "          [-0.7204,  0.2786, -1.4674,  0.4720,  0.8239, -0.8816, -0.7099,\n",
       "            0.8566],\n",
       "          [ 0.8738, -1.2502, -0.1535, -1.9817,  1.5161,  1.1610,  0.5154,\n",
       "            1.8058],\n",
       "          [ 0.3842, -0.3454,  0.6362,  0.4555, -1.3266,  1.3281,  0.7418,\n",
       "           -0.3243],\n",
       "          [-1.2001, -0.8990,  1.4726,  1.3036,  0.9414,  0.1963, -0.6406,\n",
       "            0.2436],\n",
       "          [ 0.4218,  0.4888, -1.2154, -0.8616,  0.6857,  0.2105, -1.9911,\n",
       "           -1.8040]],\n",
       "\n",
       "         [[-0.4941, -0.2721, -0.6848,  1.4417,  1.8179,  1.3482,  0.6135,\n",
       "           -0.5813],\n",
       "          [-0.4211, -0.5812,  0.1442,  0.4282, -0.1659,  0.2754, -0.3848,\n",
       "            0.6700],\n",
       "          [ 1.4737,  0.0664,  0.8370, -1.0639, -0.0446,  1.8822,  0.7793,\n",
       "            0.6796],\n",
       "          [ 0.0848, -0.7131,  0.6284, -0.3855, -0.3996, -0.1944,  0.1292,\n",
       "            0.1735],\n",
       "          [-1.6684, -1.0149, -0.2204, -0.1932,  0.3207,  1.0338,  1.0147,\n",
       "           -1.9192],\n",
       "          [-1.0818,  1.6032, -1.2984, -1.7006, -0.9274,  1.4920, -0.6414,\n",
       "            1.1525],\n",
       "          [-2.1421, -0.7037,  0.1746,  2.1510,  0.5311,  0.5262,  0.7233,\n",
       "           -1.4924],\n",
       "          [-0.5661,  0.0264, -0.8141,  0.7049,  1.2115, -1.3332,  1.3684,\n",
       "           -0.6038]],\n",
       "\n",
       "         [[-1.3161, -0.2610, -1.0147,  0.8965, -0.6043,  1.3849,  0.2011,\n",
       "            0.2962],\n",
       "          [-0.1266, -0.5177, -0.2337,  0.0965,  0.6417, -0.1702,  0.3959,\n",
       "           -1.9138],\n",
       "          [-1.0518, -0.8709,  0.7891,  0.4217,  0.6846,  0.0392,  1.1127,\n",
       "            0.3915],\n",
       "          [-1.6550, -0.7584,  0.7912, -0.3255, -0.4540,  0.6416,  1.8412,\n",
       "            0.4791],\n",
       "          [-1.3363,  0.5495,  1.1749, -0.3105,  0.9460,  0.0784, -0.2237,\n",
       "           -1.0511],\n",
       "          [-0.3919, -0.8263,  0.4154,  0.0354, -0.5892, -1.3938, -0.0219,\n",
       "            0.1559],\n",
       "          [ 1.4346,  1.9441, -2.4610, -0.3245, -0.0037,  0.5794,  0.9302,\n",
       "           -0.2160],\n",
       "          [-0.6454,  0.9594, -0.6919, -0.0129,  0.8676, -0.6532, -1.1005,\n",
       "            0.3607]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy image tensor (batch_size=1, channels=3, height=8, width=8)\n",
    "x = torch.randn(n_samples, in_channels, img_size, img_size)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 6, kernel_size=(4, 4), stride=(4, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define PatchEmbedding layer\n",
    "patch_embedding = torch.nn.Conv2d(\n",
    "    in_channels,\n",
    "    embed_dim,\n",
    "    kernel_size=patch_size,\n",
    "    stride=patch_size\n",
    ")\n",
    "patch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj = patch_embedding(x)\n",
    "x_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.9781, -1.1017],\n",
       "          [ 0.3744,  0.2382]],\n",
       "\n",
       "         [[ 0.4445, -0.2569],\n",
       "          [ 0.1958, -0.1500]],\n",
       "\n",
       "         [[ 0.7448,  0.5789],\n",
       "          [ 0.3149,  0.8070]],\n",
       "\n",
       "         [[-0.1469,  0.2645],\n",
       "          [ 0.3128, -0.3775]],\n",
       "\n",
       "         [[ 0.1720, -0.5700],\n",
       "          [ 0.2468,  0.1501]],\n",
       "\n",
       "         [[ 0.1384, -0.4763],\n",
       "          [ 0.6453, -0.1547]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten = x_proj.flatten(2) # merge the the dimension of 2 and 3 into a single dimension\n",
    "x_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9781,  0.4445,  0.7448, -0.1469,  0.1720,  0.1384],\n",
       "         [-1.1017, -0.2569,  0.5789,  0.2645, -0.5700, -0.4763],\n",
       "         [ 0.3744,  0.1958,  0.3149,  0.3128,  0.2468,  0.6453],\n",
       "         [ 0.2382, -0.1500,  0.8070, -0.3775,  0.1501, -0.1547]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_flatten.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch: `nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  4.],\n",
       "        [-1.,  7.],\n",
       "        [ 3.,  5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.tensor([[0, 4.], [-1, 7], [3, 5]])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm = nn.LayerNorm(n_features, elementwise_affine=False) #  elementwise_affine=False: no learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm model (or layer).\n",
    "sum(p.numel() for p in layernorm.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm.weight, layernorm.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.mean(-1) # calculate the mean of the last dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "If unbiased=False, the standard deviation is computed using N (population standard deviation).\n",
    "If unbiased=True, the standard deviation is computed using N-1 (sample standard deviation, also called Besselâ€™s correction).\n",
    "\n",
    "When calculating the standard deviation of a sample, dividing by N-1 corrects the bias in estimating the population standard deviation.\n",
    "This is useful in statistics when working with small sample sizes.\n",
    "\n",
    "When to Use Each?\n",
    "Use unbiased=True (default) when working with samples and need an unbiased estimator of population std.\n",
    "Use unbiased=False when working with the full dataset (population statistics)\n",
    "\"\"\"\n",
    "input.std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.]), tensor([1.0000, 1.0000, 1.0000]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies Layer Normalization to the input tensor and then computes the mean along the last dimension.\n",
    "layernorm(input).mean(-1), layernorm(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layernorm2 = nn.LayerNorm(n_features, elementwise_affine=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes the total number of trainable parameters in the layernorm2 model (or layer).\n",
    "sum(p.numel() for p in layernorm2.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm2.weight, layernorm2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.], grad_fn=<MeanBackward1>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both have grad_fn, meaning they are part of the computational graph in PyTorch and support autograd.\n",
    "layernorm2(input).mean(-1), layernorm2(input).std(-1, unbiased=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 4      # number of images in a batch\n",
    "n_patches = 10      # number of patches per image\n",
    "embed_dim = 8       # embedding dimension for each patch/token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings shape: torch.Size([4, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# Simulate patch embeddings for a batch of images.\n",
    "# Shape: (batch_size, n_patches, embed_dim)\n",
    "patch_embeddings = torch.randn(batch_size, n_patches, embed_dim)\n",
    "print(\"Patch embeddings shape:\", patch_embeddings.shape)  # (4,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.9520e-01,  9.1205e-01, -4.5434e-02, -5.9831e-01,  1.6236e-01,\n",
       "           1.0749e+00, -1.4761e+00,  2.2378e+00],\n",
       "         [ 1.4578e+00, -5.9560e-01, -9.2054e-01,  1.5206e+00, -6.8571e-01,\n",
       "           9.9370e-01, -3.3203e-01,  2.0326e-01],\n",
       "         [-1.7800e-01, -1.1020e+00, -1.0643e+00, -4.3989e-02, -5.7469e-01,\n",
       "          -8.7325e-01, -2.7625e-01, -1.9118e+00],\n",
       "         [-9.3121e-01,  6.4652e-01, -7.8528e-01,  6.9302e-01, -1.9613e-01,\n",
       "           2.5961e+00,  3.1525e-01,  1.2773e+00],\n",
       "         [-1.3783e+00, -5.0352e-01, -8.8609e-01, -3.3275e-02, -1.1520e+00,\n",
       "           5.3384e-01,  2.9944e+00,  1.9708e+00],\n",
       "         [-5.7775e-01, -9.0095e-03,  2.1483e+00,  8.6338e-01,  2.1738e+00,\n",
       "           5.8588e-01, -4.5173e-01,  4.7345e-01],\n",
       "         [-9.1210e-01, -1.6091e+00, -1.0199e-01,  1.5419e+00,  8.1252e-01,\n",
       "           1.9598e-01, -2.1218e+00, -1.8783e-01],\n",
       "         [ 6.9087e-01,  1.4726e-01,  1.0598e-01,  1.1172e-01,  1.0222e+00,\n",
       "           4.6652e-02, -8.6382e-01,  1.0793e+00],\n",
       "         [-2.0360e-01,  1.8624e-01,  5.9851e-01, -5.3705e-01,  1.8718e+00,\n",
       "          -5.3375e-01, -9.3472e-01,  2.0719e+00],\n",
       "         [ 1.5342e-01,  1.5651e+00,  1.0941e+00,  8.0819e-01, -6.0415e-01,\n",
       "           2.0926e+00, -1.5114e+00, -6.9260e-01]],\n",
       "\n",
       "        [[-9.3499e-01,  1.3923e+00, -9.3175e-01, -4.1960e-01, -7.7060e-01,\n",
       "          -1.3027e+00,  7.4217e-01,  7.0061e-01],\n",
       "         [-4.2250e-01, -3.0092e-01, -6.0574e-01, -2.4548e-01,  8.3522e-01,\n",
       "          -1.7390e+00,  1.2426e+00, -1.3894e+00],\n",
       "         [-8.7251e-01, -2.0168e-01, -5.2375e-01, -1.4968e-01,  7.9220e-01,\n",
       "          -1.8638e+00, -1.1310e+00,  9.9071e-01],\n",
       "         [ 8.4201e-01, -9.0746e-01,  1.0262e+00, -3.2840e-01, -3.3879e-01,\n",
       "          -1.9500e-01,  9.9193e-01, -5.7356e-01],\n",
       "         [-5.2110e-01, -3.3165e-01,  2.4672e-01,  3.9469e-01,  8.6450e-01,\n",
       "          -1.4829e+00,  2.2842e-01,  1.2138e+00],\n",
       "         [ 2.5892e+00, -1.2432e+00,  2.7225e-01, -1.0974e+00, -7.9414e-01,\n",
       "           1.2981e+00,  1.0026e+00, -1.1261e+00],\n",
       "         [-6.6663e-01, -3.1136e-01, -1.1295e+00,  7.2443e-01,  1.3812e-01,\n",
       "          -5.4075e-01, -1.4222e+00, -4.6936e-01],\n",
       "         [-1.7855e+00, -1.4684e+00, -6.9006e-02,  5.2692e-01,  1.3711e+00,\n",
       "           6.8444e-01,  6.1895e-01,  1.3766e-01],\n",
       "         [-6.5473e-01, -1.4462e+00,  1.1788e-01, -2.1177e+00, -1.5642e-01,\n",
       "           6.5334e-01,  1.9157e+00,  2.8843e-01],\n",
       "         [-2.2905e-01, -5.8215e-01, -1.3680e+00, -6.5566e-01,  6.2802e-01,\n",
       "          -5.3337e-01,  2.6797e-02,  4.2832e-01]],\n",
       "\n",
       "        [[ 1.3042e+00,  1.9520e-01,  8.7355e-01, -2.2702e+00, -3.3873e-01,\n",
       "          -2.5747e-01, -8.1862e-01,  8.9968e-01],\n",
       "         [-3.4981e-01, -9.3528e-01,  1.5609e-01, -5.8196e-01, -4.5422e-01,\n",
       "          -4.4977e-01, -3.6030e-01, -8.3727e-01],\n",
       "         [-1.2756e+00, -1.0078e+00, -1.5982e+00,  2.3127e-01,  8.1574e-02,\n",
       "          -1.3748e+00,  7.3007e-01,  5.5827e-01],\n",
       "         [-1.0645e+00, -1.3873e-01,  1.0533e+00, -9.9447e-01, -1.0609e-01,\n",
       "          -1.4022e+00,  1.7224e+00, -7.0760e-01],\n",
       "         [ 1.7197e+00,  8.5582e-01, -1.0477e+00,  4.3893e-03, -1.0689e+00,\n",
       "           1.6871e+00, -9.3741e-01, -2.0947e-03],\n",
       "         [-1.2332e-01, -8.5738e-01, -1.3482e-01,  8.6958e-02,  4.5386e-01,\n",
       "          -1.5260e-01, -5.3320e-01, -1.0569e-01],\n",
       "         [ 1.1714e+00,  1.2687e-01,  1.2050e+00,  1.8358e+00, -1.3825e+00,\n",
       "          -5.7552e-01,  1.4995e+00, -1.3776e+00],\n",
       "         [ 4.4408e-01,  9.8254e-02,  4.5550e-01,  1.1023e-01,  1.7819e-01,\n",
       "          -1.7429e-01, -2.8191e-01,  1.7055e-01],\n",
       "         [ 3.9310e-01,  2.5694e-01,  1.3956e+00, -1.3598e+00, -7.2179e-01,\n",
       "           1.7362e+00, -1.5973e+00,  6.6734e-01],\n",
       "         [ 4.1295e-01,  9.6404e-01,  4.2732e-01,  1.4033e+00, -1.0266e+00,\n",
       "           2.4617e+00,  8.2325e-01,  1.2148e+00]],\n",
       "\n",
       "        [[ 5.5056e-01, -8.4024e-01, -3.3527e-01, -8.5944e-01, -1.2540e+00,\n",
       "           7.4744e-01, -3.3304e-01, -2.4331e-01],\n",
       "         [-1.7808e-01, -7.6053e-02, -4.9512e-01, -8.4694e-01,  1.9367e+00,\n",
       "          -1.4159e-01, -4.7342e-02,  1.6154e+00],\n",
       "         [-6.1849e-01, -1.3733e+00, -2.8057e-01,  1.7446e-01,  6.6037e-01,\n",
       "          -1.6244e+00, -2.4175e-01, -2.9469e-01],\n",
       "         [-2.6812e-01, -1.1531e+00, -5.0744e-01,  3.3289e-01,  4.5687e-01,\n",
       "           3.9358e-01, -6.5571e-02, -2.2850e-01],\n",
       "         [-3.2167e+00,  2.6580e-01,  2.6080e-01, -1.1741e+00, -1.8398e-01,\n",
       "          -1.6624e-01, -2.5699e-01,  2.4719e-01],\n",
       "         [ 9.8154e-01,  2.5411e+00,  1.0235e+00, -1.4992e+00, -1.5777e-01,\n",
       "          -9.6607e-01,  1.1425e+00, -1.4434e+00],\n",
       "         [-1.4751e+00,  1.0940e+00, -1.8879e-02,  1.1266e+00,  2.0738e-01,\n",
       "          -1.3202e-02, -4.2373e-01,  1.1929e+00],\n",
       "         [ 4.9716e-01,  3.1148e-01,  1.0679e+00,  5.0262e-02,  3.2624e-01,\n",
       "           1.4003e+00,  6.5483e-01,  2.0185e-01],\n",
       "         [-7.1470e-01, -1.2433e-01, -9.2667e-01,  2.7363e+00,  3.6622e-02,\n",
       "           1.2165e+00,  7.3674e-01,  3.1653e-01],\n",
       "         [-2.0378e+00,  1.2805e+00,  1.0735e+00, -6.3858e-02, -1.1057e+00,\n",
       "          -1.8794e-01,  1.2972e+00,  2.3260e+00]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token shape: torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the learnable classification token (cls_token)\n",
    "# It is defined as (1, 1, embed_dim) meaning:\n",
    "#   1: placeholder for a single token\n",
    "#   1: one token (the classification token itself)\n",
    "#   embed_dim: the token's embedding dimension\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "print(\"cls_token shape:\", cls_token.shape)  # (1, 1, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.]]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token # one token (the classification token itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded cls_token shape: torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# In order to prepend the cls_token to every image in the batch,\n",
    "# we expand it along the batch dimension.\n",
    "# This does not create new data; it simply views the same parameter for each item.\n",
    "expanded_cls_token = cls_token.expand(batch_size, -1, -1)\n",
    "print(\"Expanded cls_token shape:\", expanded_cls_token.shape)  # (4, 1, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape after concatenation: torch.Size([4, 11, 8])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the cls_token with the patch embeddings along the token dimension (dim=1)\n",
    "# The resulting tensor shape will be (batch_size, n_patches + 1, embed_dim)\n",
    "\n",
    "# since the cls_token is prepended to the entire batch of image patches, i.e., n_patches + 1 tokens, the dimension of cls_token is embed_dim.\n",
    "tokens = torch.cat([expanded_cls_token, patch_embeddings], dim=1)\n",
    "print(\"Tokens shape after concatenation:\", tokens.shape)  # (4, 11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional embeddings shape: torch.Size([11, 8])\n"
     ]
    }
   ],
   "source": [
    "# Then we add a learnable positional embedding to the tokens\n",
    "# Initialize the positional embeddings\n",
    "# Note: The positional embeddings are shared across the batch\n",
    "n_positions = n_patches + 1  # number of tokens\n",
    "positional_embeddings = nn.Parameter(torch.randn(n_positions, embed_dim))\n",
    "print(\"Positional embeddings shape:\", positional_embeddings.shape)  # (11, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
