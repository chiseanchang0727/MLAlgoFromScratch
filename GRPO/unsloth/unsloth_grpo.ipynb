{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884dc606",
   "metadata": {},
   "source": [
    "- Ref: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb#scrollTo=DkIvEkIIkEyB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b9ec6",
   "metadata": {},
   "source": [
    "Instal unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac4576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "# uv add \"unsloth[cu124-torch251] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db15e63",
   "metadata": {},
   "source": [
    "Pre-fine-tune the model to make GRPO follow the desired output format â€” this speeds up GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7052ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/MLAlgoFromScratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 09-05 04:00:24 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 09-05 04:00:24 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 04:00:26,739\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fdbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb056b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.7.8: Fast Qwen3 patching. Transformers: 4.54.0. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.581 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 59.0%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.58 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.94 GB. Also swap space = 0 GB.\n",
      "WARNING 09-05 04:00:39 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 09-05 04:00:48 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'generate', 'score', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 09-05 04:00:49 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 09-05 04:00:49 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":8,\"combo_kernels\":false,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 09-05 04:00:51 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 09-05 04:00:51 [cuda.py:289] Using XFormers backend.\n",
      "INFO 09-05 04:00:51 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 09-05 04:00:51 [model_runner.py:1108] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n",
      "INFO 09-05 04:00:52 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 09-05 04:00:53 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 09-05 04:00:54 [weight_utils.py:281] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 0.505118 seconds\n",
      "INFO 09-05 04:00:54 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 04:00:58 [punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 04:00:58 [model_runner.py:1140] Model loading took 3.1446 GiB and 6.297541 seconds\n",
      "INFO 09-05 04:01:02 [worker.py:287] Memory profiling takes 3.77 seconds\n",
      "INFO 09-05 04:01:02 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.58GiB) x gpu_memory_utilization (0.59) = 8.60GiB\n",
      "INFO 09-05 04:01:02 [worker.py:287] model weights take 3.14GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 4.39GiB.\n",
      "INFO 09-05 04:01:02 [executor_base.py:112] # cuda blocks: 1995, # CPU blocks: 0\n",
      "INFO 09-05 04:01:02 [executor_base.py:117] Maximum concurrency for 512 tokens per request: 62.34x\n",
      "INFO 09-05 04:01:02 [vllm_utils.py:669] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 09-05 04:01:02 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:14<00:00,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 04:01:17 [model_runner.py:1592] Graph capturing finished in 14 secs, took 4.24 GiB\n",
      "INFO 09-05 04:01:17 [vllm_utils.py:676] Unsloth: Patched vLLM v0 graph capture finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-05 04:01:18 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 19.73 seconds\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['pre_feedforward_layernorm', 'post_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "# key: vllm==0.8.5.post1\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name='unsloth/Qwen3-4B-base',\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True, # Enable vLLM fast inference\n",
    "    max_lora_rank=8,\n",
    "    gpu_memory_utilization=0.6 # Reduce if OOM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa61c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer can handle up to 32768 tokens.\n"
     ]
    }
   ],
   "source": [
    "max_dim = tokenizer.model_max_length\n",
    "print(f\"Tokenizer can handle up to {max_dim} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd7872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template is None\n",
    "# ValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37a0c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (2): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (3): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (4): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (5): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (6): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (7-35): 29 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b42afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "lora_rank = 8  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc681be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (2): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (3): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (5): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (7-35): 29 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53675cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5780a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Always includes a system prompt (either from messages or a default one).\n",
    "- Marks assistant turns with an eos_token so the model knows where each reply ends.\n",
    "- Optionally adds a â€œstart reasoningâ€ marker to signal when the model should generate output.\n",
    "- Uses placeholders that get replaced at runtime so you can change system prompts or special tokens without editing the Jinja template.\n",
    "\"\"\"\n",
    "\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace without specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7198ff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>What is 1+1?<start_working_out>I think it's 2.<end_working_out><SOLUTION>2</SOLUTION><|endoftext|>What is 2+2?<start_working_out>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn a structured list of messages into the exact string prompt the model will see.\n",
    "tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
    "], tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "474d6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "# dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140ef8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use few sample of quick sanity check\n",
    "# dataset = dataset[:2]\n",
    "# dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d4ea3",
   "metadata": {},
   "source": [
    "Note\n",
    "- I think the reason of using math here is the answer is precise\n",
    "- If the result is text, we may compute the text similarity to confirm the answer ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b36bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.to_pandas()[\n",
    "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "]\n",
    "\n",
    "# Try converting to number - if not, replace with NaN\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
    "# Select only numbers\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7deb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(x):\n",
    "    expected_answer = x[\"expected_answer\"]\n",
    "    problem = x[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + expected_answer + solution_end\n",
    "    return [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532c1aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'},\n",
       " {'role': 'user',\n",
       "  'content': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<start_working_out>Okay, let's see. I need to solve the equation âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\nâˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\nâˆš(xÂ² + 165) = 7 + âˆš(xÂ² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(âˆš(xÂ² + 165))Â² = (7 + âˆš(xÂ² - 52))Â².\\n\\nSimplifying the left side:\\n\\nxÂ² + 165 = 49 + 14âˆš(xÂ² - 52) + (âˆš(xÂ² - 52))Â².\\n\\nThe right side is expanded using the formula (a + b)Â² = aÂ² + 2ab + bÂ². So the right side becomes 7Â² + 2*7*âˆš(xÂ² - 52) + (âˆš(xÂ² - 52))Â², which is 49 + 14âˆš(xÂ² - 52) + (xÂ² - 52).\\n\\nSo putting it all together:\\n\\nxÂ² + 165 = 49 + 14âˆš(xÂ² - 52) + xÂ² - 52.\\n\\nHmm, let's simplify the right side. The xÂ² terms will cancel out, right? Let's subtract xÂ² from both sides:\\n\\n165 = 49 + 14âˆš(xÂ² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14âˆš(xÂ² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14âˆš(xÂ² - 52).\\n\\nSo 168 = 14âˆš(xÂ² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = âˆš(xÂ² - 52).\\n\\n12 = âˆš(xÂ² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12Â² = xÂ² - 52.\\n\\n144 = xÂ² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = xÂ².\\n\\n196 = xÂ².\\n\\nSo x = âˆš196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\nâˆš(14Â² + 165) - âˆš(14Â² - 52) = ?\\n\\nCalculate each term:\\n\\n14Â² is 196.\\n\\nSo first radical: âˆš(196 + 165) = âˆš361 = 19.\\n\\nSecond radical: âˆš(196 - 52) = âˆš144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION>\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg = dataset.iloc[0].Messages\n",
    "test_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bce393",
   "metadata": {},
   "source": [
    "Check to see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc0e4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<start_working_out>Okay, let's see. I need to solve the equation âˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\nâˆš(xÂ² + 165) - âˆš(xÂ² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\nâˆš(xÂ² + 165) = 7 + âˆš(xÂ² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(âˆš(xÂ² + 165))Â² = (7 + âˆš(xÂ² - 52))Â².\\n\\nSimplifying the left side:\\n\\nxÂ² + 165 = 49 + 14âˆš(xÂ² - 52) + (âˆš(xÂ² - 52))Â².\\n\\nThe right side is expanded using the formula (a + b)Â² = aÂ² + 2ab + bÂ². So the right side becomes 7Â² + 2*7*âˆš(xÂ² - 52) + (âˆš(xÂ² - 52))Â², which is 49 + 14âˆš(xÂ² - 52) + (xÂ² - 52).\\n\\nSo putting it all together:\\n\\nxÂ² + 165 = 49 + 14âˆš(xÂ² - 52) + xÂ² - 52.\\n\\nHmm, let's simplify the right side. The xÂ² terms will cancel out, right? Let's subtract xÂ² from both sides:\\n\\n165 = 49 + 14âˆš(xÂ² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14âˆš(xÂ² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14âˆš(xÂ² - 52).\\n\\nSo 168 = 14âˆš(xÂ² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = âˆš(xÂ² - 52).\\n\\n12 = âˆš(xÂ² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12Â² = xÂ² - 52.\\n\\n144 = xÂ² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = xÂ².\\n\\n196 = xÂ².\\n\\nSo x = âˆš196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\nâˆš(14Â² + 165) - âˆš(14Â² - 52) = ?\\n\\nCalculate each term:\\n\\n14Â² is 196.\\n\\nSo first radical: âˆš(196 + 165) = âˆš361 = 19.\\n\\nSecond radical: âˆš(196 - 52) = âˆš144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION><|endoftext|>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default of tokenize is True\n",
    "tokenizer.apply_chat_template(test_msg, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d2bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized result: [2610, 525, 2661, 264, 3491, 624, 38687, 911, 279, 3491]... \n",
      "with dimension: 1682\n"
     ]
    }
   ],
   "source": [
    "tokenized_result = tokenizer.apply_chat_template(test_msg)\n",
    "print(f\"Tokenized result: {tokenized_result[:10]}... \\nwith dimension: {len(tokenized_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81920244",
   "metadata": {},
   "source": [
    "Limit sequence length to max_seq_length / 2 during pre-fine-tuning to avoid overly long reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "769dbb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 5)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.loc[[len(tokenizer.apply_chat_template(x)) < max_seq_length/2 for x in dataset['Messages']]]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb5ae3",
   "metadata": {},
   "source": [
    "Tokenize data and convert it to a HF compatible dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3d0e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_382184/1368114567.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['text'] = tokenizer.apply_chat_template(dataset['Messages'].values.tolist(), tokenize=False)\n"
     ]
    }
   ],
   "source": [
    "dataset['text'] = tokenizer.apply_chat_template(dataset['Messages'].values.tolist(), tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3417a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b6ef3",
   "metadata": {},
   "source": [
    "Pre fine-tune the model so it follows the custom GRPO formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bbc8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.02s/it, est. speed input: 1.00 toks/s, output: 27.44 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Quora\n",
      "What is the sqrt of 101?\n",
      "6 Answers\n",
      "Best\n",
      "Gaurav Bakliwal\n",
      "Answered 2 years ago Â· Author has 526 answers and 384.9K answer author has 20.8K answers and 10M answer views 2 y\n",
      "What is the sqrt of 101?\n",
      "First of all 101 is not a square. It is pretty close to 100 which is 10Ã—10. However 101 is 4 more than 100, so take âˆš(100+4)\n",
      "=(âˆš100)+âˆš4\n",
      "=10 + 2\n",
      "100.50.00.25.00â€¦\n",
      "âˆš101 =10.05\n",
      "Related\n",
      "What is the square root of 614? (in square root symbol)\n",
      "What is the square root of 614? (in square root symbol)\n",
      "2,08.8\n",
      "614 = 4 *153.5\n",
      "2,âˆš153.5\n",
      "614 is a prime number.\n",
      "Related Question\n",
      "What is the square root of 614? (in squared symbol)\n",
      "2,20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check the result before pre fine-tune\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "text = \"What is the sqrt of 101?\"\n",
    "output = model.fast_generate(\n",
    "    [text], \n",
    "    sampling_params=sampling_params, \n",
    "    lora_request=None\n",
    ")\n",
    "print(output[0].outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:01<00:00, 49.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field='text',\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8, # update after accumulation step\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=5,\n",
    "        optim='adamw_8bit',\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type='linear',\n",
    "        seed=3407,\n",
    "        report_to='none'\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bda30c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 58 | Num Epochs = 2 | Total steps = 16\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 01:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.650900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=0.8443804010748863, metrics={'train_runtime': 116.2844, 'train_samples_per_second': 0.998, 'train_steps_per_second': 0.138, 'total_flos': 733895643365376.0, 'train_loss': 0.8443804010748863})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52678b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have?<start_working_out>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"Messages\"][:2],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # add this then the ending is \"<start_working_out>\"\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "953ee5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# _ = model.generate(\n",
    "#     **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "#     temperature = 0,\n",
    "#     max_new_tokens = 1024,\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "609db5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean memory\n",
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5288ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
       "    num_rows: 14116\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84efda41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used few sample for quick sanity check\n",
    "# dataset = dataset[:2]\n",
    "# dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7eb3aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\n"
     ]
    }
   ],
   "source": [
    "# the problem\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9e3aa",
   "metadata": {},
   "source": [
    "Synthesis the dataset with customized system prompt and quetion(the name is prompt in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8179f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x[\"prompt\"]}\n",
    "    ],\n",
    "    \"answer\": x['solution']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863143f",
   "metadata": {},
   "source": [
    "Extract the final answer from a structured text that contains both:\n",
    "- a reasoning section (e.g., <REASONING>...</REASONING>)\n",
    "- and a solution section (e.g., <SOLUTION>...</SOLUTION>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e493aa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<end_working_out>.*?<SOLUTION>(.+?)</SOLUTION>[\\s]{0,}(?:<\\|endoftext\\|>)?[\\s]{0,}$',\n",
       "           re.MULTILINE|re.DOTALL|re.UNICODE)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Add optional EOS token matching\n",
    "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dccdc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n2\\n']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case\n",
    "match_format.findall(\n",
    "    \"Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf13e9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['364']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_format.findall(\n",
    "    \"Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>364</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cd87c",
   "metadata": {},
   "source": [
    "Create reward function to match the format \n",
    "- +3 points if the output matches the full format exactly\n",
    "- Partial reward if key format tags (<REASONING>, </REASONING>, etc.) are present (each adds +0.5).\n",
    "- Penalty (-1.0) if a required tag is missing or appears more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "add0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff0a0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_partially(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <start_working_out> since we always prepend it!\n",
    "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb71197",
   "metadata": {},
   "source": [
    "Reward of penalize the generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "209a4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1]['content']\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        result.group(1)\n",
    "        if (result := match_format.search(r)) is not None else None for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0) # Note that it is append to scores not score, which means no answer=-2, then next run\n",
    "            continue\n",
    "        \n",
    "        # 5 points for correct answer\n",
    "        if guess  == true_answer:\n",
    "            score += 5.0\n",
    "\n",
    "        # Match if spaces are seen but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        \n",
    "        else:\n",
    "            # Reward the answer if it is close via ratios\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if ratio >= 0.9 and ratio <= 1.1:\n",
    "                    score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2:\n",
    "                    score += 1.5\n",
    "                else:\n",
    "                    score += -2.5\n",
    "            except:\n",
    "                score += -4.5\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b23b8",
   "metadata": {},
   "source": [
    "Remove commas such as 123,456 or extract the number in a sentence for example \"The solution is $20\" -> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91b9885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.34']\n",
      "['123456']\n",
      "['-0.234']\n",
      "['17']\n"
     ]
    }
   ],
   "source": [
    "# Match numbers like 17, 123,456, 0.34, -0.234, 1,234,567.89\n",
    "match_numbers = re.compile(r\"-?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?\", re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def find_numbers_clean(text):\n",
    "    \"\"\"Return numbers as strings with commas removed.\"\"\"\n",
    "    return [n.replace(\",\", \"\") for n in match_numbers.findall(text)]\n",
    "\n",
    "# If you'd like floats instead, use this:\n",
    "def find_numbers_float(text):\n",
    "    return [float(n.replace(\",\", \"\")) for n in match_numbers.findall(text)]\n",
    "\n",
    "# Tests\n",
    "print(find_numbers_clean(\"<SOLUTION>  0.34  </SOLUTION>\"))      # ['0.34']\n",
    "print(find_numbers_clean(\"<SOLUTION>  123,456  </SOLUTION>\"))   # ['123456']\n",
    "print(find_numbers_clean(\"<SOLUTION>  -0.234  </SOLUTION>\"))    # ['-0.234']\n",
    "print(find_numbers_clean(\"<SOLUTION> The solution is 17  </SOLUTION>\"))  # ['17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e37e9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0 # a counter that increments every time check_numbers is called.\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5 # ets the frequency (here 5).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5f8c66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluate numeric answers extracted from model completions against ground truth.\n",
    "\n",
    "    Scoring (per item):\n",
    "      - 3.5  if the extracted numeric guess equals the true answer (after stripping & removing commas)\n",
    "      - -1.5 if a number was extracted but it does not equal the true answer\n",
    "      - -2.5 if no number could be extracted\n",
    "      - 0    if parsing fails unexpectedly\n",
    "\n",
    "    Args:\n",
    "        prompts:\n",
    "            A sequence of message lists. This function reads the question text from\n",
    "            `prompts[0][-1][\"content\"]`\n",
    "            Example shape: `[[{\"role\":\"user\",\"content\":\"...\"}], ...]`\n",
    "        completions:\n",
    "            A sequence of generation lists. Each item should be a list whose first element\n",
    "            has a `\"content\"` string, accessed as `completion[0][\"content\"]`.\n",
    "            Example shape: `[[{\"role\":\"assistant\",\"content\":\"...\"}], ...]`\n",
    "        answers:\n",
    "            Sequence of ground-truth answers (strings or numbers). Must align 1:1 with `completions`.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: Scores aligned with `completions` / `answers`.\n",
    "\n",
    "    Notes:\n",
    "        - Compares numeric equality after normalizing commas in guesses.\n",
    "        - `answers` are cast to float via `float(str(answer).strip())`.\n",
    "    \"\"\"\n",
    "\n",
    "    question = prompts[0][-1]['content'] # prompts are duplicate based on num_generation, so we only select one of tem\n",
    "    responses = [completion[0][\"content\"] for completion in completions] \n",
    "\n",
    "\n",
    "    # apply reg.search to extract the answer, since the completion may contain other string.\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_numbers.search(r)) is not None else None for r in responses\n",
    "    ]\n",
    "    \n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f86fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers_from_doc(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7652ab",
   "metadata": {},
   "source": [
    "Warlus operation explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d8470f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42', '-3.14', None, '1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regex to capture a number (with optional decimal part)\n",
    "match_numbers = re.compile(r\"(-?\\d+(?:\\.\\d+)?)\")\n",
    "\n",
    "responses = [\n",
    "    \"The answer is 42.\",\n",
    "    \"I think it's -3.14\",\n",
    "    \"No number here!\",\n",
    "    \"Maybe 1,234 is correct\"\n",
    "]\n",
    "extracted_responses = [\n",
    "    guess.group(1)\n",
    "    if (guess := match_numbers.search(r)) is not None else None\n",
    "    for r in responses\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Warlus operator (`:=`):\n",
    "does two things at once:\n",
    "1. Runs match_numbers.search(r)\n",
    "2. Saves the result into guess\n",
    "3. Returns the result so it can be used immediately in the expression\n",
    "\"\"\"\n",
    "\n",
    "print(extracted_responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0156b378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2, 3, 4]\n",
    "result = []\n",
    "for n in numbers:\n",
    "    x = n * n\n",
    "    if x > 5:\n",
    "        result.append(x)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "88d298d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 16]\n"
     ]
    }
   ],
   "source": [
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "result = [x for n in numbers if (x := n*n) > 5]\n",
    "\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ef53d",
   "metadata": {},
   "source": [
    "For stable traninig, we want to restrict the model_length\n",
    "1. remove the top 10% long prompts\n",
    "2. check the max model input length # it didn't involved in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "187340a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length =  171\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 1000 (default)\n",
    "tokenized = dataset.map(\n",
    "    lambda x: {\"tokens\": tokenizer.apply_chat_template(x['prompt'], add_generation_prompt=True, tokenized=True)}, # add_generation_prompt = True for gettin the max possible length\n",
    "    batched= True\n",
    ")\n",
    "\n",
    "# calculate the length\n",
    "tokenized = tokenized.map(lambda x: {\"L\": len(x['tokens'])})\n",
    "\n",
    "maximum_length = int(np.quantile(tokenized['L'], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1020be6",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be7ae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use GRPOTrainer in here, from-scratch implementation will be another folder\n",
    "\n",
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "from vllm import SamplingParams\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52d383f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 4\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params=vllm_sampling_params,\n",
    "    temperature=1.0,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='linear',\n",
    "    optim='adamw_8bit',\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1, # Increase to 4 for smoother training\n",
    "    num_generations=4,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    max_steps = 100,\n",
    "    save_steps = 100,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3d675625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,483 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:21:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / match_format_exactly / mean</th>\n",
       "      <th>rewards / match_format_exactly / std</th>\n",
       "      <th>rewards / match_format_partially / mean</th>\n",
       "      <th>rewards / match_format_partially / std</th>\n",
       "      <th>rewards / check_answer / mean</th>\n",
       "      <th>rewards / check_answer / std</th>\n",
       "      <th>rewards / check_numbers / mean</th>\n",
       "      <th>rewards / check_numbers / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.466307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.079826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.836608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.672960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>291.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.358377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.530737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.004841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>346.250000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>372.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>269.000000</td>\n",
       "      <td>11.200824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.487236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>312.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.202540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.461467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.785715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.379100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.158859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>351.750000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>261.000000</td>\n",
       "      <td>1.086657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.221652</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.679068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.176890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>367.500000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>382.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>3.763500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>369.500000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>1.788378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>345.500000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>402.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>2.803483</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>286.250000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.908005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>365.750000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>239.000000</td>\n",
       "      <td>2.988621</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-5.625000</td>\n",
       "      <td>1.030776</td>\n",
       "      <td>297.250000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.289337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>1.436141</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.545018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.891880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>188.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.679039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>300.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.390136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>-2.750000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.562866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.636105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>407.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>0.934031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.398854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>387.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.881011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.338212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.395314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>399.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.061308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>407.250000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>327.000000</td>\n",
       "      <td>4.384736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.392107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.253815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>311.750000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.275836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.526375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>400.750000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>433.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>1.340876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.076973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.045215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.502614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>300.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.807160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>373.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.206491</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.903489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-5.375000</td>\n",
       "      <td>1.436141</td>\n",
       "      <td>382.250000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>1.685160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>1.436141</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.990884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>384.250000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>1.363229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>-5.250000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>281.750000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>203.500000</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>1.774713</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>375.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.365989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.958041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.941798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>292.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>390.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.552857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.651066</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>434.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.650459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>431.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.543690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>396.250000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>422.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>1.761631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>392.500000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>6.274465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.648764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.628979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.030776</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.012880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>307.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.115164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.719725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>378.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.031052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.679338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>-5.125000</td>\n",
       "      <td>1.701715</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>241.000000</td>\n",
       "      <td>3.546947</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>320.250000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>5.244886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.319410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.662315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>385.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.165857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>342.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>368.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.904466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.120605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>428.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.774552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>376.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.352299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>351.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.692392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.896929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>423.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.626510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.703774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>394.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.229753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.017582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>379.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.436804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>-5.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.661060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>-6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>304.250000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>401.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.021973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.030776</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.202538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.244351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.773552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>392.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.016464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.164764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.890942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>-6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>366.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.229337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>-6.375000</td>\n",
       "      <td>1.030776</td>\n",
       "      <td>313.250000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>218.000000</td>\n",
       "      <td>0.850711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>-6.125000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>349.750000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>354.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>2.498889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "Acme Corporation has released an alphabet soup in which each of the vowels (A, E, I, O, U) of the English alphabet appears five times (and the consonants do not appear at all). How many five-letter words can be formed from a bowl of Acme Vowel Soup? (Note: The words do not have to be actual words in English!) \n",
      "Answer:\n",
      "3125 \n",
      "Response:\n",
      "Okay, so this problem is about forming five-letter words using only the vowels A, E, I, O, U, and each of these vowels can only be used up to five times from the soup. Got it. \n",
      "\n",
      "First, let me make sure I understand the constraints:\n",
      "1. The words are exactly five letters long.\n",
      "2. The only letters allowed are A, E, I, O, U, and each of these can appear at most five times in the same word.\n",
      "3. Words don't have to be real English words.\n",
      "\n",
      "Okay, so since the words are five letters long and we have five different vowels, it feels like permutations problem. Maybe the starting point is to consider if repetition is allowed. Hmm, wait, the problem says \"each of the vowels appears five times.\" Does that mean each vowel counts as one letter in each word, and once you have 5 vowels, you can't have more? Let me think.\n",
      "\n",
      "Wait, no. If the vowels come from the alphabet soup, and the soup has 5 A's, 5 E's, etc., then in a word, the total number of vowels could be limited only by having 5 of each. So for a five-letter word from a five-letter alphabet, the total vowels available in the soup is greater than 5, meaning there are enough vowels to make up the word? Wait, no, let me re-read that. The vowels \"each of the vowels (A, E, I, O, U)\" appear five times. So that's 5 A's, 5 E's, etc., yes. In that case, a five-letter word can have at most one of each vowel, because there are only five of each. Wait, let's check: if the letters are interchangeable, but with the constraint that no letter type exceeds 5. Hmm. So if you start putting letters together, the \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "Define $f(x) = 2x + 3$ and suppose that $g(x+2) = f(f(x-1) \\cdot f(x+1) + f(x))$. Find $g(6)$. \n",
      "Answer:\n",
      "259 \n",
      "Response:\n",
      "Let's start by looking at the problem breakdown step by step:\n",
      "The function \\( g(x+2) = f(f(x-1) \\cdot f(x+1) + f(x)) \\), with \\( f(x) = 2x + 3 \\).\n",
      "\n",
      "First, I can find \\( f(x) \\):\n",
      "\\[ f(x) = 2x + 3 \\]\n",
      "\n",
      "Now I need to find \\( f(x-1) \\). Substitute \\( x-1 \\) into \\( f(x) \\):\n",
      "\\[ f(x-1) = 2(x-1) + 3 = 2x - 2 + 3 = 2x + 1 \\]\n",
      "\n",
      "Next, calculate \\( f(x+1) \\):\n",
      "\\[ f(x+1) = 2(x+1) + 3 = 2x + 2 + 3 = 2x + 5 \\]\n",
      "\n",
      "Then, determine \\( f(x) \\) again as just \\( f(x) \\):\n",
      "\\[ f(x) = 2x + 3 \\]\n",
      "\n",
      "Now plug into \\( f(f(x) \\):\n",
      "\\[ f(x-1) \\cdot f(x+1) + f(x) \\]\n",
      "\\[ = (2x + 1)(2x + 5) + (2x + 3) \n",
      "\\]\n",
      "Let's multiply the terms \\((2x + 1)(2x + 5)\\)\n",
      "\\[ = (2x+1)(2x+5) = (2x)^2 + 2(2x)(5) + 2x + 5 = 4x^2 + 10x + 2x + 5 = 4x^2 + 12x + 5 \\]\n",
      "Adding the other term \\( 2x + 3 \\):\n",
      "\\[ = 4x^2 + 12x + 5 + 2x + 3 = 4x^2 + 14x + 8 \\]\n",
      "\n",
      "Now evaluate \\( \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "Let $A$, $B$, $C$, $D$, $E$, $F$, $G$, $H$ be the eight vertices of a $30 \\times 30 \\times 30$ cube. The two figures $ACFH$ and $BDEG$ are congruent regular tetrahedra. Find the volume of the intersection of these two tetrahedra. \n",
      "Answer:\n",
      "4500 \n",
      "Response:\n",
      "Okay, let's breathe. So we have a cube with vertices A, B, C, D, E, F, G, H. Itâ€™s a billion-sided dice. Wait no, double check.  Yeah cube. Okay so in a cube. Let me try to picture this. So if weâ€™re gonna have two cubes. First Iâ€™m gonna label them. So Itâ€™s a three by three by three. Actually no. The cube is a thousand cubic units. Thirty by thirty by thirty. So half the dimensions. Not a thousand, a thousand.  So no. Make sense. Let's try this first. I guess the nice thing about problems on cubes and tetrahedrons is if theyâ€™re symmetrical. Usually thereâ€™s a way to visualize theyâ€™re symmetric by rotations etc. Anyways anyways. Alright. Let me try to ## think about it# in terms of things that are roughly equal. Given the symmetry of this thing. Let's say this big cube. This billion-sided dice. Exactly. Cause why not. Because it doesn't have to be like. You know the type of dice that are. wait. The freaking. Iâ€™m thinking the faces of that thing look like. That's bullshit. No you know these cubes. Okay. So anyways. So our two figures: ACFH and BDEG are congruent regular tetrahedra. So if theyâ€™re two of them, thereâ€™s a octa cube. So itâ€™s like a octahedron inside this billion-sided dice. I understand. Okay. So the intersection of these two tetrahedra without looking.\n",
      "\n",
      "I donâ€™t know. Wait no let me think about it. Alright. Calculate. Wait no. While you know the volume would be pretty high. A regular tetrahedron in a cube. If itâ€™s congruent pointed at one end. In terms of \n",
      "Extracted:\n",
      "None\n",
      "********************Question:\n",
      "In our number system, the base is ten. If the base were changed to four, you would count as follows: $1, 2, 3, 10, 11, 12, 13, 20, 21, 22, 23, 30, \\ldots$ What would be the twentieth number in this system? \n",
      "Answer:\n",
      "110 \n",
      "Response:\n",
      "Let's consider the given numeric system where the base is 4. In this base system, single digit numbers are between 0 and 3 inclusively. Once you use up all the single digit numbers, the next number is written as 10, which represents \\(1*4+0=6\\) in decimal.\n",
      "\n",
      "If you continue counting, 20 represents \\(2*4 + 0 = 8\\) in decimal, and 21 represents \\(2*4 + 1 = 9\\). So for every increment of 1 in base 4, there is an equivalent increment value in decimal. However, when the rightmost digit (least significant digit) reaches 4, it carries over to the next higher place (next left digit). Hence, the rightmost digit increases by 1 only when all single-digit values are exhausted.\n",
      "\n",
      "For the twentieth digit in base four, let us convert decimal 19 to base four. We divide 19 by 4 to get quotient and remainders for each digit position. \n",
      "\n",
      "For example:\n",
      "    19 % 4 = 3 (remainder)\n",
      "    19 / 4 = 4 (quotient)\n",
      "    4 % 4 = 0\n",
      "    4 / 4 = 1 (new quotient)\n",
      "    1 still greater than 0.\n",
      "So 19 in decimal is: 103 in base four. </code></start_working_out><SOLUTION>So the answer to the query becomes 103 in base four.</code></start_working_out> \n",
      "Extracted:\n",
      "4\n",
      "********************Question:\n",
      "The diagram shows an octagon consisting of $10$ unit squares. The portion below $\\overline{PQ}$ is a unit square and a triangle with base $5$. If $\\overline{PQ}$ bisects the area of the octagon, find the ratio \\(\\frac{XQ}{QY}\\). The original answer is in the form \\(\\frac{k}{m}\\), please provide the value of \\(k + m\\). \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "Oh. Okay, this one's gonna be a bit of a stretch because the problem is about an octagon. Basically, I need to determine the ratio \\(\\frac{XQ}{QY}\\) of two points \\(X\\) and \\(Y\\) along the line \\(\\overline{PQ}\\) such that this cut divides the octagon into two areas of equal size. First, let me lay out what I see. The problem mentions a diagram that wraps around 10 unit squares within the octagon. So if I flip open a manual or something, the diagram must be there. But look, before scrolling, I can probably reason the diagram. Wait, okay. Wait. Well â€˜unit squaresâ€™? Four? Thereâ€™s a square of this area thatâ€™s cut up by the \\\n",
      "i. guess first the octagon is formed by adding a 'triangle' slice at some part. Now, if \\(\\overline{PQ}\\) is pointing down in one corner of this octagon? Maybe starts here? and draws down, trying to mimic. Wait, oh look. Wait I have no endpoint rules. Wait this is probably, something about the AD=BE bisect area as 1080^\\circ?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wait okay. This is tricky. So geometric machinery. Let's try approaching in like a slope. Okay. Yeah, yeah. So the key insight probably is that kind of the key bisectors. Okay, the problem wants the areas cut to have equal areas. Start with the octagon area. Let see. Let's double the he answer\n",
      " <SOLUTION>The problem involves an octagon split into 10 unit squares. It's a bit tricky to work through the geometry, but let's try to visualize the setup based on the problem description.</SOLUTION \n",
      "Extracted:\n",
      "10\n",
      "********************Question:\n",
      "Four friends purchased a ball. The first friend paid half of the ball's total price. The second friend contributed one-third of the total amount paid by the other three friends. The third friend paid a quarter of the sum paid by the other three friends. The fourth friend paid $5. How much did the ball cost? \n",
      "Answer:\n",
      "100 \n",
      "Response:\n",
      "Let's denote the total price of the ball as X. Let's break down each step based on the clues provided:\n",
      "1. The first friend paid half of the total price:\n",
      "The first friend paid X/2.\n",
      "2. The second friend contributed one-third of the total amount paid by the other three friends.\n",
      "The other three friends together paid the total amount (X) minus the first friend's contribution. This sum is 3*(X/2).\n",
      "So, the second friend paid 1/3 of this sum: (1/3)*3*(X/2) = X/2.\n",
      "Wait, this is confusing, the clues still:\n",
      "The second friend contributed one-third of the total amount paid by the other three friends.\n",
      "So the amount paid by the second friend must be kept from subtracting off the other contributions, not combined with the others. Let me recheck.\n",
      "\n",
      "\n",
      "\n",
      "Okay just am sure im getting mixed up how to set equations up since different language given.\n",
      "<end_working_out><SOLUTION>Alright let me do it again step by step. Let me label:\n",
      "- let Freddy (F) = first friend paid\n",
      "- let Luke (L) = second friend paid\n",
      "- let the third friend (m) = Maggie paid\n",
      "- let the fourth friend (Z) = Zack paid\n",
      "Set other other end 3 friends paid+ Freddie.\n",
      "I should create like;\n",
      "if we assume:\n",
      "1. L + m +J = 3*x/2? but i should run over this one again as well.\n",
      "ok let re evaluate again\n",
      "On second read part:\n",
      "\n",
      "second friend paid the other 3 friends but means,\n",
      "(no or pay need need but same hold equal to sixe hoe)\n",
      "Let me \"figuring out doubled please give hints if i do bottom that work out its double error please describe error. Please trust me its bases in my train of not and will it breid peerless and Sovereign?\"\n",
      "\n",
      "Ok push start again with:\n",
      "BUY mother and \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "Suppose $a$ and $b$ are positive integers such that $\\gcd(a,b)$ is divisible by exactly $7$ distinct primes and $\\mathop{\\text{lcm}}[a,b]$ is divisible by exactly $28$ distinct primes.\n",
      "\n",
      "If $a$ has fewer distinct prime factors than $b$, then $a$ has at most how many distinct prime factors? \n",
      "Answer:\n",
      "17 \n",
      "Response:\n",
      "Okay, let's think about this problem. So we have two positive integers, a and b. Their greatest common divisor is divisible by exactly seven distinct primes, and their least common multiple is divisible by exactly twenty-eight distinct primes. Also, a has fewer distinct prime factors than b. We need to find the maximum number of distinct prime factors a could have. Hmm, that seems like it'll involve a mix of GCD and LCM properties. Let me jot down some key points from my notes.\n",
      "\n",
      "First recall that for any two integers a and b, the relationship between their GCD and LCM is given by gcd(a,b) * lcm[a,b] = a * b. That's a fundamental theorem. Okay, so that might be useful later. Also, remember that the GCD is the product of the minimum power of each prime found in a and b, and the LCM is the product of the maximum power of each prime. So when we're given gcd(a,b) has 7 distinct primes and lcm[a,b] has 28 distinct primes. Let me try to express that mathematically.\n",
      "\n",
      "If I denote the distinct prime factors as set P, then the GCD divides both a and b, so the primes in gcd(a,b) must be part of a and b. The LCM contains all the distinct primes, with the highest power possible in either a or b. Given that, maybe if I think about the distribution of these primes in a and b, I can find inequalities that limit the number of primes in a.\n",
      "\n",
      "The problem also states that a has fewer prime factors than b. Let's denote the number of distinct prime factors in a as r, and in b as s. But since 28 distinct primes are divisors of lcm(a,b), that could be less than s, or equal if there's overlap. Wait, I think I need to be careful here \n",
      "Extracted:\n",
      "7\n",
      "********************Question:\n",
      "The area of parallelogram $ABCD$ is $51\\sqrt{55}$ and $\\angle{DAC}$ is a right angle. If the side lengths of the parallelogram are integers, what is the perimeter of the parallelogram?  \n",
      "Answer:\n",
      "90 \n",
      "Response:\n",
      "Okay, letâ€™s tackle this problem step by step. The problem states:\n",
      "\n",
      "*The area of parallelogram \\( ABCD \\) is \\( 51\\sqrt{55} \\) and \\( \\angle{DAC} \\) is a right angle. If the side lengths of the parallelogram are integers, what is the perimeter of the parallelogram?*\n",
      "\n",
      "First, let's make sure I understand the problem correctly. We have a parallelogram \\( ABCD \\) with one of its angles (\\( \\angle{DAC} \\)) being a right angle. The area of the parallelogram is given as \\( 51\\sqrt{55} \\), and the side lengths are integers. We need to find the perimeter of the parallelogram.\n",
      "\n",
      "Okay, let's break this down.\n",
      "\n",
      "### Step 1: Understanding the Parallelogram\n",
      "\n",
      "A parallelogram has two pairs of opposite sides that are equal in length. Let's denote the side lengths as \\( a \\) and \\( b \\). So, the perimeter \\( P \\) would be:\n",
      "\n",
      "\\[\n",
      "P = 2(a + b)\n",
      "\\]\n",
      "\n",
      "But we need to find \\( a \\) and \\( b \\).\n",
      "\n",
      "### Step 2: Area of the Parallelogram\n",
      "\n",
      "The area \\( A \\) of a parallelogram is given by:\n",
      "\n",
      "\\[\n",
      "A = \\text{base} \\times \\text{height}\n",
      "\\]\n",
      "\n",
      "In this problem, the area is \\( 51\\sqrt{55} \\). But wait, there's a specific angle mentioned: \\( \\angle{DAC} \\) is a right angle. Let's see what that means.\n",
      "\n",
      "### Step 3: Analyzing the Given Angle\n",
      "\n",
      "\\( \\angle{DAC} \\) is a right angle. Let's label the points: \\( A \\), \\( B \\), \\( C \\), and \\( D \\). Since it's a parallelogram, opposite sides are parallel and equal. Let's assume \\( AB \n",
      "Extracted:\n",
      "51\n",
      "********************Question:\n",
      "Find the number of lattice points that the line $19x + 20y = 1909$ passes through in Quadrant I. \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "Okay, let me try this. The line equation is given as 19x + 20y = 1909. We need to find the number of lattice points on this line in Quadrant I. \n",
      "Lattice points are points with both x and y coordinates being integers. So, I need to find integer solutions for x and y such that both coordinates are positive. \n",
      "First step is probably to express y in terms of x. Let's do that. Rewriting the equation...\n",
      "Shifting the 19x over, so 20y = 1909 - 19x. Then dividing both sides by 20, we get y = (1909 - 19x)/20. Okay. Now for both x and y to be integers, the right side must simplify to an integer. So (1909 - 19x)/20 needs to be an integer. \n",
      "Let's write that out. For (1909 - 19x)/20 to be an integer, (1909 - 19x) must be a multiple of 20. Let me denote that (1909 - 19x) = 20k where k is some integer.\n",
      "Then 1909 - 19x = 20k. Rearranging for x, I get 19x = 1909 - 20k. Then x = (1909 - 20k)/19. Since x needs to be an integer, (1909 - 20k)/19 must be some integer.\n",
      "Let me break that down further. 1909 divided by 19. Let me calculate that first. 19 times what equals somewhere around 1900... 19 times 100 is 1900, then I can subtract that. Then plus 9. Wait nevermind, let me do 19 x 100 is \n",
      "Extracted:\n",
      "19\n",
      "********************Question:\n",
      "Let $a_1,a_2,\\dots,a_{2018}$ be a strictly increasing sequence of positive integers such that \\[a_1+a_2+\\cdots+a_{2018}=2018^{2018}.\\]\n",
      "Find the remainder when $a_1^3+a_2^3+\\cdots+a_{2018}^3$ is divided by $6$. \n",
      "Answer:\n",
      "4 \n",
      "Response:\n",
      "Let's start by considering the problem step by step. We need to find the remainder when the sum of the cubes of a strictly increasing sequence of positive integers \\(a_1, a_2, \\dots, a_{2018}\\) is divided by 6, where the sum of the sequence itself is \\(2018^{2018}\\). The sequence being strictly increasing suggests that the smallest possible sequence is 1 through 2018, where \\(a_1 = 1\\), \\(a_2 = 2\\), ..., \\(a_{2018} = 2018\\). Let's check the sum in this scenario and then proceed to compute the sum of cubes and find the remainder modulo 6.<end_working_out><SOLUTION>First, let's compute the sum of the sequence from 1 to 2018. The sum can be expressed as: \\[\n",
      "\\text{Sum} = \\sum_{k=1}^{2018} k = \\frac{2018 \\times 2019}{2} = 2018 \\times 1009.5.\n",
      "\\] The total sum \\(2018^{2018}\\) must equal this value or a rearrangement of it (since it's strictly increasing). So, we simply rewrite the sum-equality transformations to find which \\(a_i\\) terms should be used. We know:\n",
      "\\[ 1 + 2 + 3 + \\ldots + 2018 = \\frac{2018 \\times 2019}{2}. \\] Let's compute 2019 modulo 6 because it can show properties useful in calculations:\n",
      "\\[ 2018 \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "If $a,b,c$ are positive integers less than $13$ such that \\begin{align*}\n",
      "2ab+bc+ca&\\equiv 0\\pmod{13}\\\\\n",
      "ab+2bc+ca&\\equiv 6abc\\pmod{13}\\\\\n",
      "ab+bc+2ca&\\equiv 8abc\\pmod {13}\n",
      "\\end{align*}then determine the remainder when $a+b+c$ is divided by $13$. \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "First, let me lay out the purpose of the problem. You're providing a Diophantine-like equation with some modular congruence conditions. Sentence, the given problem involves positive integers $a, b, c$ less than 13, all confined within the range of 1 being positive integers, and 13 zero. You have multiple variables because I just listed out all three congruences. The goal is to acquire results that together in order to condense it reduced unknowns so that really this can be determined. Alright, let me go through each step you need to think about.  They from modular algebra point of view before doing the rest of the modulo. Now let's just try starting to approach that because this is a a linear problem Wow. They are similar equations from similar equations but then next time appear arranged differently. Generally working from right and left. So I may check into 666 xmodules then check her time you arrive at a contradiction here. Then but here there didn't loss of contrast Event on hydrate. We can proceed at least left handed repeating as you got. This Hill more steps courage left head. So I wait for the right answer as well. <End_Partial_Working_Out> \n",
      "Extracted:\n",
      "13\n",
      "********************Question:\n",
      "For some real number $c$, the graphs of the equation $y = |x - 20| + |x + 18|$ and the line $y = x + c$ intersect at exactly one point. What is $c$? \n",
      "Answer:\n",
      "18 \n",
      "Response:\n",
      "Okay, let's tackle this problem. So, I have the equations:\n",
      "\n",
      "1. \\( y = |x - 20| + |x + 18| \\)\n",
      "2. \\( y = x + c \\)\n",
      "\n",
      "And they intersect at exactly one point. I need to find the value of \\( c \\).\n",
      "\n",
      "First, I should understand what these equations represent. The first equation is the sum of two absolute values. Absolute value functions usually create V-shapes on the graph. The second equation is a straight line. So, when these two graphs intersect, they should touch at exactly one point.\n",
      "\n",
      "Hmm, so I need to find the value of \\( c \\) such that these two graphs touch at exactly one point. That means there's exactly one solution where both equations equal each other. So, I can set them equal to each other:\n",
      "\n",
      "\\[ |x - 20| + |x + 18| = x + c \\]\n",
      "\n",
      "Wait, but absolute value equations can be tricky because they have different behaviors depending on the value inside. So, maybe I should consider different cases based on the value of \\( x \\).\n",
      "\n",
      "The absolute value expressions \\( |x - 20| \\) and \\( |x + 18| \\) will change their behavior based on where \\( x \\) is relative to 20 and -18. So, perhaps I should consider different intervals for \\( x \\):\n",
      "\n",
      "1. \\( x < -18 \\)\n",
      "2. \\( -18 \\leq x < 20 \\)\n",
      "3. \\( x \\geq 20 \\)\n",
      "\n",
      "Let me see, I have two absolute values here, which means there's a break point at each of them: \\( x = -18 \\) and \\( x = 20 \\). So, the intervals are those three intervals above.\n",
      "\n",
      "Alright, let me consider each interval separately.\n",
      "\n",
      "**Case 1:** \\( x < -18 \\)\n",
      "\n",
      "In this interval, \\( |x - 20 \n",
      "Extracted:\n",
      "1\n",
      "********************Question:\n",
      "A right pyramid with a square base has total surface area 432 square units.  The area of each triangular face is half the area of the square face. What is the volume of the pyramid in cubic units?The answer is in the form k\\sqrt{m}+n,. Please provide the value of k + m + n. \n",
      "Answer:\n",
      "291 \n",
      "Response:\n",
      "Okay, let's see. The problem is about a right pyramid with a square base, and the total surface area is 432 square units. The area of each triangular face is half the area of the square base. Need to find the volume of the pyramid in the form kâˆšm + n. Alright, let's break this down.<end_working_out><SOLUTION>Alright, let's tackle this step by step. First, let's understand what we're given and what we need to find.<br><br>Given: - A right pyramid with a square base. - Total surface area = 432 square units. - The area of each triangular face is half the area of the square base.<br><br>Need to find: Volume in the form kâˆšm + n. <br><br>First, let's recall that in a right pyramid:<br><br>1. Total surface area = Base area + (Number of sides Ã— Lateral face area).<br><br>For a square pyramid: - Base area = sÂ², where s is the side length of the base.<br><br>2. Volume = (Base area Ã— Height)/3 = (sÂ²Ã—H)/3.<br><br>We need to find s and H.<br><br>According to the problem: - Each triangular face area is (1/2)Ã—Base area.<br><br>Let's denote:<br><br>Base area (B) = sÂ².<br><br>Since there are four triangular faces in a square pyramid: Lateral area (L) = 4Ã—[(sÃ—l)/2] = 2sÃ—l, where l = slant height.<br><br>Given: Lateral area = T (Total lateral area). However, total surface area includes base, so total S.A. = B + L.<br><br \n",
      "Extracted:\n",
      "432\n",
      "********************Question:\n",
      "Jane is 25 years old. Dick is older than Jane. In $n$ years, where $n$ is a positive integer, Dick's age and Jane's age will both be two-digit number and will have the property that Jane's age is obtained by interchanging the digits of Dick's age. Let $d$ be Dick's present age. How many ordered pairs of positive integers $(d,n)$ are possible? \n",
      "Answer:\n",
      "25 \n",
      "Response:\n",
      "Alright, let me start by understanding the problem. Jane is currently 25 years old. Dick is older than Jane, so Dick's current age must be greater than 25. After 'n' years, both their ages will be two-digit numbers. Importantly, after 'n' years, Jane's age will be the reverse of Dick's age. Let's denote Dick's current age as 'd'. So, in 'n' years, Jane's age will be 25 + n, and Dick's age will be d + n. Since both ages are two-digit numbers after 'n' years, so does the equation:\n",
      "\n",
      "[ (d + n) \\times 10 + unit\\_digit = (25 + n) \\times 10 + tens\\_digit ]\n",
      "\n",
      "But the problem states that after 'n' years, Jane's age will be the reverse of Dick's age. Let me denote:\n",
      "\n",
      "[ \\text{Dick's age after } n \\text{ years} ] [ = 10a + b ] [ \\text{Jane's age after } n \\text{ years} ] [ = 10b + a ]\n",
      "\n",
      "Then, given that:\n",
      "\n",
      "[ 10a + b = d + n + 10(10) + 0 ] [ 10b + a = 25 + n + 10(10) + 0 ]\n",
      "\n",
      "Wait no. Holding onto my earlier notation:\n",
      "\n",
      "[ d + n = 10a + b ] [ 25 + n = 10b + a ]\n",
      "\n",
      "Subtracting the second equation from the first:\n",
      "\n",
      "[ d - 25 = 9(a - b) ] [ d + n - (10b + a) = (10a + b) - (10b \n",
      "Extracted:\n",
      "25\n",
      "********************Question:\n",
      "A club consisting of $11$ men and $12$ women needs to choose a committee from among its members so that the number of women on the committee is one more than the number of men on the committee. The committee could have as few as $1$ member or as many as $23$ members. Let $N$ be the number of such committees that can be formed. Find the sum of the prime numbers that divide $N.$ \n",
      "Answer:\n",
      "81 \n",
      "Response:\n",
      "Okay. So there's a club with 11 men and 12 women, and they need to form a committee with the condition that the number of women is one more than the number of men. The committee can have anywhere from 1 to 23 members. I need to find out how many such committees can be formed, and then determine the sum of the prime numbers that divide that number. Let me break this down.\n",
      "\n",
      "First, let's understand the total number of members. There are 11 men and 12 women, so 11 + 12 = 23 members total. The committee size can range from 1 to 23, which makes sense because if there were more than 23 members on the committee, that wouldn't be possible with just the 23 people in the club. The condition says that the number of women must be one more than the number of men. Let's denote the number of men selected as m. Then the number of women selected would be m + 1. The total number of selected members would then be m (men) + (m + 1) = 2m + 1. But the total number of members is 23, so 2m + 1 â‰¤ 23. Solving for m, subtract 1 from both sides to get 2m â‰¤ 22, then divide by 2 to get m â‰¤ 11. So possible values for m range from 0 up to 11 because m represents the number of men selected, and in the case when m = 0, there are 0 men selected but 1 woman selected, which totals 1 member. Let's test when m = 11, then number of women would be 12, \n",
      "Extracted:\n",
      "11\n",
      "********************Question:\n",
      "How many digits are in the value of the following expression: $2^{2001}\\times 5^{1950}\\div 4^{27}$? \n",
      "Answer:\n",
      "1950 \n",
      "Response:\n",
      "Okay, let's approach this step by step. We need to simplify the expression and figure out the number of digits in the resulting number. Let's write it down first:\n",
      "\n",
      "$2^{2001} \\times 5^{1950} \\div 4^{27}$\n",
      "\n",
      "Step 1: Simplify the expression.\n",
      "\n",
      "Okay, I know 4 is 2 squared, so maybe that could help. Let's rewrite 4 as $2^2$.\n",
      "\n",
      "So $4^{27} = (2^2)^{27} = 2^{54}$.\n",
      "\n",
      "Now, let's look at the whole thing:\n",
      "\n",
      "$2^{2001} \\times 5^{1950} \\div 2^{54} = 2^{2001 - 54} \\times 5^{1950} = 2^{1947} \\times 5^{1950}$.\n",
      "\n",
      "Hmm, that's interesting. So now we have $2^{1947} \\times 5^{1950}$.\n",
      "\n",
      "Let me think. When we have exponents, maybe we can use properties of exponents. For example, $a^m \\times a^n = a^{m+n}$.\n",
      "\n",
      "Wait, but here we have different bases, 2 and 5. Maybe we can rewrite this so they have the same base or something. Alternatively, perhaps we can separate them.\n",
      "\n",
      "Let me see. Look at the exponents: 1947 and 1950. 1950 is 1947 plus 3. So let's express 1950 as 1947 + 3.\n",
      "\n",
      "So $5^{1950} = 5^{1947 + 3} = 5^{1947} \\times 5^3$.\n",
      "\n",
      "Now substitute back:\n",
      "\n",
      "$2^{1947} \\times 5^{1947} \\times 5^ \n",
      "Extracted:\n",
      "2\n",
      "********************Question:\n",
      "Consider an arithmetic progression made up of $100$ terms. If the sum of all the terms of the progression is $150$ and the sum of the even terms is $50$, find the sum of the squares of the $100$ terms of the progression. \n",
      "Answer:\n",
      "83550 \n",
      "Response:\n",
      "Okay, so I have this problem about an arithmetic progression with 100 terms. The sum of all the terms is 150, and the sum of the even terms is 50. I need to find the sum of the squares of these 100 terms. Hmm, let me see how I can approach this.\n",
      "\n",
      "First, let me recall what an arithmetic progression is. It's a sequence where each term after the first is the sum of the previous term and a common difference. Let me denote the first term as \\( a \\) and the common difference as \\( d \\). Then, the terms can be written as:\n",
      "\n",
      "\\[ a, a + d, a + 2d, \\ldots, a + 99d \\]\n",
      "\n",
      "So, there are 100 terms. The sum of all terms is given as 150. I remember the formula for the sum of an arithmetic progression: \n",
      "\n",
      "\\[ S_n = \\frac{n}{2} [2a + (n - 1)d] \\]\n",
      "\n",
      "Here, \\( n = 100 \\), so:\n",
      "\n",
      "\\[ S_{100} = \\frac{100}{2} [2a + 99d] = 150 \\]\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "\\[ 50 (2a + 99d) = 150 \\]\n",
      "\\[ 2a + 99d = 3 \\]  --- (1)\n",
      "\n",
      "Then, the sum of the even terms is 50. In an arithmetic progression with 100 terms, the even terms would be terms 2, 4, ..., 100. There are 50 even terms. The sequence of even terms would be:\n",
      "\n",
      "\\[ a + d, a + 3d, \\ldots, a + 99d \\]\n",
      "\n",
      "Wait, let me check that. The general term of the AP is \\( a + td \\ \n",
      "Extracted:\n",
      "100\n",
      "********************Question:\n",
      "A sequence of three real numbers forms an arithmetic progression with a first term of $9$. If $2$ is added to the second term and $20$ is added to the third term, the three resulting numbers form a geometric progression. What is the smallest possible value for the third term in the geometric progression? \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "Okay, I need to solve this problem. Let me see... It says I have an arithmetic progression with three real numbers, and the first term is 9. Then if I add 2 to the second term and 20 to the third term, the three numbers form a geometric progression. I need to find the smallest possible value for the third term in the geometric progression. Hmm, that sounds a bit tricky, but maybe I can break it down step by step.\n",
      "\n",
      "First, let's recall what an arithmetic progression and a geometric progression are. An arithmetic progression is a sequence where each term after the first is obtained by adding a constant difference. So if the first term is 9, and the common difference is d, then the second term would be 9 + d, and the third term would be 9 + 2d. Right?\n",
      "\n",
      "Then, if I add 2 to the second term and 20 to the third term, the new sequence becomes 9, [9 + d + 2], [9 + 2d + 20], which simplifies to 9, 11 + d, 29 + 2d. This new sequence is supposed to be a geometric progression. So in a geometric progression, each term is found by multiplying the previous term by a common ratio, let's call it r.\n",
      "\n",
      "So for the sequence 9, 11 + d, 29 + 2d to be geometric, the ratio between consecutive terms should be the same. That means the ratio between the second and first term should equal the ratio between the third and second term. So let's write that down:\n",
      "\n",
      "(11 + d) / 9 = (29 + 2d) / (11 + d)\n",
      "\n",
      "Okay, now I need to solve this equation for d. Let's see:\n",
      "\n",
      "Multiply both sides by 9 to eliminate the denominator:\n",
      "\n",
      "(11 + d) = 9 * \n",
      "Extracted:\n",
      "9\n",
      "********************Question:\n",
      "There are $20n$ members in the Trumpington marching band, and when they line up in rows of 26, there are 4 band members left over. If $n$ is an integer and there are fewer than 1000 band members, what is the maximum number of people that could be in the Trumpington marching band? \n",
      "Answer:\n",
      "940 \n",
      "Response:\n",
      "### Understanding the Problem\n",
      "\n",
      "The problem states:\n",
      "\n",
      "*\"There are $20n$ members in the Trumpington marching band, and when they line up in rows of 26, there are 4 band members left over. If $n$ is an integer and there are fewer than 1000 band members, what is the maximum number of people that could be in the Trumpington marching band?\"*\n",
      "\n",
      "Let me break this down to understand what's being asked.\n",
      "\n",
      "1. **Total Band Members:** The number of band members is $20n$, where $n$ is an integer.\n",
      "2. **Remainder Condition:** When the band members line up in rows of 26, there are 4 left over. This means that when $20n$ is divided by 26, the remainder is 4.\n",
      "3. **Constraints:** There are fewer than 1000 band members. So, $20n < 1000$.\n",
      "4. **Objective:** Find the maximum number of band members possible under these conditions.\n",
      "\n",
      "### Translating the Problem into Mathematical Terms\n",
      "\n",
      "Okay, so if there are $20n$ band members, and when they line up in rows of 26, there are 4 left over. Mathematically, this can be expressed as a congruence:\n",
      "\n",
      "$$\n",
      "20n \\equiv 4 \\pmod{26}\n",
      "$$\n",
      "\n",
      "This reads as \"20n is congruent to 4 modulo 26,\" which means that $20n - 4$ is divisible by 26. So we can write:\n",
      "\n",
      "$$\n",
      "20n - 4 = 26k \\quad \\text{for some integer } k\n",
      "$$\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "$$\n",
      "20n = 26k + 4 \\\\\n",
      "\\Rightarrow 10n = 13k + \n",
      "Extracted:\n",
      "20\n",
      "********************Question:\n",
      "Billy wants to determine the average number of cookies in a package of cookies. He purchases 7 packages of cookies, opens them, and counts the number of cookies in each one. He finds that the packages have 8, 10, 12, 15, 16, 17, and 20 cookies. Based on these observations, what is the average number of cookies in a package?\n",
      "\n",
      "(Note: the average of a list of numbers is the sum of the numbers divided by how many numbers there are.) \n",
      "Answer:\n",
      "14 \n",
      "Response:\n",
      "Alright, let me see now. So, Billy wants to find the average number of cookies in a package. He has seven packages, and the counts are 8, 10, 12, 15, 16, 17, and 20. Right, so to find the average, I need to add up all those numbers and then divide by 7, since there are seven packages. Let me do this step by step.\n",
      "\n",
      "First, I'll add them up. That's 8 plus 10 plus 12 plus 15 plus 16 plus 17 plus 20. Hmm, let me calculate that. 8+10 is 18. 18+12 is 30. 30+15 is 45. 45+16 is 61. 61+17 is 78. 78+20 is 98. So the total number of cookies is 98.\n",
      "\n",
      "Now, divide that by 7. 98 divided by 7. Wait, uh... let's see. 7 times 14 is 98 because 7 times 10 is 70, plus 7 times 4 is 28, so 70+28 is 98. So 98 divided by 7 is 14. So the average is 14.\n",
      "\n",
      "Wait, let me check my calculations. Adding them up was 8+10+12+15+16+17+20. I did that step by step:\n",
      "\n",
      "8+10=18  \n",
      " \n",
      "Extracted:\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.006163071701594163, metrics={'train_runtime': 4924.7777, 'train_samples_per_second': 0.081, 'train_steps_per_second': 0.02, 'total_flos': 0.0, 'train_loss': 0.006163071701594163})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,  \n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_partially,\n",
    "        check_answer,\n",
    "        check_numbers\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    \n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c8759",
   "metadata": {},
   "source": [
    "Check the prompts and completions used in function: `check_numbers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [[{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': 'Let $ABCD$ be a unit square in the plane. Points $X$ and $Y$ are chosen independently and uniformly at random on the perimeter of $ABCD$. If the expected value of the area of triangle $\\\\triangle AXY$ can be expressed as $\\\\frac{m}{n}$ for relatively prime positive integers $m$ and $n$, compute $m+n$.', 'role': 'user'}], [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': 'Let $ABCD$ be a unit square in the plane. Points $X$ and $Y$ are chosen independently and uniformly at random on the perimeter of $ABCD$. If the expected value of the area of triangle $\\\\triangle AXY$ can be expressed as $\\\\frac{m}{n}$ for relatively prime positive integers $m$ and $n$, compute $m+n$.', 'role': 'user'}], [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': 'Let $ABCD$ be a unit square in the plane. Points $X$ and $Y$ are chosen independently and uniformly at random on the perimeter of $ABCD$. If the expected value of the area of triangle $\\\\triangle AXY$ can be expressed as $\\\\frac{m}{n}$ for relatively prime positive integers $m$ and $n$, compute $m+n$.', 'role': 'user'}], [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': 'Let $ABCD$ be a unit square in the plane. Points $X$ and $Y$ are chosen independently and uniformly at random on the perimeter of $ABCD$. If the expected value of the area of triangle $\\\\triangle AXY$ can be expressed as $\\\\frac{m}{n}$ for relatively prime positive integers $m$ and $n$, compute $m+n$.', 'role': 'user'}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_completions = [[{'role': 'assistant', 'content': \" \\nWe start by considering the unit square \\\\(ABCD\\\\) with vertices \\\\(A = (0,0)\\\\), \\\\(B = (1,0)\\\\), \\\\(C = (1,1)\\\\), and \\\\(D = (0,1)\\\\).\\n\\nNext, we analyze the problem by breaking it down into manageable parts:\\n1. **Perimeter and Coordinates**: We need to consider the perimeter of the square. The perimeter consists of four segments: \\\\(AB\\\\), \\\\(BC\\\\), \\\\(CD\\\\), and \\\\(DA\\\\). We parameterize each segment using a variable, for example, for segment \\\\(AB\\\\), we can use a parameter \\\\(t\\\\) such that \\\\(0 \\\\le t \\\\le 1\\\\), where \\\\((x,y) = (t,0)\\\\).\\n2. **General Point Representation**: The points \\\\(X\\\\) and \\\\(Y\\\\) can be represented on this parameterized perimeter. Let \\\\(X = (x_1, y_1)\\\\) and \\\\(Y = (x_2, y_2)\\\\).\\n3. **Area of Triangle \\\\(\\\\triangle AXY\\\\)**: We need to compute the area of triangle \\\\(\\\\triangle AXY\\\\) for any random points \\\\(X\\\\) and \\\\(Y\\\\) on the perimeter.\\n4. **Expected Value of Area**: We need to find the expected value of the area of \\\\(\\\\triangle AXY\\\\) over all possible positions of \\\\(X\\\\) and \\\\(Y\\\\). By symmetry, the expected value remains invariant across each segment of the perimeter.\\n\\nUsing the formula for the area of a triangle with vertices \\\\((0,0)\\\\), \\\\((a,b)\\\\), and \\\\((c,d)\\\\):\\n\\n\\\\[\\n\\\\text{Area} = \\\\frac{1}{2} \\\\left| ad - bc \\\\right|\\n\\\\]\\n\\nWe can apply this formula for our specific squares and derived points.\\n\\nLet's compute the explicit expected value using\"}], [{'role': 'assistant', 'content': \"First, let's analyze the problem. We need to find the expected value of the area of triangle $\\\\triangle AXY$, where $X$ and $Y$ are uniformly and independently chosen on the perimeter of a unit square $ABCD$. \\n\\nTo solve this, we will consider different cases based on the positions of points $X$ and $Y$ along the perimeter. Points on the perimeter can be described by their coordinates relative to the corners of the square. Let's assume the square is oriented such that $A$ is at $(0,0)$, $B$ is at $(1,0)$, $C$ is at $(1,1)$, and $D$ is at $(0,1)$.\\n\\nWe will divide the problem into smaller cases based on the location of $X$ and $Y$:\\n\\n1. Both $X$ and $Y$ are on the same side:\\n2. $X$ and $Y$ are on adjacent sides:\\n3. $X$ and $Y$ are on opposite sides:\\n\\nLet's calculate the expected area for each case:\\n\\n1. Both $X$ and $Y$ on the same side:\\n  - Formula for area: $\\\\frac{1}{2} \\\\times \\\\text{base} \\\\times \\\\text{height}$\\n  - Expected area: $\\\\frac{1}{4}$ (formed by $\\\\triangle ABX$ and $\\\\triangle ADY$)\\n  \\n2. $X$ and $Y$ on adjacent sides:\\n  - To calculate the expected area here, we consider the midpoint as a balancing point. The expected value is the weighted average of areas comparing midpoint to $X$ and $Y$, which would average out over a quarter of the square:\\n  - We can see that the expected area for this swapping (split of the diamond shape made) 4 times, giving us $\\\\frac{1}{\"}], [{'role': 'assistant', 'content': ' <SOLUTION>51</SOLUTION>'}], [{'role': 'assistant', 'content': \"Oh dear - what a messy one for you. I might start by drawing a picture (while remembering that a unit square has all sides of unit length).\\n\\nHere is a good Aops discussion of the topic: https://artofproblemsolving.com/wiki/index.php?title=2001_AIME_I_Problems/Problem_9 (the problem could be restated here as, Essentially, it says that two points on the perimeter are chosen and we want the average area of the triangle connecting them with $(0,0)$. This questions assumes the area is evenly 'spread' along the triangular 'space' ie 2/3 of it can define the solutions. So the answer should be 1*1/3)</START_WORKING_OUT><SOLUTION>The area of the interior of the square bounded by the triangle $AXY$ is \\n\\\\[\\\\iint_{\\\\square ABCD} x = \\\\frac{1}{4} \\\\iint_{\\\\square 0001} xy\\\\,dx\\\\, dy = \\\\frac{1}{4},\\\\]so expect \\\\[\\\\boxed02.\\\\]</SOLUTION>\\n\\nReference: https://web2.0calc.com/questions/this-is-an-imo-geometry-problem-2022-july\"}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be851fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>',\n",
       " 'role': 'system'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sys_dict = {}\n",
    "test_user_dict = {}\n",
    "for i in range(len(test_prompts)):\n",
    "    for j in range(i, len(test_prompts), 1):\n",
    "        test_sys_dict[str(test_prompts[i][0])] = str(test_prompts[j][0])\n",
    "        test_user_dict[str(test_prompts[i][1])] = str(test_prompts[j][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(input_list: list):\n",
    "    test_role_dict = {}\n",
    "    test_content_dict = {}\n",
    "    for i in range(len(input_list)):\n",
    "        for j in range(i, len(input_list), 1):\n",
    "            test_role_dict[str(input_list[i][0])] = str(input_list[j][0])\n",
    "            test_content_dict[str(input_list[i][-1])] = str(input_list[j][-1])\n",
    "\n",
    "    print(f\"num of unique {input_list[0][0]['role']}: {len(test_role_dict.keys())}\")\n",
    "    print(f\"num of unique content: {len(test_content_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of unique system: 1\n",
      "num of unique content: 1\n"
     ]
    }
   ],
   "source": [
    "check_data(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of unique assistant: 4\n",
      "num of unique content: 4\n"
     ]
    }
   ],
   "source": [
    "check_data(test_completions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fe128",
   "metadata": {},
   "source": [
    "- The prompts are duplicate\n",
    "- There are `num_generation` different completions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlalgofromscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
