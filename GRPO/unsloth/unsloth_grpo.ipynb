{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884dc606",
   "metadata": {},
   "source": [
    "- Ref: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb#scrollTo=DkIvEkIIkEyB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b9ec6",
   "metadata": {},
   "source": [
    "Instal unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac4576a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "# uv add \"unsloth[cu124-torch251] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db15e63",
   "metadata": {},
   "source": [
    "Pre-fine-tune the model to make GRPO follow the desired output format — this speeds up GRPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7052ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sean/MLAlgoFromScratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 08-14 06:52:59 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 08-14 06:52:59 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 06:53:00,928\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7bb6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fdbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb056b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching vLLM v1 graph capture\n",
      "Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.7.8: Fast Qwen3 patching. Transformers: 4.54.0. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.581 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 59.0%\n",
      "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.58 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 512. Num Sequences = 192.\n",
      "Unsloth: vLLM's KV Cache can use up to 5.94 GB. Also swap space = 0 GB.\n",
      "WARNING 08-14 06:53:16 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-14 06:53:26 [config.py:717] This model supports multiple tasks: {'reward', 'generate', 'classify', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 08-14 06:53:26 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 08-14 06:53:26 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=512, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":4,\"combo_kernels\":false,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":192}, use_cached_outputs=False, \n",
      "INFO 08-14 06:53:30 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 08-14 06:53:30 [cuda.py:289] Using XFormers backend.\n",
      "INFO 08-14 06:53:30 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 08-14 06:53:30 [model_runner.py:1108] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n",
      "INFO 08-14 06:53:31 [loader.py:1187] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 08-14 06:53:33 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 08-14 06:53:34 [weight_utils.py:281] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 0.760242 seconds\n",
      "INFO 08-14 06:53:34 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.20s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.20s/it]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.98s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 06:53:59 [punica_selector.py:18] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 06:53:59 [model_runner.py:1140] Model loading took 3.1446 GiB and 28.325048 seconds\n",
      "INFO 08-14 06:54:05 [worker.py:287] Memory profiling takes 5.17 seconds\n",
      "INFO 08-14 06:54:05 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.58GiB) x gpu_memory_utilization (0.59) = 8.60GiB\n",
      "INFO 08-14 06:54:05 [worker.py:287] model weights take 3.14GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 1.05GiB; the rest of the memory reserved for KV Cache is 4.39GiB.\n",
      "INFO 08-14 06:54:05 [executor_base.py:112] # cuda blocks: 1995, # CPU blocks: 0\n",
      "INFO 08-14 06:54:05 [executor_base.py:117] Maximum concurrency for 512 tokens per request: 62.34x\n",
      "INFO 08-14 06:54:05 [vllm_utils.py:669] Unsloth: Running patched vLLM v0 `capture_model`.\n",
      "INFO 08-14 06:54:05 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 27/27 [00:14<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 06:54:19 [model_runner.py:1592] Graph capturing finished in 14 secs, took 4.24 GiB\n",
      "INFO 08-14 06:54:19 [vllm_utils.py:676] Unsloth: Patched vLLM v0 graph capture finished in 14 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-14 06:54:20 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 21.07 seconds\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n",
      "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n"
     ]
    }
   ],
   "source": [
    "# key: vllm==0.8.5.post1\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name='unsloth/Qwen3-4B-base',\n",
    "    max_seq_length=512,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=True, # Enable vLLM fast inference\n",
    "    max_lora_rank=8,\n",
    "    gpu_memory_utilization=0.6 # Reduce if OOM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa61c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer can handle up to 32768 tokens.\n"
     ]
    }
   ],
   "source": [
    "max_dim = tokenizer.model_max_length\n",
    "print(f\"Tokenizer can handle up to {max_dim} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd7872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template is None\n",
    "# ValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c37a0c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (2): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (3): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (4): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (5): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (6): Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "      (7-35): 29 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b42afd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.8 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "lora_rank = 8  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank,\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc681be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (2): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (3): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (4): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (5): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (6): Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "          (7-35): 29 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2560, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=9728, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9728, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2560, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53675cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5780a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Always includes a system prompt (either from messages or a default one).\n",
    "- Marks assistant turns with an eos_token so the model knows where each reply ends.\n",
    "- Optionally adds a “start reasoning” marker to signal when the model should generate output.\n",
    "- Uses placeholders that get replaced at runtime so you can change system prompts or special tokens without editing the Jinja template.\n",
    "\"\"\"\n",
    "\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace without specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7198ff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>What is 1+1?<start_working_out>I think it's 2.<end_working_out><SOLUTION>2</SOLUTION><|endoftext|>What is 2+2?<start_working_out>\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn a structured list of messages into the exact string prompt the model will see.\n",
    "tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
    "], tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "474d6e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>problem_type</th>\n",
       "      <th>problem_source</th>\n",
       "      <th>generation_model</th>\n",
       "      <th>pass_rate_72b_tir</th>\n",
       "      <th>problem</th>\n",
       "      <th>generated_solution</th>\n",
       "      <th>inference_mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c4_high_school_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>convergent</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c7_college_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Let \\( \\sum a_n \\) be a convergent series with...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I have this problem here whe...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\(\\frac{\\pi \\ln a}{2a}\\)</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c7_college_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>For \\( a &gt; 0 \\), find the value of \\( \\int_0^{...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to solve the integral...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\(+\\infty\\)</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c7_college_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Calculate $\\lim_{n\\to\\infty}\\sqrt[n]{n!}$.</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the limit as ...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\( n = 1 \\)</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c6_high_school_olympiads</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Find all positive integers \\( n \\) such that \\...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find all positive ...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>4</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c4_high_school_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>A bus left point X for point Y. Two hours late...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19248</th>\n",
       "      <td>18</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c4_high_school_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Each interior angle of a regular n-gon measure...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19249</th>\n",
       "      <td>\\( e^3 - 1 \\)</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c7_college_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Evaluate the series \\( \\sum_{k=1}^{\\infty}\\fra...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the serie...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19250</th>\n",
       "      <td>0.8960</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c4_high_school_math</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Find the probability that the second blue resu...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19251</th>\n",
       "      <td>\\( x = 1, y = -1, z = 2 \\)</td>\n",
       "      <td>has_answer_extracted</td>\n",
       "      <td>aops_c6_high_school_olympiads</td>\n",
       "      <td>DeepSeek-R1</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>Solve the system of equations:\\n\\[ x^2 + y^2 +...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I've got this system of thre...</td>\n",
       "      <td>cot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19252 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  expected_answer          problem_type  \\\n",
       "0                              14  has_answer_extracted   \n",
       "1                      convergent  has_answer_extracted   \n",
       "2        \\(\\frac{\\pi \\ln a}{2a}\\)  has_answer_extracted   \n",
       "3                     \\(+\\infty\\)  has_answer_extracted   \n",
       "4                     \\( n = 1 \\)  has_answer_extracted   \n",
       "...                           ...                   ...   \n",
       "19247                           4  has_answer_extracted   \n",
       "19248                          18  has_answer_extracted   \n",
       "19249               \\( e^3 - 1 \\)  has_answer_extracted   \n",
       "19250                      0.8960  has_answer_extracted   \n",
       "19251  \\( x = 1, y = -1, z = 2 \\)  has_answer_extracted   \n",
       "\n",
       "                      problem_source generation_model pass_rate_72b_tir  \\\n",
       "0           aops_c4_high_school_math      DeepSeek-R1           0.96875   \n",
       "1               aops_c7_college_math      DeepSeek-R1           0.96875   \n",
       "2               aops_c7_college_math      DeepSeek-R1           0.96875   \n",
       "3               aops_c7_college_math      DeepSeek-R1           0.96875   \n",
       "4      aops_c6_high_school_olympiads      DeepSeek-R1           0.96875   \n",
       "...                              ...              ...               ...   \n",
       "19247       aops_c4_high_school_math      DeepSeek-R1           0.96875   \n",
       "19248       aops_c4_high_school_math      DeepSeek-R1           0.96875   \n",
       "19249           aops_c7_college_math      DeepSeek-R1           0.96875   \n",
       "19250       aops_c4_high_school_math      DeepSeek-R1           0.96875   \n",
       "19251  aops_c6_high_school_olympiads      DeepSeek-R1           0.96875   \n",
       "\n",
       "                                                 problem  \\\n",
       "0      Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
       "1      Let \\( \\sum a_n \\) be a convergent series with...   \n",
       "2      For \\( a > 0 \\), find the value of \\( \\int_0^{...   \n",
       "3             Calculate $\\lim_{n\\to\\infty}\\sqrt[n]{n!}$.   \n",
       "4      Find all positive integers \\( n \\) such that \\...   \n",
       "...                                                  ...   \n",
       "19247  A bus left point X for point Y. Two hours late...   \n",
       "19248  Each interior angle of a regular n-gon measure...   \n",
       "19249  Evaluate the series \\( \\sum_{k=1}^{\\infty}\\fra...   \n",
       "19250  Find the probability that the second blue resu...   \n",
       "19251  Solve the system of equations:\\n\\[ x^2 + y^2 +...   \n",
       "\n",
       "                                      generated_solution inference_mode  \n",
       "0      <think>\\nOkay, let's see. I need to solve the ...            cot  \n",
       "1      <think>\\nOkay, so I have this problem here whe...            cot  \n",
       "2      <think>\\nOkay, so I need to solve the integral...            cot  \n",
       "3      <think>\\nOkay, so I need to find the limit as ...            cot  \n",
       "4      <think>\\nOkay, so I need to find all positive ...            cot  \n",
       "...                                                  ...            ...  \n",
       "19247  <think>\\nOkay, let's tackle this problem step ...            cot  \n",
       "19248  <think>\\nOkay, let's see. I need to find the n...            cot  \n",
       "19249  <think>\\nOkay, so I need to evaluate the serie...            cot  \n",
       "19250  <think>\\nOkay, so I need to find the probabili...            cot  \n",
       "19251  <think>\\nOkay, so I've got this system of thre...            cot  \n",
       "\n",
       "[19252 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8d4ea3",
   "metadata": {},
   "source": [
    "Note\n",
    "- I think the reason of using math here is the answer is precise\n",
    "- If the result is text, we may compute the text similarity to confirm the answer ?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b36bdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>problem</th>\n",
       "      <th>generated_solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2</td>\n",
       "      <td>Find the value of the parameter $a$ for which ...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>What is the sum of all real numbers $x$ for wh...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to solve the equation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the infin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>What is the largest positive integer that divi...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, so I need to find the larges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19243</th>\n",
       "      <td>244</td>\n",
       "      <td>Let \\( p \\), \\( q \\), and \\( r \\) be the disti...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19245</th>\n",
       "      <td>1</td>\n",
       "      <td>A bug is on the $0$ of a number line. At any p...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I have this problem where a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>4</td>\n",
       "      <td>A bus left point X for point Y. Two hours late...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19248</th>\n",
       "      <td>18</td>\n",
       "      <td>Each interior angle of a regular n-gon measure...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19250</th>\n",
       "      <td>0.8960</td>\n",
       "      <td>Find the probability that the second blue resu...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7507 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      expected_answer                                            problem  \\\n",
       "0                  14  Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
       "6                  -2  Find the value of the parameter $a$ for which ...   \n",
       "9                  18  What is the sum of all real numbers $x$ for wh...   \n",
       "13                  2  Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...   \n",
       "17                 30  What is the largest positive integer that divi...   \n",
       "...               ...                                                ...   \n",
       "19243             244  Let \\( p \\), \\( q \\), and \\( r \\) be the disti...   \n",
       "19245               1  A bug is on the $0$ of a number line. At any p...   \n",
       "19247               4  A bus left point X for point Y. Two hours late...   \n",
       "19248              18  Each interior angle of a regular n-gon measure...   \n",
       "19250          0.8960  Find the probability that the second blue resu...   \n",
       "\n",
       "                                      generated_solution  \n",
       "0      <think>\\nOkay, let's see. I need to solve the ...  \n",
       "6      <think>\\nOkay, so I need to find the value of ...  \n",
       "9      <think>\\nOkay, so I need to solve the equation...  \n",
       "13     <think>\\nOkay, so I need to evaluate the infin...  \n",
       "17     <think>\\nAlright, so I need to find the larges...  \n",
       "...                                                  ...  \n",
       "19243  <think>\\nOkay, so I need to find the value of ...  \n",
       "19245  <think>\\nOkay, so I have this problem where a ...  \n",
       "19247  <think>\\nOkay, let's tackle this problem step ...  \n",
       "19248  <think>\\nOkay, let's see. I need to find the n...  \n",
       "19250  <think>\\nOkay, so I need to find the probabili...  \n",
       "\n",
       "[7507 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.to_pandas()[\n",
    "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "]\n",
    "\n",
    "# Try converting to number - if not, replace with NaN\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
    "# Select only numbers\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7deb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(x):\n",
    "    expected_answer = x[\"expected_answer\"]\n",
    "    problem = x[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + expected_answer + solution_end\n",
    "    return [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532c1aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'},\n",
       " {'role': 'user',\n",
       "  'content': 'Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"<start_working_out>Okay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION>\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_msg = dataset.iloc[0].Messages\n",
    "test_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bce393",
   "metadata": {},
   "source": [
    "Check to see if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc0e4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<start_working_out>Okay, let's see. I need to solve the equation √(x² + 165) - √(x² - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n√(x² + 165) - √(x² - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n√(x² + 165) = 7 + √(x² - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(√(x² + 165))² = (7 + √(x² - 52))².\\n\\nSimplifying the left side:\\n\\nx² + 165 = 49 + 14√(x² - 52) + (√(x² - 52))².\\n\\nThe right side is expanded using the formula (a + b)² = a² + 2ab + b². So the right side becomes 7² + 2*7*√(x² - 52) + (√(x² - 52))², which is 49 + 14√(x² - 52) + (x² - 52).\\n\\nSo putting it all together:\\n\\nx² + 165 = 49 + 14√(x² - 52) + x² - 52.\\n\\nHmm, let's simplify the right side. The x² terms will cancel out, right? Let's subtract x² from both sides:\\n\\n165 = 49 + 14√(x² - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14√(x² - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14√(x² - 52).\\n\\nSo 168 = 14√(x² - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = √(x² - 52).\\n\\n12 = √(x² - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12² = x² - 52.\\n\\n144 = x² - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x².\\n\\n196 = x².\\n\\nSo x = √196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n√(14² + 165) - √(14² - 52) = ?\\n\\nCalculate each term:\\n\\n14² is 196.\\n\\nSo first radical: √(196 + 165) = √361 = 19.\\n\\nSecond radical: √(196 - 52) = √144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION><|endoftext|>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default of tokenize is True\n",
    "tokenizer.apply_chat_template(test_msg, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4d2bfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized result: [2610, 525, 2661, 264, 3491, 624, 38687, 911, 279, 3491]... \n",
      "with dimension: 1682\n"
     ]
    }
   ],
   "source": [
    "tokenized_result = tokenizer.apply_chat_template(test_msg)\n",
    "print(f\"Tokenized result: {tokenized_result[:10]}... \\nwith dimension: {len(tokenized_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81920244",
   "metadata": {},
   "source": [
    "Limit sequence length to max_seq_length / 2 during pre-fine-tuning to avoid overly long reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "769dbb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.loc[[len(tokenizer.apply_chat_template(x)) < max_seq_length/2 for x in dataset['Messages']]]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb5ae3",
   "metadata": {},
   "source": [
    "Tokenize data and convert it to a HF compatible dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d0e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2373089/1368114567.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset['text'] = tokenizer.apply_chat_template(dataset['Messages'].values.tolist(), tokenize=False)\n"
     ]
    }
   ],
   "source": [
    "dataset['text'] = tokenizer.apply_chat_template(dataset['Messages'].values.tolist(), tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aaeabe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>problem</th>\n",
       "      <th>generated_solution</th>\n",
       "      <th>Messages</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>17</td>\n",
       "      <td>Jenifer has 82 cents in pennies and nickels. H...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. So the problem is ab...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are given ...</td>\n",
       "      <td>You are given a problem.\\nThink about the prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>4</td>\n",
       "      <td>What is the average book width, in centimeters...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the a...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are given ...</td>\n",
       "      <td>You are given a problem.\\nThink about the prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>20</td>\n",
       "      <td>There are 103 honors students in 10th grade. I...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. So the problem says ...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are given ...</td>\n",
       "      <td>You are given a problem.\\nThink about the prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>120</td>\n",
       "      <td>In how many different ways can 14 identical ca...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to figure out how man...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are given ...</td>\n",
       "      <td>You are given a problem.\\nThink about the prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>74</td>\n",
       "      <td>Mary has taken four tests and has a test avera...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. Mary has taken four ...</td>\n",
       "      <td>[{'role': 'system', 'content': 'You are given ...</td>\n",
       "      <td>You are given a problem.\\nThink about the prob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     expected_answer                                            problem  \\\n",
       "345               17  Jenifer has 82 cents in pennies and nickels. H...   \n",
       "765                4  What is the average book width, in centimeters...   \n",
       "998               20  There are 103 honors students in 10th grade. I...   \n",
       "1055             120  In how many different ways can 14 identical ca...   \n",
       "1598              74  Mary has taken four tests and has a test avera...   \n",
       "\n",
       "                                     generated_solution  \\\n",
       "345   <think>\\nOkay, let's see. So the problem is ab...   \n",
       "765   <think>\\nOkay, let's see. I need to find the a...   \n",
       "998   <think>\\nOkay, let's see. So the problem says ...   \n",
       "1055  <think>\\nOkay, so I need to figure out how man...   \n",
       "1598  <think>\\nOkay, let's see. Mary has taken four ...   \n",
       "\n",
       "                                               Messages  \\\n",
       "345   [{'role': 'system', 'content': 'You are given ...   \n",
       "765   [{'role': 'system', 'content': 'You are given ...   \n",
       "998   [{'role': 'system', 'content': 'You are given ...   \n",
       "1055  [{'role': 'system', 'content': 'You are given ...   \n",
       "1598  [{'role': 'system', 'content': 'You are given ...   \n",
       "\n",
       "                                                   text  \n",
       "345   You are given a problem.\\nThink about the prob...  \n",
       "765   You are given a problem.\\nThink about the prob...  \n",
       "998   You are given a problem.\\nThink about the prob...  \n",
       "1055  You are given a problem.\\nThink about the prob...  \n",
       "1598  You are given a problem.\\nThink about the prob...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f177fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e4ca959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['expected_answer', 'problem', 'generated_solution', 'Messages', 'text', '__index_level_0__'],\n",
       "    num_rows: 58\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b6ef3",
   "metadata": {},
   "source": [
    "Pre fine-tune the model so it follows the custom GRPO formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bbc8371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:17<00:00, 17.25s/it, est. speed input: 0.58 toks/s, output: 29.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Answers\n",
      "Math and Arithmetic\n",
      "Algebra\n",
      "What is the sqrt of 101?\n",
      "Wiki User\n",
      "∙ 2010-03-08 21:11:46\n",
      "See Answer\n",
      "Best Answer\n",
      "Copy\n",
      "10 is between the perfect squares of 36 and 49, so sqrt101 is between 6 and 7, and is very close to 10.\n",
      "Wiki User\n",
      "∙ 2010-03-08 21:11:46\n",
      "This answer is:\n",
      "Helpful\n",
      "Not Helpful\n",
      "🙏\n",
      "0\n",
      "🤨\n",
      "0\n",
      "😮\n",
      "0\n",
      "Add a Comment\n",
      "Add your answer:\n",
      "Earn +20 pts\n",
      "Q: What is the sqrt of 101?\n",
      "Write your answer...\n",
      "Submit\n",
      "Sign up for more answers\n",
      "Already have an account? Log in\n",
      "Related Questions\n",
      "What is the sqrt of 1 01?\n",
      "sqrt (101) = +-10.050... (approx)\n",
      "Simplify sqrt 101 over 900?\n",
      "sqrt 101/900 = sqrt 101/30 or sqrt 101 / (30)\n",
      "What is sqrt 25x401?\n",
      "sqrt (25 x 401) = 5 sqrt (401)\n",
      "What is the square root of 101?\n",
      "sqrt (101) approx = +/- 10.05\n",
      "How do you simplify sqrt 50 to sqrt 98?\n",
      "Sqrt (50) x sqrt (98) = sqrt (50 x 98) = 50 x 98 = 4900 = 70  But I'm not sure what you are asking.\n",
      "What is 5x101?\n",
      "sqrt (901)\n",
      "What is 7x 7?\n",
      "sqrt (901)\n",
      "What is 30.8x30.8?\n",
      "sqrt (901)\n",
      "What 67 by 14 equals?\n",
      "sqrt (9538)\n",
      "What is 9 multiplied by 5?\n",
      "sqrt (901)\n",
      "What is 2x4 with an exponent of 21?\n",
      "sqrt (901)\n",
      "What is 197 minus 95?\n",
      "sqrt (9538)\n",
      "What is 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# check the result before pre fine-tune\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "text = \"What is the sqrt of 101?\"\n",
    "output = model.fast_generate(\n",
    "    [text], \n",
    "    sampling_params=sampling_params, \n",
    "    lora_request=None\n",
    ")\n",
    "print(output[0].outputs[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "423bd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|██████████| 58/58 [00:01<00:00, 45.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field='text',\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8, # update after accumulation step\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=5,\n",
    "        optim='adamw_8bit',\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type='linear',\n",
    "        seed=3407,\n",
    "        report_to='none'\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bda30c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 58 | Num Epochs = 3 | Total steps = 24\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 16,515,072 of 4,038,983,168 (0.41% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 02:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.862600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.596900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.7471394836902618, metrics={'train_runtime': 175.1351, 'train_samples_per_second': 0.994, 'train_steps_per_second': 0.137, 'total_flos': 1100843465048064.0, 'train_loss': 0.7471394836902618})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52678b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have?<start_working_out>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"Messages\"][:2],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True # add this then the ending is \"<start_working_out>\"\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "953ee5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TextStreamer\n",
    "# _ = model.generate(\n",
    "#     **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "#     temperature = 0,\n",
    "#     max_new_tokens = 1024,\n",
    "#     streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "609db5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean memory\n",
    "del dataset\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5288ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
       "    num_rows: 14116\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "911a89ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the overall prompt (including problem and system prompt) in the OpenR1 datatset\n",
    "dataset[0][\"source_prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b7eb3aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\n"
     ]
    }
   ],
   "source": [
    "# the problem\n",
    "print(dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a9e3aa",
   "metadata": {},
   "source": [
    "Synthesis the dataset with customized system prompt and quetion(the name is prompt in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8179f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x[\"prompt\"]}\n",
    "    ],\n",
    "    \"answer\": x['solution']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1192b813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Three 12 cm $\\\\times$ 12 cm squares are each cut into two pieces $A$ and $B$, as shown in the first figure below, by joining the midpoints of two adjacent sides. These six pieces are then attached to a regular hexagon, as shown in the second figure, so as to fold into a polyhedron. What is the volume (in $\\\\text{cm}^3$) of this polyhedron?',\n",
       "   'role': 'user'}],\n",
       " 'solution': '864',\n",
       " 'data_source': 'math_dapo',\n",
       " 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nThree 12 cm $\\\\times$ 12 cm squares are each cut into two pieces $A$ and $B$, as shown in the first figure below, by joining the midpoints of two adjacent sides. These six pieces are then attached to a regular hexagon, as shown in the second figure, so as to fold into a polyhedron. What is the volume (in $\\\\text{cm}^3$) of this polyhedron?\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
       "   'role': 'user'}],\n",
       " 'ability': 'MATH',\n",
       " 'reward_model': {'ground_truth': '864', 'style': 'rule-lighteval/MATH_v2'},\n",
       " 'extra_info': {'index': 'fe84e2ab-5d14-453f-aa72-5e5b39100c55'},\n",
       " 'answer': '864'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1863143f",
   "metadata": {},
   "source": [
    "Extract the final answer from a structured text that contains both:\n",
    "- a reasoning section (e.g., <REASONING>...</REASONING>)\n",
    "- and a solution section (e.g., <SOLUTION>...</SOLUTION>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e493aa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'<end_working_out>.*?<SOLUTION>(.+?)</SOLUTION>[\\s]{0,}(?:<\\|endoftext\\|>)?[\\s]{0,}$',\n",
       "           re.MULTILINE|re.DOTALL|re.UNICODE)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Add optional EOS token matching\n",
    "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dccdc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n2\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test case\n",
    "match_format.findall(\n",
    "    \"Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf13e9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['364']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_format.findall(\n",
    "    \"Let me think!<end_working_out>\"\\\n",
    "    f\"<SOLUTION>364</SOLUTION>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574cd87c",
   "metadata": {},
   "source": [
    "Create reward function to match the format \n",
    "- +3 points if the output matches the full format exactly\n",
    "- Partial reward if key format tags (<REASONING>, </REASONING>, etc.) are present (each adds +0.5).\n",
    "- Penalty (-1.0) if a required tag is missing or appears more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "add0b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions: list, **kwargs):\n",
    "    scores = []\n",
    "    for competion in completions:\n",
    "        score = 0\n",
    "        response = competion[0]['content']\n",
    "        \n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff0a0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_partially(completions, **kwargs):\n",
    "    scores = []\n",
    "    for competion in completions:\n",
    "        score = 0\n",
    "        response = competion[0]['content']\n",
    "\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb71197",
   "metadata": {},
   "source": [
    "Reward of penalize the generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "209a4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer):\n",
    "    question = prompts[0][-1]['content']\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        result.group(1)\n",
    "        if (result := match_format.search(r)) is not None else None for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0) # Note that it is append to scores not score, which means no answer=-2, then next run\n",
    "            continue\n",
    "        \n",
    "        # 5 points for correct answer\n",
    "        if guess  == true_answer:\n",
    "            score += 5.0\n",
    "\n",
    "        # Match if spaces are seen but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        \n",
    "        else:\n",
    "            # Reward the answer if it is close via ratios\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if ratio >= 0.9 and ratio <= 1.1:\n",
    "                    score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2:\n",
    "                    score += 1.5\n",
    "                else:\n",
    "                    score += -2.5\n",
    "            except:\n",
    "                score += -4.5\n",
    "        scores.append(score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b23b8",
   "metadata": {},
   "source": [
    "Remove commas such as 123,456 or extract the number in a sentence for example \"The solution is $20\" -> 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "91b9885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.34']\n",
      "['123456']\n",
      "['-0.234']\n",
      "['17']\n"
     ]
    }
   ],
   "source": [
    "# Match numbers like 17, 123,456, 0.34, -0.234, 1,234,567.89\n",
    "match_numbers = re.compile(r\"-?(?:\\d{1,3}(?:,\\d{3})+|\\d+)(?:\\.\\d+)?\", re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def find_numbers_clean(text):\n",
    "    \"\"\"Return numbers as strings with commas removed.\"\"\"\n",
    "    return [n.replace(\",\", \"\") for n in match_numbers.findall(text)]\n",
    "\n",
    "# If you'd like floats instead, use this:\n",
    "def find_numbers_float(text):\n",
    "    return [float(n.replace(\",\", \"\")) for n in match_numbers.findall(text)]\n",
    "\n",
    "# Tests\n",
    "print(find_numbers_clean(\"<SOLUTION>  0.34  </SOLUTION>\"))      # ['0.34']\n",
    "print(find_numbers_clean(\"<SOLUTION>  123,456  </SOLUTION>\"))   # ['123456']\n",
    "print(find_numbers_clean(\"<SOLUTION>  -0.234  </SOLUTION>\"))    # ['-0.234']\n",
    "print(find_numbers_clean(\"<SOLUTION> The solution is 17  </SOLUTION>\"))  # ['17']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e37e9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlalgofromscratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
